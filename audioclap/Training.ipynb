{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "\n",
    "* pickle1.p\n",
    "* Marker = einzelne Channel + Mono\n",
    "* Random Mono-Snippets\n",
    "* getMeanMFCC() x 40 (untereinander)\n",
    "* Architektur: CNN von https://medium.com/@mikesmales/sound-classification-using-deep-learning-8bc2aa1990b7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-272.93866, 78.29002, -5.7069273, 12.786187, ...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>[-605.18494, 94.289894, -12.569014, 18.861961,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>[-409.41293, 122.481445, -41.31422, 19.439909,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>[-602.84, 65.77663, -3.561166, 43.207966, -8.1...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>[-542.72516, 172.78352, -28.462677, 27.740074,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>[-427.63205, 72.44588, 10.115246, 27.494013, -...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                feature class_label\n",
       "0     [-272.93866, 78.29002, -5.7069273, 12.786187, ...       klapp\n",
       "950   [-605.18494, 94.289894, -12.569014, 18.861961,...       klapp\n",
       "1900  [-409.41293, 122.481445, -41.31422, 19.439909,...       klapp\n",
       "2850  [-602.84, 65.77663, -3.561166, 43.207966, -8.1...     noklapp\n",
       "3800  [-542.72516, 172.78352, -28.462677, 27.740074,...     noklapp\n",
       "4750  [-427.63205, 72.44588, 10.115246, 27.494013, -...     noklapp"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pickle.load(open(\"Pickles/pickle1.p\", \"rb\"))\n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "featuresdf[::950]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([-2.4183440e+02,  7.5498520e+01, -8.5412226e+00,  8.3656340e+00,\n",
       "         -4.5100298e+00,  3.9749825e+00, -1.0074351e+00,  1.6417691e+00,\n",
       "         -1.9899381e+00,  2.0210383e+00, -1.7172641e+00,  2.8858922e+00,\n",
       "         -1.5965629e+00,  2.4626105e+00,  1.0557063e+00,  2.3589902e+00,\n",
       "         -3.9865655e-01,  2.6551561e+00, -3.8521993e-01,  1.9522010e+00,\n",
       "         -3.0949333e-01,  2.6241479e+00,  1.2644924e+00,  2.8620899e+00,\n",
       "         -2.7649137e-01,  1.4117364e+00,  2.2772765e-01,  1.6094500e+00,\n",
       "          1.0077388e+00,  2.0562439e+00,  6.8004107e-01,  1.0299205e+00,\n",
       "         -2.7629113e-01,  5.7498223e-01,  4.9020937e-01,  1.3876550e+00,\n",
       "          4.7108080e-02,  3.8061407e-01, -5.4256582e-01,  1.0440761e+00],\n",
       "        dtype=float32)],\n",
       " [array([-2.4183440e+02,  7.5498520e+01, -8.5412226e+00,  8.3656340e+00,\n",
       "         -4.5100298e+00,  3.9749825e+00, -1.0074351e+00,  1.6417691e+00,\n",
       "         -1.9899381e+00,  2.0210383e+00, -1.7172641e+00,  2.8858922e+00,\n",
       "         -1.5965629e+00,  2.4626105e+00,  1.0557063e+00,  2.3589902e+00,\n",
       "         -3.9865655e-01,  2.6551561e+00, -3.8521993e-01,  1.9522010e+00,\n",
       "         -3.0949333e-01,  2.6241479e+00,  1.2644924e+00,  2.8620899e+00,\n",
       "         -2.7649137e-01,  1.4117364e+00,  2.2772765e-01,  1.6094500e+00,\n",
       "          1.0077388e+00,  2.0562439e+00,  6.8004107e-01,  1.0299205e+00,\n",
       "         -2.7629113e-01,  5.7498223e-01,  4.9020937e-01,  1.3876550e+00,\n",
       "          4.7108080e-02,  3.8061407e-01, -5.4256582e-01,  1.0440761e+00],\n",
       "        dtype=float32)]]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_cp = []\n",
    "for f,c in features:\n",
    "    features_cp.append([ [[f],]*40, c ])\n",
    "    \n",
    "features_cp[12][0][0:2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[-272.93866, 78.29002, -5.7069273, 12.786187...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>[[[-605.18494, 94.289894, -12.569014, 18.86196...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>[[[-409.41293, 122.481445, -41.31422, 19.43990...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>[[[-602.84, 65.77663, -3.561166, 43.207966, -8...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>[[[-542.72516, 172.78352, -28.462677, 27.74007...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>[[[-427.63205, 72.44588, 10.115246, 27.494013,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                feature class_label\n",
       "0     [[[-272.93866, 78.29002, -5.7069273, 12.786187...       klapp\n",
       "950   [[[-605.18494, 94.289894, -12.569014, 18.86196...       klapp\n",
       "1900  [[[-409.41293, 122.481445, -41.31422, 19.43990...       klapp\n",
       "2850  [[[-602.84, 65.77663, -3.561166, 43.207966, -8...     noklapp\n",
       "3800  [[[-542.72516, 172.78352, -28.462677, 27.74007...     noklapp\n",
       "4750  [[[-427.63205, 72.44588, 10.115246, 27.494013,...     noklapp"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresdf = pd.DataFrame(features_cp, columns=['feature','class_label'])\n",
    "featuresdf[::950]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.swapaxes(np.array(featuresdf.feature.tolist()), 2,3)\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_45 (Conv2D)           (None, 39, 39, 16)        80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 19, 19, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 19, 19, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 18, 18, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_46 (MaxPooling (None, 9, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 9, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 8, 8, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 3, 3, 128)         32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_12  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 43,570\n",
      "Trainable params: 43,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "953/953 [==============================] - 1s 537us/step\n",
      "Pre-training accuracy: 50.9969%\n"
     ]
    }
   ],
   "source": [
    "num_rows = 40 #40\n",
    "num_columns = 40 #1\n",
    "num_channels = 1\n",
    "\n",
    "#x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "#x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in time:  0:00:50.693866\n",
      "Training Accuracy:  0.8569929152609825\n",
      "Testing Accuracy:  0.8121720888932379\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "num_batch_size = 256\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.basic_cnn.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=0)\n",
    "\n",
    "clear_output()\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)\n",
    "\n",
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in time:  0:01:03.642409\n",
      "Training Accuracy:  0.9092101810704815\n",
      "Testing Accuracy:  0.8205666324399326\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.basic_cnn.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=0)\n",
    "\n",
    "clear_output()\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)\n",
    "\n",
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2\n",
    "\n",
    "* Marker = einzelne Channel + Mono\n",
    "* Random Mono-Snippets\n",
    "* getMeanMFCC()\n",
    "* Architektur: von https://towardsdatascience.com/how-to-apply-machine-learning-and-deep-learning-methods-to-audio-analysis-615e286fcbbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-272.93866, 78.29002, -5.7069273, 12.786187, ...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>[-605.18494, 94.289894, -12.569014, 18.861961,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>[-409.41293, 122.481445, -41.31422, 19.439909,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>[-602.84, 65.77663, -3.561166, 43.207966, -8.1...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>[-542.72516, 172.78352, -28.462677, 27.740074,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>[-427.63205, 72.44588, 10.115246, 27.494013, -...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                feature class_label\n",
       "0     [-272.93866, 78.29002, -5.7069273, 12.786187, ...       klapp\n",
       "950   [-605.18494, 94.289894, -12.569014, 18.861961,...       klapp\n",
       "1900  [-409.41293, 122.481445, -41.31422, 19.439909,...       klapp\n",
       "2850  [-602.84, 65.77663, -3.561166, 43.207966, -8.1...     noklapp\n",
       "3800  [-542.72516, 172.78352, -28.462677, 27.740074,...     noklapp\n",
       "4750  [-427.63205, 72.44588, 10.115246, 27.494013, -...     noklapp"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pickle.load(open(\"Pickles/pickle1.p\", \"rb\"))\n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "featuresdf[::950]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(featuresdf.feature.tolist())\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (1, 256)                  10496     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (1, 256)                  0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (1, 256)                  0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (1, 256)                  65792     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (1, 256)                  0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (1, 256)                  0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (1, 2)                    514       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (1, 2)                    0         \n",
      "=================================================================\n",
      "Total params: 76,802\n",
      "Trainable params: 76,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Pre-training accuracy: 46.9045%\n"
     ]
    }
   ],
   "source": [
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "input_shape=(1,40)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "model.build(input_shape)\n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3811 samples, validate on 953 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Inputs to operation training_1/Adam/gradients/loss_4/activation_30_loss/clip_by_value_grad/Select of type Select must have the same size and shape.  Input 0: [32,2] != input 2: [1,2]\n\t [[{{node training_1/Adam/gradients/loss_4/activation_30_loss/clip_by_value_grad/Select}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-243aa4b09d23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.direnv/python-3.7.3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.direnv/python-3.7.3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.direnv/python-3.7.3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.direnv/python-3.7.3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.direnv/python-3.7.3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Inputs to operation training_1/Adam/gradients/loss_4/activation_30_loss/clip_by_value_grad/Select of type Select must have the same size and shape.  Input 0: [32,2] != input 2: [1,2]\n\t [[{{node training_1/Adam/gradients/loss_4/activation_30_loss/clip_by_value_grad/Select}}]]"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3\n",
    "\n",
    "* Marker = einzelne Channel + Mono\n",
    "* Random Mono-Snippets\n",
    "* getMFCC()\n",
    "* Architektur: CNN von https://medium.com/@mikesmales/sound-classification-using-deep-learning-8bc2aa1990b7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-401.91074, -364.88733, -400.43365, -425.726...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>[[-705.9435, -687.03906, -656.64685, -627.5050...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>[[-465.21176, -459.15497, -468.00946, -471.744...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>[[-611.73395, -593.7865, -598.858, -603.34607,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>[[-429.78662, -431.02716, -436.34952, -435.642...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>[[-364.97586, -353.12744, -347.85767, -353.366...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                feature class_label\n",
       "0     [[-401.91074, -364.88733, -400.43365, -425.726...       klapp\n",
       "950   [[-705.9435, -687.03906, -656.64685, -627.5050...       klapp\n",
       "1900  [[-465.21176, -459.15497, -468.00946, -471.744...       klapp\n",
       "2850  [[-611.73395, -593.7865, -598.858, -603.34607,...     noklapp\n",
       "3800  [[-429.78662, -431.02716, -436.34952, -435.642...     noklapp\n",
       "4750  [[-364.97586, -353.12744, -347.85767, -353.366...     noklapp"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pickle.load(open(\"Pickles/pickle2.p\", \"rb\"))\n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "featuresdf[::950]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = [np.expand_dims(features[0][0], axis=2)] #[] #np.array(featuresdf.feature.tolist())\n",
    "for f in features[1::]:\n",
    "    xi = np.expand_dims(f[0], axis=2)\n",
    "    X = np.append(X, [xi], axis=0)\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0319 09:59:22.148208 140192084776768 deprecation_wrapper.py:119] From /home/mf151/.direnv/python-3.7.3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0319 09:59:22.161520 140192084776768 deprecation_wrapper.py:119] From /home/mf151/.direnv/python-3.7.3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0319 09:59:22.163864 140192084776768 deprecation_wrapper.py:119] From /home/mf151/.direnv/python-3.7.3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0319 09:59:22.176325 140192084776768 deprecation_wrapper.py:119] From /home/mf151/.direnv/python-3.7.3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0319 09:59:22.178600 140192084776768 deprecation_wrapper.py:119] From /home/mf151/.direnv/python-3.7.3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0319 09:59:22.184828 140192084776768 deprecation.py:506] From /home/mf151/.direnv/python-3.7.3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "num_rows = 40 #40\n",
    "num_columns = 94 #1\n",
    "num_channels = 1\n",
    "\n",
    "#x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "#x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0319 09:59:27.987616 140192084776768 deprecation_wrapper.py:119] From /home/mf151/.direnv/python-3.7.3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0319 09:59:28.010834 140192084776768 deprecation_wrapper.py:119] From /home/mf151/.direnv/python-3.7.3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 39, 93, 16)        80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 19, 46, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 19, 46, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 18, 45, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 9, 22, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 9, 22, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 21, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 9, 128)         32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 43,570\n",
      "Trainable params: 43,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "951/951 [==============================] - 0s 279us/step\n",
      "Pre-training accuracy: 51.1041%\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0319 09:59:42.942686 140192084776768 deprecation.py:323] From /home/mf151/.direnv/python-3.7.3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3800 samples, validate on 951 samples\n",
      "Epoch 1/72\n",
      "3800/3800 [==============================] - 2s 435us/step - loss: 2.0006 - acc: 0.5550 - val_loss: 0.6759 - val_acc: 0.5636\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67589, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 2/72\n",
      "3800/3800 [==============================] - 1s 342us/step - loss: 0.7552 - acc: 0.6182 - val_loss: 0.5990 - val_acc: 0.6646\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.67589 to 0.59902, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 3/72\n",
      "3800/3800 [==============================] - 1s 343us/step - loss: 0.5779 - acc: 0.7003 - val_loss: 0.4980 - val_acc: 0.7907\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.59902 to 0.49803, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 4/72\n",
      "3800/3800 [==============================] - 1s 340us/step - loss: 0.4808 - acc: 0.7776 - val_loss: 0.4300 - val_acc: 0.8349\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49803 to 0.42998, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 5/72\n",
      "3800/3800 [==============================] - 1s 341us/step - loss: 0.4436 - acc: 0.8150 - val_loss: 0.3881 - val_acc: 0.8486\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.42998 to 0.38813, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 6/72\n",
      "3800/3800 [==============================] - 1s 342us/step - loss: 0.3786 - acc: 0.8439 - val_loss: 0.3053 - val_acc: 0.9222\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.38813 to 0.30525, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 7/72\n",
      "3800/3800 [==============================] - 1s 340us/step - loss: 0.3226 - acc: 0.8782 - val_loss: 0.2568 - val_acc: 0.9411\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.30525 to 0.25679, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 8/72\n",
      "3800/3800 [==============================] - 1s 340us/step - loss: 0.2663 - acc: 0.9016 - val_loss: 0.1957 - val_acc: 0.9558\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.25679 to 0.19568, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 9/72\n",
      "3800/3800 [==============================] - 1s 343us/step - loss: 0.2399 - acc: 0.9129 - val_loss: 0.2050 - val_acc: 0.9695\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.19568\n",
      "Epoch 10/72\n",
      "3800/3800 [==============================] - 1s 341us/step - loss: 0.2154 - acc: 0.9261 - val_loss: 0.1659 - val_acc: 0.9695\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.19568 to 0.16595, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 11/72\n",
      "3800/3800 [==============================] - 1s 340us/step - loss: 0.1974 - acc: 0.9297 - val_loss: 0.1634 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.16595 to 0.16345, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 12/72\n",
      "3800/3800 [==============================] - 1s 340us/step - loss: 0.1771 - acc: 0.9368 - val_loss: 0.1237 - val_acc: 0.9727\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.16345 to 0.12375, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 13/72\n",
      "3800/3800 [==============================] - 1s 339us/step - loss: 0.1636 - acc: 0.9463 - val_loss: 0.1294 - val_acc: 0.9758\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.12375\n",
      "Epoch 14/72\n",
      "3800/3800 [==============================] - 1s 337us/step - loss: 0.1590 - acc: 0.9432 - val_loss: 0.1215 - val_acc: 0.9758\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.12375 to 0.12154, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 15/72\n",
      "3800/3800 [==============================] - 1s 339us/step - loss: 0.1371 - acc: 0.9529 - val_loss: 0.1057 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.12154 to 0.10573, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 16/72\n",
      "3800/3800 [==============================] - 1s 342us/step - loss: 0.1393 - acc: 0.9534 - val_loss: 0.1596 - val_acc: 0.9706\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.10573\n",
      "Epoch 17/72\n",
      "3800/3800 [==============================] - 1s 340us/step - loss: 0.1276 - acc: 0.9566 - val_loss: 0.0873 - val_acc: 0.9811\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.10573 to 0.08734, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 18/72\n",
      "3800/3800 [==============================] - 1s 340us/step - loss: 0.1222 - acc: 0.9603 - val_loss: 0.1097 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.08734\n",
      "Epoch 19/72\n",
      "3800/3800 [==============================] - 1s 339us/step - loss: 0.1248 - acc: 0.9597 - val_loss: 0.0735 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.08734 to 0.07346, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 20/72\n",
      "3800/3800 [==============================] - 1s 339us/step - loss: 0.1138 - acc: 0.9645 - val_loss: 0.1040 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.07346\n",
      "Epoch 21/72\n",
      "3800/3800 [==============================] - 1s 340us/step - loss: 0.1138 - acc: 0.9658 - val_loss: 0.1078 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.07346\n",
      "Epoch 22/72\n",
      "3800/3800 [==============================] - 1s 345us/step - loss: 0.1104 - acc: 0.9653 - val_loss: 0.1105 - val_acc: 0.9727\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.07346\n",
      "Epoch 23/72\n",
      "3800/3800 [==============================] - 1s 344us/step - loss: 0.1100 - acc: 0.9637 - val_loss: 0.1055 - val_acc: 0.9811\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.07346\n",
      "Epoch 24/72\n",
      "3800/3800 [==============================] - 1s 344us/step - loss: 0.0960 - acc: 0.9653 - val_loss: 0.0949 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.07346\n",
      "Epoch 25/72\n",
      "3800/3800 [==============================] - 1s 346us/step - loss: 0.1165 - acc: 0.9637 - val_loss: 0.0957 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.07346\n",
      "Epoch 26/72\n",
      "3800/3800 [==============================] - 1s 345us/step - loss: 0.0951 - acc: 0.9695 - val_loss: 0.0824 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.07346\n",
      "Epoch 27/72\n",
      "3800/3800 [==============================] - 1s 339us/step - loss: 0.0923 - acc: 0.9695 - val_loss: 0.1082 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.07346\n",
      "Epoch 28/72\n",
      "3800/3800 [==============================] - 1s 340us/step - loss: 0.0905 - acc: 0.9697 - val_loss: 0.0765 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.07346\n",
      "Epoch 29/72\n",
      "3800/3800 [==============================] - 1s 342us/step - loss: 0.0921 - acc: 0.9684 - val_loss: 0.0756 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.07346\n",
      "Epoch 30/72\n",
      "3800/3800 [==============================] - 1s 338us/step - loss: 0.0866 - acc: 0.9724 - val_loss: 0.0808 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.07346\n",
      "Epoch 31/72\n",
      "3800/3800 [==============================] - 1s 339us/step - loss: 0.1049 - acc: 0.9658 - val_loss: 0.0779 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.07346\n",
      "Epoch 32/72\n",
      "3800/3800 [==============================] - 1s 341us/step - loss: 0.0958 - acc: 0.9668 - val_loss: 0.0622 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.07346 to 0.06223, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 33/72\n",
      "3800/3800 [==============================] - 1s 344us/step - loss: 0.0847 - acc: 0.9739 - val_loss: 0.0954 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.06223\n",
      "Epoch 34/72\n",
      "3800/3800 [==============================] - 1s 340us/step - loss: 0.0803 - acc: 0.9750 - val_loss: 0.0960 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.06223\n",
      "Epoch 35/72\n",
      "3800/3800 [==============================] - 1s 342us/step - loss: 0.0926 - acc: 0.9708 - val_loss: 0.0991 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.06223\n",
      "Epoch 36/72\n",
      "3800/3800 [==============================] - 1s 341us/step - loss: 0.0776 - acc: 0.9739 - val_loss: 0.0856 - val_acc: 0.9811\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.06223\n",
      "Epoch 37/72\n",
      "3800/3800 [==============================] - 1s 341us/step - loss: 0.0848 - acc: 0.9750 - val_loss: 0.0725 - val_acc: 0.9811\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.06223\n",
      "Epoch 38/72\n",
      "3800/3800 [==============================] - 1s 339us/step - loss: 0.0751 - acc: 0.9747 - val_loss: 0.0700 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.06223\n",
      "Epoch 39/72\n",
      "3800/3800 [==============================] - 1s 339us/step - loss: 0.0732 - acc: 0.9768 - val_loss: 0.0572 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.06223 to 0.05722, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 40/72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3800/3800 [==============================] - 1s 342us/step - loss: 0.0745 - acc: 0.9737 - val_loss: 0.0676 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.05722\n",
      "Epoch 41/72\n",
      "3800/3800 [==============================] - 1s 341us/step - loss: 0.0674 - acc: 0.9792 - val_loss: 0.0522 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.05722 to 0.05217, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 42/72\n",
      "3800/3800 [==============================] - 1s 341us/step - loss: 0.0727 - acc: 0.9755 - val_loss: 0.0831 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.05217\n",
      "Epoch 43/72\n",
      "3800/3800 [==============================] - 1s 340us/step - loss: 0.0716 - acc: 0.9750 - val_loss: 0.0638 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.05217\n",
      "Epoch 44/72\n",
      "3800/3800 [==============================] - 1s 341us/step - loss: 0.0697 - acc: 0.9750 - val_loss: 0.0892 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.05217\n",
      "Epoch 45/72\n",
      "3800/3800 [==============================] - 1s 341us/step - loss: 0.0681 - acc: 0.9784 - val_loss: 0.0780 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.05217\n",
      "Epoch 46/72\n",
      "3800/3800 [==============================] - 1s 341us/step - loss: 0.0669 - acc: 0.9768 - val_loss: 0.0763 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.05217\n",
      "Epoch 47/72\n",
      "3800/3800 [==============================] - 1s 343us/step - loss: 0.0560 - acc: 0.9808 - val_loss: 0.0425 - val_acc: 0.9874\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.05217 to 0.04246, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 48/72\n",
      "3800/3800 [==============================] - 1s 341us/step - loss: 0.0745 - acc: 0.9784 - val_loss: 0.0481 - val_acc: 0.9905\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.04246\n",
      "Epoch 49/72\n",
      "3800/3800 [==============================] - 1s 343us/step - loss: 0.0546 - acc: 0.9824 - val_loss: 0.0509 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.04246\n",
      "Epoch 50/72\n",
      "3800/3800 [==============================] - 1s 339us/step - loss: 0.0562 - acc: 0.9792 - val_loss: 0.0448 - val_acc: 0.9916\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.04246\n",
      "Epoch 51/72\n",
      "3800/3800 [==============================] - 1s 345us/step - loss: 0.0639 - acc: 0.9811 - val_loss: 0.0475 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.04246\n",
      "Epoch 52/72\n",
      "3800/3800 [==============================] - 1s 342us/step - loss: 0.0684 - acc: 0.9771 - val_loss: 0.0821 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.04246\n",
      "Epoch 53/72\n",
      "3800/3800 [==============================] - 1s 340us/step - loss: 0.0635 - acc: 0.9776 - val_loss: 0.1107 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.04246\n",
      "Epoch 54/72\n",
      "3800/3800 [==============================] - 1s 344us/step - loss: 0.0507 - acc: 0.9832 - val_loss: 0.0481 - val_acc: 0.9895\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.04246\n",
      "Epoch 55/72\n",
      "3800/3800 [==============================] - 1s 341us/step - loss: 0.0650 - acc: 0.9784 - val_loss: 0.0625 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.04246\n",
      "Epoch 56/72\n",
      "3800/3800 [==============================] - 1s 339us/step - loss: 0.0598 - acc: 0.9818 - val_loss: 0.0722 - val_acc: 0.9874\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.04246\n",
      "Epoch 57/72\n",
      "3800/3800 [==============================] - 1s 342us/step - loss: 0.0670 - acc: 0.9795 - val_loss: 0.0640 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.04246\n",
      "Epoch 58/72\n",
      "3800/3800 [==============================] - 1s 343us/step - loss: 0.0507 - acc: 0.9826 - val_loss: 0.0569 - val_acc: 0.9895\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.04246\n",
      "Epoch 59/72\n",
      "3800/3800 [==============================] - 1s 344us/step - loss: 0.0517 - acc: 0.9821 - val_loss: 0.0433 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.04246\n",
      "Epoch 60/72\n",
      "3800/3800 [==============================] - 1s 340us/step - loss: 0.0573 - acc: 0.9797 - val_loss: 0.0679 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.04246\n",
      "Epoch 61/72\n",
      "3800/3800 [==============================] - 1s 339us/step - loss: 0.0598 - acc: 0.9803 - val_loss: 0.0597 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.04246\n",
      "Epoch 62/72\n",
      "3800/3800 [==============================] - 1s 340us/step - loss: 0.0520 - acc: 0.9839 - val_loss: 0.0709 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.04246\n",
      "Epoch 63/72\n",
      "3800/3800 [==============================] - 1s 341us/step - loss: 0.0486 - acc: 0.9858 - val_loss: 0.0574 - val_acc: 0.9874\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.04246\n",
      "Epoch 64/72\n",
      "3800/3800 [==============================] - 1s 341us/step - loss: 0.0517 - acc: 0.9811 - val_loss: 0.0598 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.04246\n",
      "Epoch 65/72\n",
      "3800/3800 [==============================] - 1s 342us/step - loss: 0.0446 - acc: 0.9842 - val_loss: 0.0427 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.04246\n",
      "Epoch 66/72\n",
      "3800/3800 [==============================] - 1s 344us/step - loss: 0.0494 - acc: 0.9847 - val_loss: 0.0406 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.04246 to 0.04055, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 67/72\n",
      "3800/3800 [==============================] - 1s 341us/step - loss: 0.0495 - acc: 0.9824 - val_loss: 0.0425 - val_acc: 0.9874\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.04055\n",
      "Epoch 68/72\n",
      "3800/3800 [==============================] - 1s 340us/step - loss: 0.0575 - acc: 0.9821 - val_loss: 0.0460 - val_acc: 0.9874\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.04055\n",
      "Epoch 69/72\n",
      "3800/3800 [==============================] - 1s 342us/step - loss: 0.0418 - acc: 0.9853 - val_loss: 0.0298 - val_acc: 0.9895\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.04055 to 0.02983, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 70/72\n",
      "3800/3800 [==============================] - 1s 349us/step - loss: 0.0536 - acc: 0.9818 - val_loss: 0.0304 - val_acc: 0.9895\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.02983\n",
      "Epoch 71/72\n",
      "3800/3800 [==============================] - 1s 341us/step - loss: 0.0501 - acc: 0.9821 - val_loss: 0.0814 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.02983\n",
      "Epoch 72/72\n",
      "3800/3800 [==============================] - 1s 343us/step - loss: 0.0431 - acc: 0.9855 - val_loss: 0.0430 - val_acc: 0.9895\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.02983\n",
      "Training completed in time:  0:01:34.847943\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 72\n",
    "num_batch_size = 64\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9947368421052631\n",
      "Testing Accuracy:  0.9894847528916929\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('weights.model')\n",
    "model.save_weights(\"model.h5\")\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4\n",
    "\n",
    "* Marker = einzelne Channel + Mono => augmented position\n",
    "* Random Mono-Snippets\n",
    "* getMFCC()\n",
    "* Architektur: CNN von https://medium.com/@mikesmales/sound-classification-using-deep-learning-8bc2aa1990b7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-401.91074, -364.88733, -400.43365, -425.726...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>[[-560.4174, -548.76294, -535.0601, -538.0909,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>[[-546.99133, -537.48883, -538.3186, -539.6693...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>[[-546.2755, -549.9481, -560.5604, -550.44867,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>[[-705.9435, -687.03906, -656.64685, -627.5050...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>[[-420.05396, -422.36597, -434.14792, -440.575...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5700</th>\n",
       "      <td>[[-518.5563, -516.15, -525.5188, -525.7212, -5...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6650</th>\n",
       "      <td>[[-738.4582, -738.41833, -734.59064, -693.3244...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7600</th>\n",
       "      <td>[[-577.9406, -578.9035, -586.558, -593.03253, ...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8550</th>\n",
       "      <td>[[-582.8322, -582.97144, -599.4791, -608.0502,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9500</th>\n",
       "      <td>[[-293.2954, -305.46545, -314.32388, -306.2559...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10450</th>\n",
       "      <td>[[-665.52844, -660.41394, -654.5603, -655.3265...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11400</th>\n",
       "      <td>[[-643.62445, -626.3474, -603.1135, -608.6339,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12350</th>\n",
       "      <td>[[-289.85547, -286.97263, -273.6771, -266.7414...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13300</th>\n",
       "      <td>[[-635.03253, -634.2626, -650.99884, -673.9432...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14250</th>\n",
       "      <td>[[-737.1895, -740.51245, -746.04376, -734.9391...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15200</th>\n",
       "      <td>[[-500.34415, -493.04996, -496.99542, -502.768...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16150</th>\n",
       "      <td>[[-620.90466, -616.9345, -625.7172, -634.236, ...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17100</th>\n",
       "      <td>[[-550.0259, -542.6448, -549.0136, -560.2666, ...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18050</th>\n",
       "      <td>[[-414.64474, -416.33868, -427.55463, -439.226...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19000</th>\n",
       "      <td>[[-159.96072, -157.4643, -157.41324, -154.9835...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 feature class_label\n",
       "0      [[-401.91074, -364.88733, -400.43365, -425.726...       klapp\n",
       "950    [[-560.4174, -548.76294, -535.0601, -538.0909,...       klapp\n",
       "1900   [[-546.99133, -537.48883, -538.3186, -539.6693...       klapp\n",
       "2850   [[-546.2755, -549.9481, -560.5604, -550.44867,...       klapp\n",
       "3800   [[-705.9435, -687.03906, -656.64685, -627.5050...       klapp\n",
       "4750   [[-420.05396, -422.36597, -434.14792, -440.575...       klapp\n",
       "5700   [[-518.5563, -516.15, -525.5188, -525.7212, -5...       klapp\n",
       "6650   [[-738.4582, -738.41833, -734.59064, -693.3244...       klapp\n",
       "7600   [[-577.9406, -578.9035, -586.558, -593.03253, ...       klapp\n",
       "8550   [[-582.8322, -582.97144, -599.4791, -608.0502,...       klapp\n",
       "9500   [[-293.2954, -305.46545, -314.32388, -306.2559...       klapp\n",
       "10450  [[-665.52844, -660.41394, -654.5603, -655.3265...     noklapp\n",
       "11400  [[-643.62445, -626.3474, -603.1135, -608.6339,...     noklapp\n",
       "12350  [[-289.85547, -286.97263, -273.6771, -266.7414...     noklapp\n",
       "13300  [[-635.03253, -634.2626, -650.99884, -673.9432...     noklapp\n",
       "14250  [[-737.1895, -740.51245, -746.04376, -734.9391...     noklapp\n",
       "15200  [[-500.34415, -493.04996, -496.99542, -502.768...     noklapp\n",
       "16150  [[-620.90466, -616.9345, -625.7172, -634.236, ...     noklapp\n",
       "17100  [[-550.0259, -542.6448, -549.0136, -560.2666, ...     noklapp\n",
       "18050  [[-414.64474, -416.33868, -427.55463, -439.226...     noklapp\n",
       "19000  [[-159.96072, -157.4643, -157.41324, -154.9835...     noklapp"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pickle.load(open(\"Pickles/pickle3.p\", \"rb\"))\n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "featuresdf[::950]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = [np.expand_dims(features[0][0], axis=2)] #[] #np.array(featuresdf.feature.tolist())\n",
    "for f in features[1::]:\n",
    "    xi = np.expand_dims(f[0], axis=2)\n",
    "    X = np.append(X, [xi], axis=0)\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_rows = 40 #40\n",
    "num_columns = 94 #1\n",
    "num_channels = 1\n",
    "\n",
    "#x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "#x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 39, 93, 16)        80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 19, 46, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 19, 46, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 18, 45, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 9, 22, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 9, 22, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 21, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 4, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 9, 128)         32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 1, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 43,570\n",
      "Trainable params: 43,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3808/3808 [==============================] - 0s 107us/step\n",
      "Pre-training accuracy: 49.8950%\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15232 samples, validate on 3808 samples\n",
      "Epoch 1/72\n",
      "15232/15232 [==============================] - 2s 145us/step - loss: 1.1532 - acc: 0.6226 - val_loss: 0.4726 - val_acc: 0.8393\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.47258, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 2/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.4279 - acc: 0.8184 - val_loss: 0.3673 - val_acc: 0.8939\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.47258 to 0.36730, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 3/72\n",
      "15232/15232 [==============================] - 2s 124us/step - loss: 0.3381 - acc: 0.8712 - val_loss: 0.3033 - val_acc: 0.9123\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36730 to 0.30332, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 4/72\n",
      "15232/15232 [==============================] - 2s 124us/step - loss: 0.3152 - acc: 0.8836 - val_loss: 0.2986 - val_acc: 0.9152\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.30332 to 0.29858, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 5/72\n",
      "15232/15232 [==============================] - 2s 124us/step - loss: 0.3017 - acc: 0.8918 - val_loss: 0.2935 - val_acc: 0.9162\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.29858 to 0.29348, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 6/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.2847 - acc: 0.8952 - val_loss: 0.2816 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.29348 to 0.28157, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 7/72\n",
      "15232/15232 [==============================] - 2s 126us/step - loss: 0.2713 - acc: 0.9011 - val_loss: 0.2721 - val_acc: 0.9223\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.28157 to 0.27207, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 8/72\n",
      "15232/15232 [==============================] - 2s 126us/step - loss: 0.2603 - acc: 0.9057 - val_loss: 0.2374 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.27207 to 0.23738, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 9/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.2571 - acc: 0.9077 - val_loss: 0.2380 - val_acc: 0.9275\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.23738\n",
      "Epoch 10/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.2453 - acc: 0.9117 - val_loss: 0.2483 - val_acc: 0.9238\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.23738\n",
      "Epoch 11/72\n",
      "15232/15232 [==============================] - 2s 124us/step - loss: 0.2400 - acc: 0.9127 - val_loss: 0.2303 - val_acc: 0.9280\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.23738 to 0.23030, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 12/72\n",
      "15232/15232 [==============================] - 2s 124us/step - loss: 0.2384 - acc: 0.9140 - val_loss: 0.2726 - val_acc: 0.9286\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.23030\n",
      "Epoch 13/72\n",
      "15232/15232 [==============================] - 2s 124us/step - loss: 0.2354 - acc: 0.9148 - val_loss: 0.2441 - val_acc: 0.9296\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.23030\n",
      "Epoch 14/72\n",
      "15232/15232 [==============================] - 2s 126us/step - loss: 0.2302 - acc: 0.9164 - val_loss: 0.2715 - val_acc: 0.9123\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.23030\n",
      "Epoch 15/72\n",
      "15232/15232 [==============================] - 2s 126us/step - loss: 0.2250 - acc: 0.9194 - val_loss: 0.2290 - val_acc: 0.9265\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.23030 to 0.22899, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 16/72\n",
      "15232/15232 [==============================] - 2s 124us/step - loss: 0.2229 - acc: 0.9204 - val_loss: 0.2403 - val_acc: 0.9220\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.22899\n",
      "Epoch 17/72\n",
      "15232/15232 [==============================] - 2s 126us/step - loss: 0.2195 - acc: 0.9209 - val_loss: 0.2289 - val_acc: 0.9270\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.22899 to 0.22895, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 18/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.2162 - acc: 0.9232 - val_loss: 0.2208 - val_acc: 0.9349\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.22895 to 0.22080, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 19/72\n",
      "15232/15232 [==============================] - 2s 126us/step - loss: 0.2101 - acc: 0.9253 - val_loss: 0.2056 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.22080 to 0.20556, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 20/72\n",
      "15232/15232 [==============================] - 2s 124us/step - loss: 0.2129 - acc: 0.9226 - val_loss: 0.2342 - val_acc: 0.9262\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.20556\n",
      "Epoch 21/72\n",
      "15232/15232 [==============================] - 2s 126us/step - loss: 0.2094 - acc: 0.9242 - val_loss: 0.2188 - val_acc: 0.9322\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.20556\n",
      "Epoch 22/72\n",
      "15232/15232 [==============================] - 2s 126us/step - loss: 0.2053 - acc: 0.9269 - val_loss: 0.2038 - val_acc: 0.9322\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.20556 to 0.20383, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 23/72\n",
      "15232/15232 [==============================] - 2s 124us/step - loss: 0.2084 - acc: 0.9248 - val_loss: 0.2285 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.20383\n",
      "Epoch 24/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.1979 - acc: 0.9303 - val_loss: 0.2013 - val_acc: 0.9346\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.20383 to 0.20135, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 25/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.1989 - acc: 0.9307 - val_loss: 0.1998 - val_acc: 0.9346\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.20135 to 0.19983, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 26/72\n",
      "15232/15232 [==============================] - 2s 120us/step - loss: 0.1936 - acc: 0.9286 - val_loss: 0.2190 - val_acc: 0.9307\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.19983\n",
      "Epoch 27/72\n",
      "15232/15232 [==============================] - 1s 81us/step - loss: 0.1894 - acc: 0.9322 - val_loss: 0.2099 - val_acc: 0.9317\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.19983\n",
      "Epoch 28/72\n",
      "15232/15232 [==============================] - 1s 80us/step - loss: 0.1919 - acc: 0.9319 - val_loss: 0.1933 - val_acc: 0.9412\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.19983 to 0.19331, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 29/72\n",
      "15232/15232 [==============================] - 1s 79us/step - loss: 0.1890 - acc: 0.9319 - val_loss: 0.1970 - val_acc: 0.9351\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.19331\n",
      "Epoch 30/72\n",
      "15232/15232 [==============================] - 1s 77us/step - loss: 0.1854 - acc: 0.9334 - val_loss: 0.1918 - val_acc: 0.9378\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.19331 to 0.19182, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 31/72\n",
      "15232/15232 [==============================] - 2s 120us/step - loss: 0.1818 - acc: 0.9345 - val_loss: 0.1813 - val_acc: 0.9388\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.19182 to 0.18131, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 32/72\n",
      "15232/15232 [==============================] - 2s 123us/step - loss: 0.1836 - acc: 0.9336 - val_loss: 0.2075 - val_acc: 0.9278\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.18131\n",
      "Epoch 33/72\n",
      "15232/15232 [==============================] - 2s 126us/step - loss: 0.1847 - acc: 0.9340 - val_loss: 0.1795 - val_acc: 0.9372\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.18131 to 0.17952, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 34/72\n",
      "15232/15232 [==============================] - 2s 124us/step - loss: 0.1811 - acc: 0.9347 - val_loss: 0.1913 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.17952\n",
      "Epoch 35/72\n",
      "15232/15232 [==============================] - 2s 126us/step - loss: 0.1765 - acc: 0.9349 - val_loss: 0.1805 - val_acc: 0.9420\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.17952\n",
      "Epoch 36/72\n",
      "15232/15232 [==============================] - 2s 126us/step - loss: 0.1737 - acc: 0.9397 - val_loss: 0.1937 - val_acc: 0.9346\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.17952\n",
      "Epoch 37/72\n",
      "15232/15232 [==============================] - 2s 123us/step - loss: 0.1734 - acc: 0.9363 - val_loss: 0.1752 - val_acc: 0.9407\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.17952 to 0.17522, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 38/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.1708 - acc: 0.9377 - val_loss: 0.1813 - val_acc: 0.9435\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.17522\n",
      "Epoch 39/72\n",
      "15232/15232 [==============================] - 2s 126us/step - loss: 0.1689 - acc: 0.9387 - val_loss: 0.1663 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.17522 to 0.16631, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 40/72\n",
      "15232/15232 [==============================] - 2s 124us/step - loss: 0.1657 - acc: 0.9380 - val_loss: 0.1762 - val_acc: 0.9422\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.16631\n",
      "Epoch 41/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.1671 - acc: 0.9389 - val_loss: 0.1952 - val_acc: 0.9378\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.16631\n",
      "Epoch 42/72\n",
      "15232/15232 [==============================] - 2s 123us/step - loss: 0.1630 - acc: 0.9408 - val_loss: 0.1667 - val_acc: 0.9409\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.16631\n",
      "Epoch 43/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.1623 - acc: 0.9409 - val_loss: 0.1591 - val_acc: 0.9446\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.16631 to 0.15907, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 44/72\n",
      "15232/15232 [==============================] - 2s 126us/step - loss: 0.1626 - acc: 0.9397 - val_loss: 0.1605 - val_acc: 0.9435\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.15907\n",
      "Epoch 45/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.1608 - acc: 0.9422 - val_loss: 0.1790 - val_acc: 0.9417\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.15907\n",
      "Epoch 46/72\n",
      "15232/15232 [==============================] - 2s 127us/step - loss: 0.1502 - acc: 0.9445 - val_loss: 0.1722 - val_acc: 0.9435\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.15907\n",
      "Epoch 47/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.1535 - acc: 0.9436 - val_loss: 0.1636 - val_acc: 0.9443\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.15907\n",
      "Epoch 48/72\n",
      "15232/15232 [==============================] - 2s 124us/step - loss: 0.1474 - acc: 0.9450 - val_loss: 0.1619 - val_acc: 0.9451\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.15907\n",
      "Epoch 49/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.1524 - acc: 0.9439 - val_loss: 0.1627 - val_acc: 0.9430\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.15907\n",
      "Epoch 50/72\n",
      "15232/15232 [==============================] - 2s 123us/step - loss: 0.1549 - acc: 0.9429 - val_loss: 0.1692 - val_acc: 0.9449\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.15907\n",
      "Epoch 51/72\n",
      "15232/15232 [==============================] - 2s 123us/step - loss: 0.1490 - acc: 0.9444 - val_loss: 0.1641 - val_acc: 0.9483\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.15907\n",
      "Epoch 52/72\n",
      "15232/15232 [==============================] - 2s 126us/step - loss: 0.1481 - acc: 0.9449 - val_loss: 0.1526 - val_acc: 0.9470\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.15907 to 0.15265, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 53/72\n",
      "15232/15232 [==============================] - 2s 123us/step - loss: 0.1460 - acc: 0.9450 - val_loss: 0.1744 - val_acc: 0.9483\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.15265\n",
      "Epoch 54/72\n",
      "15232/15232 [==============================] - 2s 124us/step - loss: 0.1479 - acc: 0.9460 - val_loss: 0.1514 - val_acc: 0.9493\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.15265 to 0.15143, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 55/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.1424 - acc: 0.9472 - val_loss: 0.1878 - val_acc: 0.9435\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.15143\n",
      "Epoch 56/72\n",
      "15232/15232 [==============================] - 2s 124us/step - loss: 0.1386 - acc: 0.9489 - val_loss: 0.1442 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.15143 to 0.14425, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 57/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.1448 - acc: 0.9479 - val_loss: 0.1603 - val_acc: 0.9459\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.14425\n",
      "Epoch 58/72\n",
      "15232/15232 [==============================] - 2s 126us/step - loss: 0.1368 - acc: 0.9481 - val_loss: 0.1844 - val_acc: 0.9388\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.14425\n",
      "Epoch 59/72\n",
      "15232/15232 [==============================] - 2s 118us/step - loss: 0.1460 - acc: 0.9449 - val_loss: 0.1859 - val_acc: 0.9441\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.14425\n",
      "Epoch 60/72\n",
      "15232/15232 [==============================] - 1s 79us/step - loss: 0.1382 - acc: 0.9496 - val_loss: 0.1748 - val_acc: 0.9464\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.14425\n",
      "Epoch 61/72\n",
      "15232/15232 [==============================] - 1s 79us/step - loss: 0.1363 - acc: 0.9514 - val_loss: 0.1677 - val_acc: 0.9483\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.14425\n",
      "Epoch 62/72\n",
      "15232/15232 [==============================] - 1s 78us/step - loss: 0.1364 - acc: 0.9500 - val_loss: 0.1512 - val_acc: 0.9485\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.14425\n",
      "Epoch 63/72\n",
      "15232/15232 [==============================] - 1s 78us/step - loss: 0.1347 - acc: 0.9500 - val_loss: 0.1677 - val_acc: 0.9485\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.14425\n",
      "Epoch 64/72\n",
      "15232/15232 [==============================] - 2s 121us/step - loss: 0.1308 - acc: 0.9515 - val_loss: 0.1633 - val_acc: 0.9480\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.14425\n",
      "Epoch 65/72\n",
      "15232/15232 [==============================] - 2s 123us/step - loss: 0.1339 - acc: 0.9502 - val_loss: 0.1586 - val_acc: 0.9501\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.14425\n",
      "Epoch 66/72\n",
      "15232/15232 [==============================] - 2s 127us/step - loss: 0.1327 - acc: 0.9514 - val_loss: 0.1731 - val_acc: 0.9451\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.14425\n",
      "Epoch 67/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.1343 - acc: 0.9509 - val_loss: 0.1527 - val_acc: 0.9485\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.14425\n",
      "Epoch 68/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.1321 - acc: 0.9508 - val_loss: 0.1564 - val_acc: 0.9462\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.14425\n",
      "Epoch 69/72\n",
      "15232/15232 [==============================] - 2s 121us/step - loss: 0.1301 - acc: 0.9535 - val_loss: 0.1401 - val_acc: 0.9485\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.14425 to 0.14013, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 70/72\n",
      "15232/15232 [==============================] - 2s 125us/step - loss: 0.1272 - acc: 0.9521 - val_loss: 0.1413 - val_acc: 0.9506\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.14013\n",
      "Epoch 71/72\n",
      "15232/15232 [==============================] - 2s 124us/step - loss: 0.1291 - acc: 0.9515 - val_loss: 0.1534 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.14013\n",
      "Epoch 72/72\n",
      "15232/15232 [==============================] - 2s 122us/step - loss: 0.1300 - acc: 0.9534 - val_loss: 0.1637 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.14013\n",
      "Training completed in time:  0:02:12.950846\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 72\n",
    "num_batch_size = 64\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9810924369747899\n",
      "Testing Accuracy:  0.9477415966386554\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])\n",
    "!telegram-send \"server = fertig\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Models/m1/weights.model')\n",
    "model.save_weights(\"Models/m1/model.h5\")\n",
    "model_json = model.to_json()\n",
    "with open(\"Models/m1/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5\n",
    "\n",
    "* Marker = einzelne Channel + Mono => augmented position\n",
    "* Mono-Snippets von Maxima\n",
    "* getMFCC()\n",
    "* Architektur: CNN von https://medium.com/@mikesmales/sound-classification-using-deep-learning-8bc2aa1990b7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-401.91074, -364.88733, -400.43365, -425.726...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>[[-498.68527, -499.05252, -511.57742, -512.935...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>[[-459.3803, -435.96304, -426.47766, -421.5678...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>[[-155.8534, -152.13937, -165.1922, -188.13705...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>[[-580.1882, -579.0551, -560.98486, -523.3863,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>[[-570.72217, -578.7941, -598.3212, -607.0586,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5700</th>\n",
       "      <td>[[-449.0259, -443.26535, -443.9801, -455.3299,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6650</th>\n",
       "      <td>[[-624.7864, -624.66296, -634.1999, -633.471, ...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7600</th>\n",
       "      <td>[[-635.62146, -600.41113, -578.12714, -560.429...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8550</th>\n",
       "      <td>[[-331.40616, -327.403, -324.71365, -310.99008...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9500</th>\n",
       "      <td>[[-355.00485, -350.09186, -356.03943, -365.237...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10450</th>\n",
       "      <td>[[-587.58405, -585.3416, -584.0825, -583.39185...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11400</th>\n",
       "      <td>[[-405.09354, -394.8144, -390.5067, -379.76968...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12350</th>\n",
       "      <td>[[-344.39587, -339.50085, -336.12064, -341.370...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13300</th>\n",
       "      <td>[[-621.7667, -613.2909, -614.99927, -622.827, ...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 feature class_label\n",
       "0      [[-401.91074, -364.88733, -400.43365, -425.726...       klapp\n",
       "950    [[-498.68527, -499.05252, -511.57742, -512.935...     noklapp\n",
       "1900   [[-459.3803, -435.96304, -426.47766, -421.5678...     noklapp\n",
       "2850   [[-155.8534, -152.13937, -165.1922, -188.13705...     noklapp\n",
       "3800   [[-580.1882, -579.0551, -560.98486, -523.3863,...       klapp\n",
       "4750   [[-570.72217, -578.7941, -598.3212, -607.0586,...       klapp\n",
       "5700   [[-449.0259, -443.26535, -443.9801, -455.3299,...     noklapp\n",
       "6650   [[-624.7864, -624.66296, -634.1999, -633.471, ...       klapp\n",
       "7600   [[-635.62146, -600.41113, -578.12714, -560.429...     noklapp\n",
       "8550   [[-331.40616, -327.403, -324.71365, -310.99008...       klapp\n",
       "9500   [[-355.00485, -350.09186, -356.03943, -365.237...       klapp\n",
       "10450  [[-587.58405, -585.3416, -584.0825, -583.39185...       klapp\n",
       "11400  [[-405.09354, -394.8144, -390.5067, -379.76968...       klapp\n",
       "12350  [[-344.39587, -339.50085, -336.12064, -341.370...       klapp\n",
       "13300  [[-621.7667, -613.2909, -614.99927, -622.827, ...       klapp"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pickle.load(open(\"Pickles/pickle4.p\", \"rb\"))\n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "featuresdf[::950]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = [np.expand_dims(features[0][0], axis=2)] #[] #np.array(featuresdf.feature.tolist())\n",
    "for f in features[1::]:\n",
    "    xi = np.expand_dims(f[0], axis=2)\n",
    "    X = np.append(X, [xi], axis=0)\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\n",
    "!telegram-send \"Server: Features konvertiert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_rows = 40 #40\n",
    "num_columns = 94 #1\n",
    "num_channels = 1\n",
    "\n",
    "#x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "#x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = 2#yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 39, 93, 16)        80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 19, 46, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 19, 46, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 18, 45, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 9, 22, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 9, 22, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 21, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 4, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 3, 9, 128)         32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 1, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 43,570\n",
      "Trainable params: 43,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2802/2802 [==============================] - 0s 121us/step\n",
      "Pre-training accuracy: 46.7880%\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11208 samples, validate on 2802 samples\n",
      "Epoch 1/72\n",
      "11208/11208 [==============================] - 2s 155us/step - loss: 2.4008 - acc: 0.5927 - val_loss: 0.6342 - val_acc: 0.5921\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63415, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 2/72\n",
      "11208/11208 [==============================] - 1s 124us/step - loss: 0.5299 - acc: 0.7217 - val_loss: 0.4960 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63415 to 0.49599, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 3/72\n",
      "11208/11208 [==============================] - 1s 126us/step - loss: 0.4752 - acc: 0.7625 - val_loss: 0.4644 - val_acc: 0.8133\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49599 to 0.46437, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 4/72\n",
      "11208/11208 [==============================] - 1s 124us/step - loss: 0.4317 - acc: 0.7911 - val_loss: 0.3801 - val_acc: 0.8640\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.46437 to 0.38009, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 5/72\n",
      "11208/11208 [==============================] - 1s 125us/step - loss: 0.3763 - acc: 0.8275 - val_loss: 0.3253 - val_acc: 0.8901\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.38009 to 0.32530, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 6/72\n",
      "11208/11208 [==============================] - 1s 126us/step - loss: 0.3356 - acc: 0.8544 - val_loss: 0.3366 - val_acc: 0.8751\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.32530\n",
      "Epoch 7/72\n",
      "11208/11208 [==============================] - 1s 123us/step - loss: 0.3049 - acc: 0.8717 - val_loss: 0.3132 - val_acc: 0.8712\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.32530 to 0.31323, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 8/72\n",
      "11208/11208 [==============================] - 1s 123us/step - loss: 0.2870 - acc: 0.8788 - val_loss: 0.2835 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.31323 to 0.28345, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 9/72\n",
      "11208/11208 [==============================] - 1s 127us/step - loss: 0.2744 - acc: 0.8885 - val_loss: 0.2275 - val_acc: 0.9193\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.28345 to 0.22747, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 10/72\n",
      "11208/11208 [==============================] - 1s 122us/step - loss: 0.2495 - acc: 0.9002 - val_loss: 0.1993 - val_acc: 0.9340\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.22747 to 0.19933, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 11/72\n",
      "11208/11208 [==============================] - 1s 126us/step - loss: 0.2498 - acc: 0.8994 - val_loss: 0.2074 - val_acc: 0.9258\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.19933\n",
      "Epoch 12/72\n",
      "11208/11208 [==============================] - 1s 126us/step - loss: 0.2279 - acc: 0.9091 - val_loss: 0.2787 - val_acc: 0.8826\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.19933\n",
      "Epoch 13/72\n",
      "11208/11208 [==============================] - 1s 126us/step - loss: 0.2234 - acc: 0.9084 - val_loss: 0.2074 - val_acc: 0.9286\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.19933\n",
      "Epoch 14/72\n",
      "11208/11208 [==============================] - 1s 124us/step - loss: 0.2146 - acc: 0.9133 - val_loss: 0.2286 - val_acc: 0.9036\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.19933\n",
      "Epoch 15/72\n",
      "11208/11208 [==============================] - 1s 127us/step - loss: 0.2105 - acc: 0.9170 - val_loss: 0.2225 - val_acc: 0.9104\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.19933\n",
      "Epoch 16/72\n",
      "11208/11208 [==============================] - 1s 125us/step - loss: 0.2032 - acc: 0.9181 - val_loss: 0.2314 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.19933\n",
      "Epoch 17/72\n",
      "11208/11208 [==============================] - 1s 124us/step - loss: 0.2014 - acc: 0.9202 - val_loss: 0.2163 - val_acc: 0.9108\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.19933\n",
      "Epoch 18/72\n",
      "11208/11208 [==============================] - 1s 124us/step - loss: 0.1897 - acc: 0.9264 - val_loss: 0.1828 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.19933 to 0.18278, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 19/72\n",
      "11208/11208 [==============================] - 1s 125us/step - loss: 0.1958 - acc: 0.9210 - val_loss: 0.2028 - val_acc: 0.9165\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.18278\n",
      "Epoch 20/72\n",
      "11208/11208 [==============================] - 1s 103us/step - loss: 0.1840 - acc: 0.9278 - val_loss: 0.1998 - val_acc: 0.9251\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.18278\n",
      "Epoch 21/72\n",
      "11208/11208 [==============================] - 1s 74us/step - loss: 0.1762 - acc: 0.9309 - val_loss: 0.1667 - val_acc: 0.9404\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.18278 to 0.16666, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 22/72\n",
      "11208/11208 [==============================] - 1s 74us/step - loss: 0.1704 - acc: 0.9330 - val_loss: 0.1622 - val_acc: 0.9475\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.16666 to 0.16216, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 23/72\n",
      "11208/11208 [==============================] - 1s 75us/step - loss: 0.1693 - acc: 0.9354 - val_loss: 0.1800 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.16216\n",
      "Epoch 24/72\n",
      "11208/11208 [==============================] - 1s 77us/step - loss: 0.1717 - acc: 0.9351 - val_loss: 0.1764 - val_acc: 0.9297\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.16216\n",
      "Epoch 25/72\n",
      "11208/11208 [==============================] - 1s 81us/step - loss: 0.1566 - acc: 0.9376 - val_loss: 0.1555 - val_acc: 0.9383\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.16216 to 0.15547, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 26/72\n",
      "11208/11208 [==============================] - 1s 110us/step - loss: 0.1568 - acc: 0.9398 - val_loss: 0.1397 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.15547 to 0.13965, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 27/72\n",
      "11208/11208 [==============================] - 1s 123us/step - loss: 0.1541 - acc: 0.9418 - val_loss: 0.1238 - val_acc: 0.9554\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.13965 to 0.12380, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 28/72\n",
      "11208/11208 [==============================] - 1s 125us/step - loss: 0.1497 - acc: 0.9428 - val_loss: 0.1815 - val_acc: 0.9204\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.12380\n",
      "Epoch 29/72\n",
      "11208/11208 [==============================] - 1s 125us/step - loss: 0.1428 - acc: 0.9470 - val_loss: 0.1254 - val_acc: 0.9565\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.12380\n",
      "Epoch 30/72\n",
      "11208/11208 [==============================] - 1s 126us/step - loss: 0.1443 - acc: 0.9451 - val_loss: 0.1376 - val_acc: 0.9507\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.12380\n",
      "Epoch 31/72\n",
      "11208/11208 [==============================] - 1s 124us/step - loss: 0.1480 - acc: 0.9441 - val_loss: 0.1347 - val_acc: 0.9522\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.12380\n",
      "Epoch 32/72\n",
      "11208/11208 [==============================] - 1s 125us/step - loss: 0.1429 - acc: 0.9442 - val_loss: 0.1150 - val_acc: 0.9625\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.12380 to 0.11498, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 33/72\n",
      "11208/11208 [==============================] - 1s 125us/step - loss: 0.1331 - acc: 0.9490 - val_loss: 0.1149 - val_acc: 0.9575\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.11498 to 0.11489, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 34/72\n",
      "11208/11208 [==============================] - 1s 123us/step - loss: 0.1262 - acc: 0.9502 - val_loss: 0.1036 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.11489 to 0.10361, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 35/72\n",
      "11208/11208 [==============================] - 1s 122us/step - loss: 0.1404 - acc: 0.9460 - val_loss: 0.1133 - val_acc: 0.9597\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.10361\n",
      "Epoch 36/72\n",
      "11208/11208 [==============================] - 1s 125us/step - loss: 0.1286 - acc: 0.9499 - val_loss: 0.0945 - val_acc: 0.9636\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.10361 to 0.09453, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 37/72\n",
      "11208/11208 [==============================] - 1s 126us/step - loss: 0.1237 - acc: 0.9521 - val_loss: 0.1223 - val_acc: 0.9561\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.09453\n",
      "Epoch 38/72\n",
      "11208/11208 [==============================] - 1s 126us/step - loss: 0.1276 - acc: 0.9484 - val_loss: 0.1011 - val_acc: 0.9650\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.09453\n",
      "Epoch 39/72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11208/11208 [==============================] - 1s 125us/step - loss: 0.1244 - acc: 0.9532 - val_loss: 0.1098 - val_acc: 0.9607\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09453\n",
      "Epoch 40/72\n",
      "11208/11208 [==============================] - 1s 124us/step - loss: 0.1318 - acc: 0.9499 - val_loss: 0.1060 - val_acc: 0.9636\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.09453\n",
      "Epoch 41/72\n",
      "11208/11208 [==============================] - 1s 122us/step - loss: 0.1203 - acc: 0.9546 - val_loss: 0.1155 - val_acc: 0.9622\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.09453\n",
      "Epoch 42/72\n",
      "11208/11208 [==============================] - 1s 124us/step - loss: 0.1162 - acc: 0.9578 - val_loss: 0.1091 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.09453\n",
      "Epoch 43/72\n",
      "11208/11208 [==============================] - 1s 125us/step - loss: 0.1164 - acc: 0.9569 - val_loss: 0.1070 - val_acc: 0.9604\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09453\n",
      "Epoch 44/72\n",
      "11208/11208 [==============================] - 1s 125us/step - loss: 0.1256 - acc: 0.9514 - val_loss: 0.0862 - val_acc: 0.9686\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.09453 to 0.08619, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 45/72\n",
      "11208/11208 [==============================] - 1s 125us/step - loss: 0.1148 - acc: 0.9585 - val_loss: 0.1283 - val_acc: 0.9497\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.08619\n",
      "Epoch 46/72\n",
      "11208/11208 [==============================] - 1s 123us/step - loss: 0.1142 - acc: 0.9565 - val_loss: 0.1329 - val_acc: 0.9522\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.08619\n",
      "Epoch 47/72\n",
      "11208/11208 [==============================] - 1s 124us/step - loss: 0.1106 - acc: 0.9585 - val_loss: 0.0831 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.08619 to 0.08306, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 48/72\n",
      "11208/11208 [==============================] - 1s 122us/step - loss: 0.1078 - acc: 0.9606 - val_loss: 0.1543 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.08306\n",
      "Epoch 49/72\n",
      "11208/11208 [==============================] - 1s 124us/step - loss: 0.1079 - acc: 0.9608 - val_loss: 0.1129 - val_acc: 0.9582\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.08306\n",
      "Epoch 50/72\n",
      "11208/11208 [==============================] - 1s 124us/step - loss: 0.1001 - acc: 0.9615 - val_loss: 0.0822 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.08306 to 0.08218, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 51/72\n",
      "11208/11208 [==============================] - 1s 123us/step - loss: 0.1096 - acc: 0.9563 - val_loss: 0.1069 - val_acc: 0.9604\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.08218\n",
      "Epoch 52/72\n",
      "11208/11208 [==============================] - 1s 124us/step - loss: 0.1032 - acc: 0.9599 - val_loss: 0.0932 - val_acc: 0.9657\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.08218\n",
      "Epoch 53/72\n",
      "11208/11208 [==============================] - 1s 124us/step - loss: 0.0991 - acc: 0.9624 - val_loss: 0.0802 - val_acc: 0.9707\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.08218 to 0.08024, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 54/72\n",
      "11208/11208 [==============================] - 1s 124us/step - loss: 0.1000 - acc: 0.9622 - val_loss: 0.0921 - val_acc: 0.9675\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.08024\n",
      "Epoch 55/72\n",
      "11208/11208 [==============================] - 1s 125us/step - loss: 0.0973 - acc: 0.9632 - val_loss: 0.0918 - val_acc: 0.9693\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.08024\n",
      "Epoch 56/72\n",
      "11208/11208 [==============================] - 1s 125us/step - loss: 0.0949 - acc: 0.9648 - val_loss: 0.0712 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.08024 to 0.07125, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 57/72\n",
      "11208/11208 [==============================] - 1s 123us/step - loss: 0.0984 - acc: 0.9632 - val_loss: 0.0705 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.07125 to 0.07051, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 58/72\n",
      "11208/11208 [==============================] - 1s 126us/step - loss: 0.0985 - acc: 0.9632 - val_loss: 0.0768 - val_acc: 0.9732\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.07051\n",
      "Epoch 59/72\n",
      "11208/11208 [==============================] - 1s 125us/step - loss: 0.0965 - acc: 0.9671 - val_loss: 0.0760 - val_acc: 0.9739\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.07051\n",
      "Epoch 60/72\n",
      "11208/11208 [==============================] - 1s 126us/step - loss: 0.0899 - acc: 0.9673 - val_loss: 0.0853 - val_acc: 0.9697\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.07051\n",
      "Epoch 61/72\n",
      "11208/11208 [==============================] - 1s 125us/step - loss: 0.0907 - acc: 0.9650 - val_loss: 0.0852 - val_acc: 0.9739\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.07051\n",
      "Epoch 62/72\n",
      "11208/11208 [==============================] - 1s 126us/step - loss: 0.0992 - acc: 0.9623 - val_loss: 0.1072 - val_acc: 0.9554\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.07051\n",
      "Epoch 63/72\n",
      "11208/11208 [==============================] - 1s 127us/step - loss: 0.0801 - acc: 0.9705 - val_loss: 0.0716 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.07051\n",
      "Epoch 64/72\n",
      "11208/11208 [==============================] - 1s 124us/step - loss: 0.0858 - acc: 0.9679 - val_loss: 0.0758 - val_acc: 0.9714\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.07051\n",
      "Epoch 65/72\n",
      "11208/11208 [==============================] - 1s 93us/step - loss: 0.0883 - acc: 0.9667 - val_loss: 0.0680 - val_acc: 0.9786\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.07051 to 0.06797, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 66/72\n",
      "11208/11208 [==============================] - 1s 77us/step - loss: 0.0798 - acc: 0.9714 - val_loss: 0.0741 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.06797\n",
      "Epoch 67/72\n",
      "11208/11208 [==============================] - 1s 78us/step - loss: 0.0830 - acc: 0.9679 - val_loss: 0.0815 - val_acc: 0.9690\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.06797\n",
      "Epoch 68/72\n",
      "11208/11208 [==============================] - 1s 79us/step - loss: 0.0874 - acc: 0.9682 - val_loss: 0.0945 - val_acc: 0.9650\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.06797\n",
      "Epoch 69/72\n",
      "11208/11208 [==============================] - 1s 83us/step - loss: 0.0876 - acc: 0.9681 - val_loss: 0.1166 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.06797\n",
      "Epoch 70/72\n",
      "11208/11208 [==============================] - 1s 88us/step - loss: 0.0939 - acc: 0.9646 - val_loss: 0.0683 - val_acc: 0.9732\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.06797\n",
      "Epoch 71/72\n",
      "11208/11208 [==============================] - 1s 123us/step - loss: 0.0807 - acc: 0.9708 - val_loss: 0.0790 - val_acc: 0.9707\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.06797\n",
      "Epoch 72/72\n",
      "11208/11208 [==============================] - 1s 126us/step - loss: 0.0816 - acc: 0.9702 - val_loss: 0.0813 - val_acc: 0.9690\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.06797\n",
      "Training completed in time:  0:01:36.781816\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "num_epochs = 72\n",
    "num_batch_size = 64\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)\n",
    "!telegram-send \"Server: Habe alles gelernt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9908101356174162\n",
      "Testing Accuracy:  0.9689507492944907\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])\n",
    "!telegram-send \"Server: Habe alles evaluiert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Models/m3/weights.model')\n",
    "model.save_weights(\"Models/m3/model.h5\")\n",
    "model_json = model.to_json()\n",
    "with open(\"Models/m3/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5 - Part 2\n",
    "\n",
    "* Marker = einzelne Channel + Mono => augmented position\n",
    "* Mono-Snippets von Maxima\n",
    "* getMFCC()\n",
    "* Architektur: CNN von https://medium.com/@mikesmales/sound-classification-using-deep-learning-8bc2aa1990b7\n",
    "\n",
    "=> VERBESSERTE DATEN, KEINE FEHLER MEHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-401.91074, -364.88733, -400.43365, -425.726...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>[[-529.0275, -530.5678, -539.6287, -504.5892, ...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>[[-611.8759, -607.2732, -600.64984, -602.05817...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>[[-549.2779, -537.589, -535.74164, -534.98505,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>[[-355.80963, -372.6017, -409.63934, -429.1113...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>[[-663.6253, -660.92163, -667.2111, -665.5719,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5700</th>\n",
       "      <td>[[-583.66675, -575.15375, -569.56854, -551.033...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6650</th>\n",
       "      <td>[[-503.17474, -503.21194, -509.01248, -523.404...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7600</th>\n",
       "      <td>[[-601.5306, -597.71576, -598.6808, -594.0496,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8550</th>\n",
       "      <td>[[-291.71292, -300.8574, -311.50537, -264.5054...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9500</th>\n",
       "      <td>[[-427.89334, -414.8279, -416.8952, -422.65714...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10450</th>\n",
       "      <td>[[-595.82294, -480.49725, -442.06833, -451.387...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11400</th>\n",
       "      <td>[[-413.59818, -384.62302, -414.8158, -464.578,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12350</th>\n",
       "      <td>[[-267.77267, -274.80542, -290.8901, -308.5314...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13300</th>\n",
       "      <td>[[-505.9942, -500.74725, -507.98242, -514.6446...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14250</th>\n",
       "      <td>[[-611.00336, -609.4228, -616.0314, -610.869, ...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 feature class_label\n",
       "0      [[-401.91074, -364.88733, -400.43365, -425.726...       klapp\n",
       "950    [[-529.0275, -530.5678, -539.6287, -504.5892, ...       klapp\n",
       "1900   [[-611.8759, -607.2732, -600.64984, -602.05817...       klapp\n",
       "2850   [[-549.2779, -537.589, -535.74164, -534.98505,...     noklapp\n",
       "3800   [[-355.80963, -372.6017, -409.63934, -429.1113...       klapp\n",
       "4750   [[-663.6253, -660.92163, -667.2111, -665.5719,...       klapp\n",
       "5700   [[-583.66675, -575.15375, -569.56854, -551.033...     noklapp\n",
       "6650   [[-503.17474, -503.21194, -509.01248, -523.404...     noklapp\n",
       "7600   [[-601.5306, -597.71576, -598.6808, -594.0496,...     noklapp\n",
       "8550   [[-291.71292, -300.8574, -311.50537, -264.5054...     noklapp\n",
       "9500   [[-427.89334, -414.8279, -416.8952, -422.65714...       klapp\n",
       "10450  [[-595.82294, -480.49725, -442.06833, -451.387...     noklapp\n",
       "11400  [[-413.59818, -384.62302, -414.8158, -464.578,...     noklapp\n",
       "12350  [[-267.77267, -274.80542, -290.8901, -308.5314...       klapp\n",
       "13300  [[-505.9942, -500.74725, -507.98242, -514.6446...       klapp\n",
       "14250  [[-611.00336, -609.4228, -616.0314, -610.869, ...       klapp"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pickle.load(open(\"Pickles/pickle4-1.p\", \"rb\"))\n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "featuresdf[::950]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15199"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = [np.expand_dims(features[0][0], axis=2)] #[] #np.array(featuresdf.feature.tolist())\n",
    "for f in features[1::]:\n",
    "    xi = np.expand_dims(f[0], axis=2)\n",
    "    X = np.append(X, [xi], axis=0)\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\n",
    "!telegram-send \"Server: Features konvertiert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_rows = 40 #40\n",
    "num_columns = 94 #1\n",
    "num_channels = 1\n",
    "\n",
    "#x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "#x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = 2#yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 39, 93, 16)        80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 19, 46, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 19, 46, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 18, 45, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 9, 22, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 9, 22, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 8, 21, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 4, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 4, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 3, 9, 128)         32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 1, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 1, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 43,570\n",
      "Trainable params: 43,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3040/3040 [==============================] - 0s 154us/step\n",
      "Pre-training accuracy: 46.7763%\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12159 samples, validate on 3040 samples\n",
      "Epoch 1/72\n",
      "12159/12159 [==============================] - 2s 157us/step - loss: 0.7930 - acc: 0.6318 - val_loss: 0.5136 - val_acc: 0.7605\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51356, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 2/72\n",
      "12159/12159 [==============================] - 1s 121us/step - loss: 0.4799 - acc: 0.7598 - val_loss: 0.3544 - val_acc: 0.8533\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51356 to 0.35437, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 3/72\n",
      "12159/12159 [==============================] - 1s 121us/step - loss: 0.3871 - acc: 0.8217 - val_loss: 0.3773 - val_acc: 0.8046\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35437\n",
      "Epoch 4/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.3428 - acc: 0.8467 - val_loss: 0.3053 - val_acc: 0.8747\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35437 to 0.30527, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 5/72\n",
      "12159/12159 [==============================] - 2s 124us/step - loss: 0.3084 - acc: 0.8663 - val_loss: 0.3285 - val_acc: 0.8520\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.30527\n",
      "Epoch 6/72\n",
      "12159/12159 [==============================] - 1s 92us/step - loss: 0.2862 - acc: 0.8760 - val_loss: 0.2492 - val_acc: 0.8980\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.30527 to 0.24917, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 7/72\n",
      "12159/12159 [==============================] - 1s 74us/step - loss: 0.2587 - acc: 0.8911 - val_loss: 0.2665 - val_acc: 0.8776\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.24917\n",
      "Epoch 8/72\n",
      "12159/12159 [==============================] - 1s 74us/step - loss: 0.2489 - acc: 0.8956 - val_loss: 0.2554 - val_acc: 0.8819\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.24917\n",
      "Epoch 9/72\n",
      "12159/12159 [==============================] - 1s 73us/step - loss: 0.2445 - acc: 0.8956 - val_loss: 0.2874 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.24917\n",
      "Epoch 10/72\n",
      "12159/12159 [==============================] - 1s 74us/step - loss: 0.2275 - acc: 0.9057 - val_loss: 0.1942 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.24917 to 0.19420, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 11/72\n",
      "12159/12159 [==============================] - 1s 83us/step - loss: 0.2211 - acc: 0.9076 - val_loss: 0.1728 - val_acc: 0.9316\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.19420 to 0.17282, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 12/72\n",
      "12159/12159 [==============================] - 1s 117us/step - loss: 0.2162 - acc: 0.9107 - val_loss: 0.2399 - val_acc: 0.8924\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.17282\n",
      "Epoch 13/72\n",
      "12159/12159 [==============================] - 1s 120us/step - loss: 0.2070 - acc: 0.9164 - val_loss: 0.1733 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.17282\n",
      "Epoch 14/72\n",
      "12159/12159 [==============================] - 2s 124us/step - loss: 0.1999 - acc: 0.9153 - val_loss: 0.1931 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.17282\n",
      "Epoch 15/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.1996 - acc: 0.9188 - val_loss: 0.2200 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.17282\n",
      "Epoch 16/72\n",
      "12159/12159 [==============================] - 1s 123us/step - loss: 0.1815 - acc: 0.9277 - val_loss: 0.1640 - val_acc: 0.9405\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.17282 to 0.16395, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 17/72\n",
      "12159/12159 [==============================] - 1s 123us/step - loss: 0.1823 - acc: 0.9242 - val_loss: 0.2260 - val_acc: 0.8964\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.16395\n",
      "Epoch 18/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.1732 - acc: 0.9294 - val_loss: 0.1932 - val_acc: 0.9174\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.16395\n",
      "Epoch 19/72\n",
      "12159/12159 [==============================] - 1s 123us/step - loss: 0.1720 - acc: 0.9315 - val_loss: 0.1769 - val_acc: 0.9283\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.16395\n",
      "Epoch 20/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.1710 - acc: 0.9322 - val_loss: 0.1275 - val_acc: 0.9543\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.16395 to 0.12748, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 21/72\n",
      "12159/12159 [==============================] - 1s 123us/step - loss: 0.1582 - acc: 0.9355 - val_loss: 0.1669 - val_acc: 0.9322\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.12748\n",
      "Epoch 22/72\n",
      "12159/12159 [==============================] - 2s 124us/step - loss: 0.1541 - acc: 0.9386 - val_loss: 0.1367 - val_acc: 0.9480\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.12748\n",
      "Epoch 23/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.1560 - acc: 0.9400 - val_loss: 0.1198 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.12748 to 0.11979, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 24/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.1451 - acc: 0.9436 - val_loss: 0.1422 - val_acc: 0.9411\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.11979\n",
      "Epoch 25/72\n",
      "12159/12159 [==============================] - 2s 125us/step - loss: 0.1412 - acc: 0.9443 - val_loss: 0.1108 - val_acc: 0.9628\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.11979 to 0.11075, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 26/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.1569 - acc: 0.9397 - val_loss: 0.1556 - val_acc: 0.9368\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.11075\n",
      "Epoch 27/72\n",
      "12159/12159 [==============================] - 2s 124us/step - loss: 0.1318 - acc: 0.9460 - val_loss: 0.1280 - val_acc: 0.9437\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.11075\n",
      "Epoch 28/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.1349 - acc: 0.9448 - val_loss: 0.1205 - val_acc: 0.9572\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.11075\n",
      "Epoch 29/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.1337 - acc: 0.9477 - val_loss: 0.1296 - val_acc: 0.9530\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.11075\n",
      "Epoch 30/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.1255 - acc: 0.9509 - val_loss: 0.1280 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.11075\n",
      "Epoch 31/72\n",
      "12159/12159 [==============================] - 1s 123us/step - loss: 0.1287 - acc: 0.9493 - val_loss: 0.1620 - val_acc: 0.9303\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.11075\n",
      "Epoch 32/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.1262 - acc: 0.9513 - val_loss: 0.1233 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.11075\n",
      "Epoch 33/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.1212 - acc: 0.9518 - val_loss: 0.1410 - val_acc: 0.9391\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.11075\n",
      "Epoch 34/72\n",
      "12159/12159 [==============================] - 2s 124us/step - loss: 0.1205 - acc: 0.9526 - val_loss: 0.0992 - val_acc: 0.9651\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.11075 to 0.09924, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 35/72\n",
      "12159/12159 [==============================] - 1s 121us/step - loss: 0.1163 - acc: 0.9547 - val_loss: 0.1068 - val_acc: 0.9602\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.09924\n",
      "Epoch 36/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.1185 - acc: 0.9530 - val_loss: 0.1199 - val_acc: 0.9530\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.09924\n",
      "Epoch 37/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.1158 - acc: 0.9571 - val_loss: 0.1002 - val_acc: 0.9661\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.09924\n",
      "Epoch 38/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.1140 - acc: 0.9558 - val_loss: 0.1500 - val_acc: 0.9342\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.09924\n",
      "Epoch 39/72\n",
      "12159/12159 [==============================] - 1s 114us/step - loss: 0.1102 - acc: 0.9586 - val_loss: 0.1224 - val_acc: 0.9533\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09924\n",
      "Epoch 40/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.1111 - acc: 0.9574 - val_loss: 0.1027 - val_acc: 0.9638\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.09924\n",
      "Epoch 41/72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12159/12159 [==============================] - 1s 123us/step - loss: 0.0998 - acc: 0.9613 - val_loss: 0.1073 - val_acc: 0.9582\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.09924\n",
      "Epoch 42/72\n",
      "12159/12159 [==============================] - 1s 118us/step - loss: 0.1061 - acc: 0.9580 - val_loss: 0.0920 - val_acc: 0.9674\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.09924 to 0.09198, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 43/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.1003 - acc: 0.9613 - val_loss: 0.0862 - val_acc: 0.9691\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.09198 to 0.08624, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 44/72\n",
      "12159/12159 [==============================] - 1s 123us/step - loss: 0.0965 - acc: 0.9617 - val_loss: 0.0797 - val_acc: 0.9711\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.08624 to 0.07968, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 45/72\n",
      "12159/12159 [==============================] - 1s 121us/step - loss: 0.1007 - acc: 0.9604 - val_loss: 0.0982 - val_acc: 0.9641\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.07968\n",
      "Epoch 46/72\n",
      "12159/12159 [==============================] - 1s 123us/step - loss: 0.0924 - acc: 0.9603 - val_loss: 0.0933 - val_acc: 0.9635\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.07968\n",
      "Epoch 47/72\n",
      "12159/12159 [==============================] - 1s 122us/step - loss: 0.0994 - acc: 0.9635 - val_loss: 0.0953 - val_acc: 0.9622\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.07968\n",
      "Epoch 48/72\n",
      "12159/12159 [==============================] - 1s 107us/step - loss: 0.0925 - acc: 0.9652 - val_loss: 0.0859 - val_acc: 0.9691\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.07968\n",
      "Epoch 49/72\n",
      "12159/12159 [==============================] - 1s 79us/step - loss: 0.0939 - acc: 0.9646 - val_loss: 0.0637 - val_acc: 0.9799\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.07968 to 0.06372, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 50/72\n",
      "12159/12159 [==============================] - 1s 77us/step - loss: 0.0925 - acc: 0.9634 - val_loss: 0.0697 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.06372\n",
      "Epoch 51/72\n",
      "12159/12159 [==============================] - 1s 78us/step - loss: 0.0859 - acc: 0.9664 - val_loss: 0.0692 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.06372\n",
      "Epoch 52/72\n",
      "12159/12159 [==============================] - 1s 78us/step - loss: 0.0843 - acc: 0.9664 - val_loss: 0.1313 - val_acc: 0.9424\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.06372\n",
      "Epoch 53/72\n",
      "12159/12159 [==============================] - 1s 78us/step - loss: 0.0809 - acc: 0.9696 - val_loss: 0.0795 - val_acc: 0.9697\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.06372\n",
      "Epoch 54/72\n",
      "12159/12159 [==============================] - 1s 117us/step - loss: 0.0849 - acc: 0.9680 - val_loss: 0.0802 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.06372\n",
      "Epoch 55/72\n",
      "12159/12159 [==============================] - 1s 123us/step - loss: 0.0897 - acc: 0.9653 - val_loss: 0.0742 - val_acc: 0.9720\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.06372\n",
      "Epoch 56/72\n",
      "12159/12159 [==============================] - 1s 121us/step - loss: 0.0871 - acc: 0.9661 - val_loss: 0.0687 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.06372\n",
      "Epoch 57/72\n",
      "12159/12159 [==============================] - 1s 114us/step - loss: 0.0837 - acc: 0.9683 - val_loss: 0.0983 - val_acc: 0.9622\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.06372\n",
      "Epoch 58/72\n",
      "12159/12159 [==============================] - 1s 112us/step - loss: 0.0813 - acc: 0.9678 - val_loss: 0.1031 - val_acc: 0.9586\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.06372\n",
      "Epoch 59/72\n",
      "12159/12159 [==============================] - 1s 112us/step - loss: 0.0837 - acc: 0.9694 - val_loss: 0.0651 - val_acc: 0.9796\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.06372\n",
      "Epoch 60/72\n",
      "12159/12159 [==============================] - 1s 110us/step - loss: 0.0731 - acc: 0.9726 - val_loss: 0.0717 - val_acc: 0.9730\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.06372\n",
      "Epoch 61/72\n",
      "12159/12159 [==============================] - 1s 110us/step - loss: 0.0774 - acc: 0.9692 - val_loss: 0.0682 - val_acc: 0.9724\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.06372\n",
      "Epoch 62/72\n",
      "12159/12159 [==============================] - 1s 113us/step - loss: 0.0695 - acc: 0.9726 - val_loss: 0.0655 - val_acc: 0.9773\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.06372\n",
      "Epoch 63/72\n",
      "12159/12159 [==============================] - 1s 115us/step - loss: 0.0779 - acc: 0.9706 - val_loss: 0.0753 - val_acc: 0.9674\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.06372\n",
      "Epoch 64/72\n",
      "12159/12159 [==============================] - 1s 119us/step - loss: 0.0763 - acc: 0.9706 - val_loss: 0.0578 - val_acc: 0.9789\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.06372 to 0.05782, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 65/72\n",
      "12159/12159 [==============================] - 1s 111us/step - loss: 0.0713 - acc: 0.9734 - val_loss: 0.0835 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.05782\n",
      "Epoch 66/72\n",
      "12159/12159 [==============================] - 1s 111us/step - loss: 0.0744 - acc: 0.9724 - val_loss: 0.0978 - val_acc: 0.9625\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.05782\n",
      "Epoch 67/72\n",
      "12159/12159 [==============================] - 1s 113us/step - loss: 0.0768 - acc: 0.9708 - val_loss: 0.0680 - val_acc: 0.9770\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.05782\n",
      "Epoch 68/72\n",
      "12159/12159 [==============================] - 1s 110us/step - loss: 0.0756 - acc: 0.9720 - val_loss: 0.0912 - val_acc: 0.9589\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.05782\n",
      "Epoch 69/72\n",
      "12159/12159 [==============================] - 1s 111us/step - loss: 0.0700 - acc: 0.9726 - val_loss: 0.0641 - val_acc: 0.9770\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.05782\n",
      "Epoch 70/72\n",
      "12159/12159 [==============================] - 1s 112us/step - loss: 0.0649 - acc: 0.9752 - val_loss: 0.0830 - val_acc: 0.9714\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.05782\n",
      "Epoch 71/72\n",
      "12159/12159 [==============================] - 1s 110us/step - loss: 0.0724 - acc: 0.9735 - val_loss: 0.0803 - val_acc: 0.9668\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.05782\n",
      "Epoch 72/72\n",
      "12159/12159 [==============================] - 1s 117us/step - loss: 0.0656 - acc: 0.9739 - val_loss: 0.0731 - val_acc: 0.9727\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.05782\n",
      "Training completed in time:  0:01:40.758886\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "num_epochs = 72\n",
    "num_batch_size = 64\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)\n",
    "!telegram-send \"Server: Habe alles gelernt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9934205115552266\n",
      "Testing Accuracy:  0.9726973684210526\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])\n",
    "!telegram-send \"Server: Habe alles evaluiert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(os.path.join(os.getcwd(), \"Models\", \"m4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Models/m4/weights.model')\n",
    "model.save_weights(\"Models/m4/model.h5\")\n",
    "model_json = model.to_json()\n",
    "with open(\"Models/m4/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5 - Part 3\n",
    "\n",
    "* Marker = einzelne Channel + Mono => augmented position\n",
    "* Mono-Snippets von Maxima\n",
    "* getMFCC()\n",
    "* Architektur: CNN von https://medium.com/@mikesmales/sound-classification-using-deep-learning-8bc2aa1990b7\n",
    "\n",
    "=> VERBESSERTE DATEN, KEINE FEHLER MEHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(features) =  15222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-552.32306, -539.6811, -537.4932, -546.9935,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>[[-504.08615, -497.9757, -487.82037, -492.8520...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>[[-701.8161, -693.2406, -688.5021, -684.5822, ...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>[[-546.99133, -537.48883, -538.3186, -539.6693...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>[[-716.00964, -701.72455, -692.82227, -694.809...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>[[-626.96893, -629.7167, -639.7845, -654.0444,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5700</th>\n",
       "      <td>[[-849.5224, -838.67224, -832.0527, -824.084, ...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6650</th>\n",
       "      <td>[[-359.46896, -355.5226, -362.19086, -360.8022...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7600</th>\n",
       "      <td>[[-574.2854, -599.0054, -650.21686, -676.9735,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8550</th>\n",
       "      <td>[[-453.0503, -455.63123, -489.18375, -518.4922...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9500</th>\n",
       "      <td>[[-630.36194, -630.92975, -631.2441, -626.1053...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10450</th>\n",
       "      <td>[[-432.01434, -421.70865, -401.99835, -391.610...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11400</th>\n",
       "      <td>[[-669.495, -671.3407, -678.3855, -679.11066, ...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12350</th>\n",
       "      <td>[[-533.44696, -541.816, -553.2887, -549.1016, ...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13300</th>\n",
       "      <td>[[-577.5787, -561.6607, -555.95544, -554.75696...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14250</th>\n",
       "      <td>[[-408.88312, -416.75235, -447.59137, -454.108...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15200</th>\n",
       "      <td>[[-332.56412, -317.58676, -294.2678, -294.8517...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 feature class_label\n",
       "0      [[-552.32306, -539.6811, -537.4932, -546.9935,...       klapp\n",
       "950    [[-504.08615, -497.9757, -487.82037, -492.8520...       klapp\n",
       "1900   [[-701.8161, -693.2406, -688.5021, -684.5822, ...     noklapp\n",
       "2850   [[-546.99133, -537.48883, -538.3186, -539.6693...       klapp\n",
       "3800   [[-716.00964, -701.72455, -692.82227, -694.809...     noklapp\n",
       "4750   [[-626.96893, -629.7167, -639.7845, -654.0444,...       klapp\n",
       "5700   [[-849.5224, -838.67224, -832.0527, -824.084, ...       klapp\n",
       "6650   [[-359.46896, -355.5226, -362.19086, -360.8022...       klapp\n",
       "7600   [[-574.2854, -599.0054, -650.21686, -676.9735,...     noklapp\n",
       "8550   [[-453.0503, -455.63123, -489.18375, -518.4922...     noklapp\n",
       "9500   [[-630.36194, -630.92975, -631.2441, -626.1053...       klapp\n",
       "10450  [[-432.01434, -421.70865, -401.99835, -391.610...     noklapp\n",
       "11400  [[-669.495, -671.3407, -678.3855, -679.11066, ...       klapp\n",
       "12350  [[-533.44696, -541.816, -553.2887, -549.1016, ...       klapp\n",
       "13300  [[-577.5787, -561.6607, -555.95544, -554.75696...     noklapp\n",
       "14250  [[-408.88312, -416.75235, -447.59137, -454.108...     noklapp\n",
       "15200  [[-332.56412, -317.58676, -294.2678, -294.8517...     noklapp"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pickle.load(open(\"Pickles/pickle4-2.p\", \"rb\"))\n",
    "print(\"len(features) = \", len(features))\n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "featuresdf[::950]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = [np.expand_dims(features[0][0], axis=2)] #[] #np.array(featuresdf.feature.tolist())\n",
    "for f in features[1::]:\n",
    "    xi = np.expand_dims(f[0], axis=2)\n",
    "    X = np.append(X, [xi], axis=0)\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\n",
    "!telegram-send \"Server: Features konvertiert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_rows = 40 #40\n",
    "num_columns = 94 #1\n",
    "num_channels = 1\n",
    "\n",
    "#x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "#x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 39, 93, 16)        80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 19, 46, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 19, 46, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 18, 45, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 9, 22, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 9, 22, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 8, 21, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 4, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 4, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 3, 9, 128)         32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 1, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 1, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_5 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 43,570\n",
      "Trainable params: 43,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3045/3045 [==============================] - 1s 266us/step\n",
      "Pre-training accuracy: 49.9179%\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12177 samples, validate on 3045 samples\n",
      "Epoch 1/72\n",
      "12177/12177 [==============================] - 3s 206us/step - loss: 1.3860 - acc: 0.6114 - val_loss: 0.5775 - val_acc: 0.7172\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57751, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 2/72\n",
      "12177/12177 [==============================] - 1s 118us/step - loss: 0.5392 - acc: 0.7101 - val_loss: 0.4727 - val_acc: 0.7934\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57751 to 0.47268, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 3/72\n",
      "12177/12177 [==============================] - 1s 117us/step - loss: 0.4692 - acc: 0.7643 - val_loss: 0.3968 - val_acc: 0.8397\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.47268 to 0.39680, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 4/72\n",
      "12177/12177 [==============================] - 1s 115us/step - loss: 0.4212 - acc: 0.7989 - val_loss: 0.3346 - val_acc: 0.8617\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.39680 to 0.33457, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 5/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.3738 - acc: 0.8284 - val_loss: 0.3020 - val_acc: 0.8890\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.33457 to 0.30199, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 6/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.3404 - acc: 0.8483 - val_loss: 0.2673 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.30199 to 0.26725, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 7/72\n",
      "12177/12177 [==============================] - 1s 117us/step - loss: 0.3187 - acc: 0.8593 - val_loss: 0.2741 - val_acc: 0.8995\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.26725\n",
      "Epoch 8/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.2932 - acc: 0.8732 - val_loss: 0.2396 - val_acc: 0.9143\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.26725 to 0.23958, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 9/72\n",
      "12177/12177 [==============================] - 1s 114us/step - loss: 0.2859 - acc: 0.8756 - val_loss: 0.2569 - val_acc: 0.8995\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.23958\n",
      "Epoch 10/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.2685 - acc: 0.8860 - val_loss: 0.2305 - val_acc: 0.9176\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.23958 to 0.23049, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 11/72\n",
      "12177/12177 [==============================] - 1s 115us/step - loss: 0.2494 - acc: 0.8941 - val_loss: 0.1974 - val_acc: 0.9287\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.23049 to 0.19737, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 12/72\n",
      "12177/12177 [==============================] - 1s 115us/step - loss: 0.2461 - acc: 0.8968 - val_loss: 0.2127 - val_acc: 0.9209\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.19737\n",
      "Epoch 13/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.2268 - acc: 0.9084 - val_loss: 0.1884 - val_acc: 0.9340\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.19737 to 0.18841, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 14/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.2151 - acc: 0.9139 - val_loss: 0.1907 - val_acc: 0.9310\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.18841\n",
      "Epoch 15/72\n",
      "12177/12177 [==============================] - 1s 114us/step - loss: 0.2202 - acc: 0.9099 - val_loss: 0.2018 - val_acc: 0.9258\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.18841\n",
      "Epoch 16/72\n",
      "12177/12177 [==============================] - 1s 114us/step - loss: 0.2091 - acc: 0.9163 - val_loss: 0.1752 - val_acc: 0.9432\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.18841 to 0.17517, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 17/72\n",
      "12177/12177 [==============================] - 1s 110us/step - loss: 0.2020 - acc: 0.9189 - val_loss: 0.1771 - val_acc: 0.9340\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.17517\n",
      "Epoch 18/72\n",
      "12177/12177 [==============================] - 1s 109us/step - loss: 0.1919 - acc: 0.9221 - val_loss: 0.1889 - val_acc: 0.9245\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.17517\n",
      "Epoch 19/72\n",
      "12177/12177 [==============================] - 1s 113us/step - loss: 0.1834 - acc: 0.9264 - val_loss: 0.2100 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.17517\n",
      "Epoch 20/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.1844 - acc: 0.9278 - val_loss: 0.1890 - val_acc: 0.9291\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.17517\n",
      "Epoch 21/72\n",
      "12177/12177 [==============================] - 1s 117us/step - loss: 0.1739 - acc: 0.9289 - val_loss: 0.1527 - val_acc: 0.9484\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.17517 to 0.15273, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 22/72\n",
      "12177/12177 [==============================] - 1s 115us/step - loss: 0.1707 - acc: 0.9314 - val_loss: 0.1466 - val_acc: 0.9465\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.15273 to 0.14664, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 23/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.1681 - acc: 0.9333 - val_loss: 0.1368 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.14664 to 0.13684, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 24/72\n",
      "12177/12177 [==============================] - 1s 117us/step - loss: 0.1691 - acc: 0.9327 - val_loss: 0.1234 - val_acc: 0.9530\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.13684 to 0.12343, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 25/72\n",
      "12177/12177 [==============================] - 1s 117us/step - loss: 0.1629 - acc: 0.9355 - val_loss: 0.1338 - val_acc: 0.9537\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.12343\n",
      "Epoch 26/72\n",
      "12177/12177 [==============================] - 1s 117us/step - loss: 0.1573 - acc: 0.9356 - val_loss: 0.1452 - val_acc: 0.9448\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.12343\n",
      "Epoch 27/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.1515 - acc: 0.9399 - val_loss: 0.1265 - val_acc: 0.9547\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.12343\n",
      "Epoch 28/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.1510 - acc: 0.9411 - val_loss: 0.1253 - val_acc: 0.9557\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.12343\n",
      "Epoch 29/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.1419 - acc: 0.9444 - val_loss: 0.1318 - val_acc: 0.9544\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.12343\n",
      "Epoch 30/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.1395 - acc: 0.9460 - val_loss: 0.1193 - val_acc: 0.9599\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.12343 to 0.11933, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 31/72\n",
      "12177/12177 [==============================] - 1s 115us/step - loss: 0.1444 - acc: 0.9434 - val_loss: 0.1165 - val_acc: 0.9619\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.11933 to 0.11646, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 32/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.1387 - acc: 0.9471 - val_loss: 0.1166 - val_acc: 0.9635\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.11646\n",
      "Epoch 33/72\n",
      "12177/12177 [==============================] - 1s 117us/step - loss: 0.1339 - acc: 0.9474 - val_loss: 0.1280 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.11646\n",
      "Epoch 34/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.1329 - acc: 0.9489 - val_loss: 0.1161 - val_acc: 0.9573\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.11646 to 0.11605, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 35/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.1280 - acc: 0.9496 - val_loss: 0.1086 - val_acc: 0.9616\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.11605 to 0.10862, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 36/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.1226 - acc: 0.9529 - val_loss: 0.1402 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.10862\n",
      "Epoch 37/72\n",
      "12177/12177 [==============================] - 1s 115us/step - loss: 0.1205 - acc: 0.9548 - val_loss: 0.1132 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.10862\n",
      "Epoch 38/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.1177 - acc: 0.9544 - val_loss: 0.1081 - val_acc: 0.9635\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.10862 to 0.10809, saving model to weights.best.basic_cnn.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/72\n",
      "12177/12177 [==============================] - 1s 115us/step - loss: 0.1215 - acc: 0.9519 - val_loss: 0.1273 - val_acc: 0.9537\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.10809\n",
      "Epoch 40/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.1170 - acc: 0.9526 - val_loss: 0.1136 - val_acc: 0.9560\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.10809\n",
      "Epoch 41/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.1119 - acc: 0.9553 - val_loss: 0.0904 - val_acc: 0.9685\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.10809 to 0.09042, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 42/72\n",
      "12177/12177 [==============================] - 1s 115us/step - loss: 0.1218 - acc: 0.9521 - val_loss: 0.1112 - val_acc: 0.9603\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.09042\n",
      "Epoch 43/72\n",
      "12177/12177 [==============================] - 1s 117us/step - loss: 0.1102 - acc: 0.9584 - val_loss: 0.1199 - val_acc: 0.9537\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09042\n",
      "Epoch 44/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.1121 - acc: 0.9558 - val_loss: 0.0926 - val_acc: 0.9698\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09042\n",
      "Epoch 45/72\n",
      "12177/12177 [==============================] - 1s 117us/step - loss: 0.1086 - acc: 0.9575 - val_loss: 0.1202 - val_acc: 0.9527\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.09042\n",
      "Epoch 46/72\n",
      "12177/12177 [==============================] - 1s 115us/step - loss: 0.1009 - acc: 0.9640 - val_loss: 0.0870 - val_acc: 0.9675\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.09042 to 0.08702, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 47/72\n",
      "12177/12177 [==============================] - 1s 118us/step - loss: 0.1044 - acc: 0.9571 - val_loss: 0.0926 - val_acc: 0.9691\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.08702\n",
      "Epoch 48/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.1042 - acc: 0.9599 - val_loss: 0.0897 - val_acc: 0.9711\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.08702\n",
      "Epoch 49/72\n",
      "12177/12177 [==============================] - 1s 114us/step - loss: 0.0987 - acc: 0.9590 - val_loss: 0.0946 - val_acc: 0.9668\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.08702\n",
      "Epoch 50/72\n",
      "12177/12177 [==============================] - 1s 110us/step - loss: 0.1058 - acc: 0.9603 - val_loss: 0.1049 - val_acc: 0.9632\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.08702\n",
      "Epoch 51/72\n",
      "12177/12177 [==============================] - 1s 111us/step - loss: 0.1007 - acc: 0.9619 - val_loss: 0.1116 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.08702\n",
      "Epoch 52/72\n",
      "12177/12177 [==============================] - 1s 113us/step - loss: 0.0995 - acc: 0.9594 - val_loss: 0.1111 - val_acc: 0.9658\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.08702\n",
      "Epoch 53/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.0895 - acc: 0.9647 - val_loss: 0.1162 - val_acc: 0.9530\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.08702\n",
      "Epoch 54/72\n",
      "12177/12177 [==============================] - 1s 117us/step - loss: 0.0875 - acc: 0.9646 - val_loss: 0.0866 - val_acc: 0.9718\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.08702 to 0.08660, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 55/72\n",
      "12177/12177 [==============================] - 1s 115us/step - loss: 0.0892 - acc: 0.9639 - val_loss: 0.0748 - val_acc: 0.9741\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.08660 to 0.07481, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 56/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.0928 - acc: 0.9635 - val_loss: 0.0888 - val_acc: 0.9708\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.07481\n",
      "Epoch 57/72\n",
      "12177/12177 [==============================] - 1s 117us/step - loss: 0.0879 - acc: 0.9670 - val_loss: 0.0806 - val_acc: 0.9724\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.07481\n",
      "Epoch 58/72\n",
      "12177/12177 [==============================] - 1s 115us/step - loss: 0.0945 - acc: 0.9636 - val_loss: 0.1104 - val_acc: 0.9567\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.07481\n",
      "Epoch 59/72\n",
      "12177/12177 [==============================] - 1s 117us/step - loss: 0.0839 - acc: 0.9676 - val_loss: 0.0848 - val_acc: 0.9711\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.07481\n",
      "Epoch 60/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.0894 - acc: 0.9676 - val_loss: 0.0832 - val_acc: 0.9672\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.07481\n",
      "Epoch 61/72\n",
      "12177/12177 [==============================] - 1s 115us/step - loss: 0.0892 - acc: 0.9654 - val_loss: 0.0823 - val_acc: 0.9718\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.07481\n",
      "Epoch 62/72\n",
      "12177/12177 [==============================] - 1s 115us/step - loss: 0.0861 - acc: 0.9651 - val_loss: 0.0869 - val_acc: 0.9708\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.07481\n",
      "Epoch 63/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.0881 - acc: 0.9667 - val_loss: 0.0793 - val_acc: 0.9721\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.07481\n",
      "Epoch 64/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.0870 - acc: 0.9641 - val_loss: 0.1074 - val_acc: 0.9629\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.07481\n",
      "Epoch 65/72\n",
      "12177/12177 [==============================] - 1s 114us/step - loss: 0.0852 - acc: 0.9672 - val_loss: 0.0779 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.07481\n",
      "Epoch 66/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.0900 - acc: 0.9639 - val_loss: 0.0836 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.07481\n",
      "Epoch 67/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.0782 - acc: 0.9694 - val_loss: 0.0861 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.07481\n",
      "Epoch 68/72\n",
      "12177/12177 [==============================] - 1s 115us/step - loss: 0.0861 - acc: 0.9678 - val_loss: 0.0971 - val_acc: 0.9681\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.07481\n",
      "Epoch 69/72\n",
      "12177/12177 [==============================] - 1s 120us/step - loss: 0.0777 - acc: 0.9703 - val_loss: 0.0793 - val_acc: 0.9721\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.07481\n",
      "Epoch 70/72\n",
      "12177/12177 [==============================] - 1s 112us/step - loss: 0.0787 - acc: 0.9690 - val_loss: 0.0965 - val_acc: 0.9662\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.07481\n",
      "Epoch 71/72\n",
      "12177/12177 [==============================] - 1s 114us/step - loss: 0.0773 - acc: 0.9699 - val_loss: 0.0906 - val_acc: 0.9681\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.07481\n",
      "Epoch 72/72\n",
      "12177/12177 [==============================] - 1s 116us/step - loss: 0.0779 - acc: 0.9709 - val_loss: 0.0787 - val_acc: 0.9708\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.07481\n",
      "Training completed in time:  0:01:45.051190\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "num_epochs = 72\n",
    "num_batch_size = 64\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)\n",
    "!telegram-send \"Server: Habe alles gelernt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.988256549232159\n",
      "Testing Accuracy:  0.9707717569786535\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(os.path.join(os.getcwd(), \"Models\", \"m5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Models/m5/weights.model')\n",
    "model.save_weights(\"Models/m5/model.h5\")\n",
    "model_json = model.to_json()\n",
    "with open(\"Models/m5/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 7\n",
    "\n",
    "* Marker = einzelne Channel + Mono + random mixes => augmented position\n",
    "* Mono- und Random-Mix-Snippets von Maxima (mehr verschiedene Snippets, nicht positionsaugmentiert)\n",
    "* getMFCC()\n",
    "* Architektur: CNN von https://medium.com/@mikesmales/sound-classification-using-deep-learning-8bc2aa1990b7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(features) =  14394\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-552.32306, -539.6811, -537.4932, -546.9935,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>[[-504.08615, -497.9757, -487.82037, -492.8520...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>[[-701.8161, -693.2406, -688.5021, -684.5822, ...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>[[-546.99133, -537.48883, -538.3186, -539.6693...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>[[-716.00964, -701.72455, -692.82227, -694.809...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>[[-626.96893, -629.7167, -639.7845, -654.0444,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5700</th>\n",
       "      <td>[[-849.5224, -838.67224, -832.0527, -824.084, ...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6650</th>\n",
       "      <td>[[-359.46896, -355.5226, -362.19086, -360.8022...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7600</th>\n",
       "      <td>[[-574.2854, -599.0054, -650.21686, -676.9735,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8550</th>\n",
       "      <td>[[-453.0503, -455.63123, -489.18375, -518.4922...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9500</th>\n",
       "      <td>[[-630.36194, -630.92975, -631.2441, -626.1053...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10450</th>\n",
       "      <td>[[-432.01434, -421.70865, -401.99835, -391.610...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11400</th>\n",
       "      <td>[[-669.495, -671.3407, -678.3855, -679.11066, ...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12350</th>\n",
       "      <td>[[-533.44696, -541.816, -553.2887, -549.1016, ...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13300</th>\n",
       "      <td>[[-512.7151, -523.637, -532.47516, -534.72284,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14250</th>\n",
       "      <td>[[-585.6826, -578.3902, -570.0773, -565.31696,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 feature class_label\n",
       "0      [[-552.32306, -539.6811, -537.4932, -546.9935,...       klapp\n",
       "950    [[-504.08615, -497.9757, -487.82037, -492.8520...       klapp\n",
       "1900   [[-701.8161, -693.2406, -688.5021, -684.5822, ...     noklapp\n",
       "2850   [[-546.99133, -537.48883, -538.3186, -539.6693...       klapp\n",
       "3800   [[-716.00964, -701.72455, -692.82227, -694.809...     noklapp\n",
       "4750   [[-626.96893, -629.7167, -639.7845, -654.0444,...       klapp\n",
       "5700   [[-849.5224, -838.67224, -832.0527, -824.084, ...       klapp\n",
       "6650   [[-359.46896, -355.5226, -362.19086, -360.8022...       klapp\n",
       "7600   [[-574.2854, -599.0054, -650.21686, -676.9735,...     noklapp\n",
       "8550   [[-453.0503, -455.63123, -489.18375, -518.4922...     noklapp\n",
       "9500   [[-630.36194, -630.92975, -631.2441, -626.1053...       klapp\n",
       "10450  [[-432.01434, -421.70865, -401.99835, -391.610...     noklapp\n",
       "11400  [[-669.495, -671.3407, -678.3855, -679.11066, ...       klapp\n",
       "12350  [[-533.44696, -541.816, -553.2887, -549.1016, ...       klapp\n",
       "13300  [[-512.7151, -523.637, -532.47516, -534.72284,...       klapp\n",
       "14250  [[-585.6826, -578.3902, -570.0773, -565.31696,...       klapp"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pickle.load(open(\"Pickles/pickle6.p\", \"rb\"))\n",
    "print(\"len(features) = \", len(features))\n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "featuresdf[::950]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = [np.expand_dims(features[0][0], axis=2)] #[] #np.array(featuresdf.feature.tolist())\n",
    "for f in features[1::]:\n",
    "    xi = np.expand_dims(f[0], axis=2)\n",
    "    X = np.append(X, [xi], axis=0)\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\n",
    "!telegram-send \"Server: Features konvertiert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_rows = 40 #40\n",
    "num_columns = 94 #1\n",
    "num_channels = 1\n",
    "\n",
    "#x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "#x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax')) # -> sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 39, 93, 16)        80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 19, 46, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 19, 46, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 18, 45, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 9, 22, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 9, 22, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 8, 21, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 4, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 4, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 3, 9, 128)         32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 1, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 1, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_7 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 43,570\n",
      "Trainable params: 43,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2879/2879 [==============================] - 1s 207us/step\n",
      "Pre-training accuracy: 49.1143%\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11515 samples, validate on 2879 samples\n",
      "Epoch 1/72\n",
      "11515/11515 [==============================] - 2s 174us/step - loss: 1.3467 - acc: 0.6324 - val_loss: 0.5778 - val_acc: 0.7371\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57775, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 2/72\n",
      "11515/11515 [==============================] - 1s 123us/step - loss: 0.5529 - acc: 0.7112 - val_loss: 0.5198 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57775 to 0.51977, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 3/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.4737 - acc: 0.7618 - val_loss: 0.4494 - val_acc: 0.7906\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.51977 to 0.44942, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 4/72\n",
      "11515/11515 [==============================] - 1s 125us/step - loss: 0.4102 - acc: 0.8076 - val_loss: 0.3491 - val_acc: 0.8628\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.44942 to 0.34913, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 5/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.3502 - acc: 0.8423 - val_loss: 0.2692 - val_acc: 0.8993\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.34913 to 0.26922, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 6/72\n",
      "11515/11515 [==============================] - 1s 122us/step - loss: 0.3109 - acc: 0.8644 - val_loss: 0.2395 - val_acc: 0.9090\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26922 to 0.23953, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 7/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.2895 - acc: 0.8756 - val_loss: 0.2339 - val_acc: 0.9166\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.23953 to 0.23393, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 8/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.2761 - acc: 0.8843 - val_loss: 0.2199 - val_acc: 0.9173\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.23393 to 0.21989, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 9/72\n",
      "11515/11515 [==============================] - 1s 123us/step - loss: 0.2614 - acc: 0.8925 - val_loss: 0.2333 - val_acc: 0.9229\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.21989\n",
      "Epoch 10/72\n",
      "11515/11515 [==============================] - 1s 123us/step - loss: 0.2467 - acc: 0.8972 - val_loss: 0.2024 - val_acc: 0.9298\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.21989 to 0.20237, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 11/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.2393 - acc: 0.9019 - val_loss: 0.2051 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.20237\n",
      "Epoch 12/72\n",
      "11515/11515 [==============================] - 1s 122us/step - loss: 0.2332 - acc: 0.9031 - val_loss: 0.1983 - val_acc: 0.9368\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.20237 to 0.19832, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 13/72\n",
      "11515/11515 [==============================] - 1s 126us/step - loss: 0.2200 - acc: 0.9103 - val_loss: 0.1829 - val_acc: 0.9382\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.19832 to 0.18288, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 14/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.2125 - acc: 0.9143 - val_loss: 0.2201 - val_acc: 0.9316\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.18288\n",
      "Epoch 15/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.2048 - acc: 0.9159 - val_loss: 0.1588 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.18288 to 0.15875, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 16/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.2042 - acc: 0.9172 - val_loss: 0.1722 - val_acc: 0.9410\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.15875\n",
      "Epoch 17/72\n",
      "11515/11515 [==============================] - 1s 122us/step - loss: 0.1981 - acc: 0.9200 - val_loss: 0.1607 - val_acc: 0.9476\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.15875\n",
      "Epoch 18/72\n",
      "11515/11515 [==============================] - 1s 126us/step - loss: 0.1929 - acc: 0.9199 - val_loss: 0.1669 - val_acc: 0.9489\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.15875\n",
      "Epoch 19/72\n",
      "11515/11515 [==============================] - 1s 125us/step - loss: 0.1901 - acc: 0.9258 - val_loss: 0.1524 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.15875 to 0.15238, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 20/72\n",
      "11515/11515 [==============================] - 1s 123us/step - loss: 0.1804 - acc: 0.9296 - val_loss: 0.1460 - val_acc: 0.9514\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.15238 to 0.14598, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 21/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.1747 - acc: 0.9312 - val_loss: 0.1672 - val_acc: 0.9434\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.14598\n",
      "Epoch 22/72\n",
      "11515/11515 [==============================] - 1s 123us/step - loss: 0.1684 - acc: 0.9326 - val_loss: 0.1490 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.14598\n",
      "Epoch 23/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.1665 - acc: 0.9329 - val_loss: 0.1585 - val_acc: 0.9465\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.14598\n",
      "Epoch 24/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.1572 - acc: 0.9375 - val_loss: 0.1437 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.14598 to 0.14365, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 25/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.1591 - acc: 0.9350 - val_loss: 0.1548 - val_acc: 0.9444\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.14365\n",
      "Epoch 26/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.1520 - acc: 0.9406 - val_loss: 0.1365 - val_acc: 0.9486\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.14365 to 0.13648, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 27/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.1449 - acc: 0.9430 - val_loss: 0.1173 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.13648 to 0.11732, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 28/72\n",
      "11515/11515 [==============================] - 1s 125us/step - loss: 0.1500 - acc: 0.9422 - val_loss: 0.1136 - val_acc: 0.9653\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.11732 to 0.11360, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 29/72\n",
      "11515/11515 [==============================] - 1s 122us/step - loss: 0.1451 - acc: 0.9427 - val_loss: 0.1265 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.11360\n",
      "Epoch 30/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.1393 - acc: 0.9442 - val_loss: 0.1111 - val_acc: 0.9639\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.11360 to 0.11112, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 31/72\n",
      "11515/11515 [==============================] - 1s 123us/step - loss: 0.1273 - acc: 0.9528 - val_loss: 0.0944 - val_acc: 0.9708\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.11112 to 0.09442, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 32/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.1423 - acc: 0.9441 - val_loss: 0.1142 - val_acc: 0.9632\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.09442\n",
      "Epoch 33/72\n",
      "11515/11515 [==============================] - 1s 123us/step - loss: 0.1358 - acc: 0.9475 - val_loss: 0.1141 - val_acc: 0.9625\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.09442\n",
      "Epoch 34/72\n",
      "11515/11515 [==============================] - 1s 125us/step - loss: 0.1273 - acc: 0.9508 - val_loss: 0.0915 - val_acc: 0.9719\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.09442 to 0.09147, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 35/72\n",
      "11515/11515 [==============================] - 1s 125us/step - loss: 0.1332 - acc: 0.9487 - val_loss: 0.1341 - val_acc: 0.9555\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.09147\n",
      "Epoch 36/72\n",
      "11515/11515 [==============================] - 1s 125us/step - loss: 0.1256 - acc: 0.9494 - val_loss: 0.0985 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.09147\n",
      "Epoch 37/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.1205 - acc: 0.9515 - val_loss: 0.1073 - val_acc: 0.9649\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.09147\n",
      "Epoch 38/72\n",
      "11515/11515 [==============================] - 1s 125us/step - loss: 0.1171 - acc: 0.9561 - val_loss: 0.1033 - val_acc: 0.9670\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.09147\n",
      "Epoch 39/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.1140 - acc: 0.9542 - val_loss: 0.0899 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.09147 to 0.08988, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 40/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.1183 - acc: 0.9535 - val_loss: 0.0824 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.08988 to 0.08239, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 41/72\n",
      "11515/11515 [==============================] - 1s 123us/step - loss: 0.1148 - acc: 0.9536 - val_loss: 0.0899 - val_acc: 0.9670\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.08239\n",
      "Epoch 42/72\n",
      "11515/11515 [==============================] - 1s 125us/step - loss: 0.1055 - acc: 0.9574 - val_loss: 0.0883 - val_acc: 0.9719\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.08239\n",
      "Epoch 43/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.1109 - acc: 0.9573 - val_loss: 0.0920 - val_acc: 0.9719\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.08239\n",
      "Epoch 44/72\n",
      "11515/11515 [==============================] - 1s 122us/step - loss: 0.1009 - acc: 0.9606 - val_loss: 0.1001 - val_acc: 0.9673\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.08239\n",
      "Epoch 45/72\n",
      "11515/11515 [==============================] - 1s 127us/step - loss: 0.1133 - acc: 0.9563 - val_loss: 0.1119 - val_acc: 0.9628\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.08239\n",
      "Epoch 46/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.1060 - acc: 0.9560 - val_loss: 0.1026 - val_acc: 0.9639\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.08239\n",
      "Epoch 47/72\n",
      "11515/11515 [==============================] - 1s 122us/step - loss: 0.0999 - acc: 0.9612 - val_loss: 0.0937 - val_acc: 0.9656\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.08239\n",
      "Epoch 48/72\n",
      "11515/11515 [==============================] - 1s 125us/step - loss: 0.0993 - acc: 0.9612 - val_loss: 0.0754 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.08239 to 0.07540, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 49/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.0981 - acc: 0.9607 - val_loss: 0.0996 - val_acc: 0.9625\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.07540\n",
      "Epoch 50/72\n",
      "11515/11515 [==============================] - 1s 123us/step - loss: 0.0951 - acc: 0.9637 - val_loss: 0.0993 - val_acc: 0.9656\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.07540\n",
      "Epoch 51/72\n",
      "11515/11515 [==============================] - 1s 126us/step - loss: 0.0991 - acc: 0.9620 - val_loss: 0.0940 - val_acc: 0.9670\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.07540\n",
      "Epoch 52/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.0964 - acc: 0.9613 - val_loss: 0.0742 - val_acc: 0.9736\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.07540 to 0.07421, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 53/72\n",
      "11515/11515 [==============================] - 1s 126us/step - loss: 0.0872 - acc: 0.9665 - val_loss: 0.0776 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.07421\n",
      "Epoch 54/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.0907 - acc: 0.9655 - val_loss: 0.0815 - val_acc: 0.9733\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.07421\n",
      "Epoch 55/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.0898 - acc: 0.9667 - val_loss: 0.0790 - val_acc: 0.9736\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.07421\n",
      "Epoch 56/72\n",
      "11515/11515 [==============================] - 1s 123us/step - loss: 0.0834 - acc: 0.9686 - val_loss: 0.0760 - val_acc: 0.9719\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.07421\n",
      "Epoch 57/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.0891 - acc: 0.9656 - val_loss: 0.0775 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.07421\n",
      "Epoch 58/72\n",
      "11515/11515 [==============================] - 1s 127us/step - loss: 0.0892 - acc: 0.9662 - val_loss: 0.0739 - val_acc: 0.9760\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.07421 to 0.07385, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 59/72\n",
      "11515/11515 [==============================] - 1s 125us/step - loss: 0.0854 - acc: 0.9672 - val_loss: 0.0787 - val_acc: 0.9746\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.07385\n",
      "Epoch 60/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.0824 - acc: 0.9680 - val_loss: 0.0701 - val_acc: 0.9767\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.07385 to 0.07011, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 61/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.0899 - acc: 0.9648 - val_loss: 0.0869 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.07011\n",
      "Epoch 62/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.0806 - acc: 0.9681 - val_loss: 0.0787 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.07011\n",
      "Epoch 63/72\n",
      "11515/11515 [==============================] - 1s 126us/step - loss: 0.0833 - acc: 0.9673 - val_loss: 0.0712 - val_acc: 0.9774\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.07011\n",
      "Epoch 64/72\n",
      "11515/11515 [==============================] - 1s 123us/step - loss: 0.0786 - acc: 0.9694 - val_loss: 0.0829 - val_acc: 0.9705\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.07011\n",
      "Epoch 65/72\n",
      "11515/11515 [==============================] - 1s 122us/step - loss: 0.0813 - acc: 0.9680 - val_loss: 0.0761 - val_acc: 0.9746\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.07011\n",
      "Epoch 66/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.0775 - acc: 0.9707 - val_loss: 0.0623 - val_acc: 0.9799\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.07011 to 0.06228, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 67/72\n",
      "11515/11515 [==============================] - 1s 124us/step - loss: 0.0699 - acc: 0.9721 - val_loss: 0.0810 - val_acc: 0.9719\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.06228\n",
      "Epoch 68/72\n",
      "11515/11515 [==============================] - 1s 126us/step - loss: 0.0802 - acc: 0.9698 - val_loss: 0.0678 - val_acc: 0.9774\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.06228\n",
      "Epoch 69/72\n",
      "11515/11515 [==============================] - 1s 125us/step - loss: 0.0767 - acc: 0.9693 - val_loss: 0.0598 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.06228 to 0.05984, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 70/72\n",
      "11515/11515 [==============================] - 1s 123us/step - loss: 0.0740 - acc: 0.9699 - val_loss: 0.1049 - val_acc: 0.9566\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.05984\n",
      "Epoch 71/72\n",
      "11515/11515 [==============================] - 1s 122us/step - loss: 0.0738 - acc: 0.9744 - val_loss: 0.0688 - val_acc: 0.9767\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.05984\n",
      "Epoch 72/72\n",
      "11515/11515 [==============================] - 1s 123us/step - loss: 0.0777 - acc: 0.9706 - val_loss: 0.0807 - val_acc: 0.9705\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.05984\n",
      "Training completed in time:  0:01:45.763566\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "num_epochs = 72\n",
    "num_batch_size = 64\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)\n",
    "!telegram-send \"Server: Habe alles gelernt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9889709075119409\n",
      "Testing Accuracy:  0.970475859362949\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"m7\"\n",
    "os.mkdir(os.path.join(os.getcwd(), \"Models\", folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Models/'+folder+'/weights.model')\n",
    "model.save_weights(\"Models/\"+folder+\"/model.h5\")\n",
    "model_json = model.to_json()\n",
    "with open(\"Models/\"+folder+\"/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 40, 94, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 41, 95, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 20, 47, 32)        288       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 20, 47, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 20, 47, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 20, 47, 32)        288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 20, 47, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 20, 47, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 20, 47, 64)        2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 20, 47, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 20, 47, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 21, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 10, 23, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 10, 23, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 10, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 10, 23, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 10, 23, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 10, 23, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 10, 23, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 10, 23, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 10, 23, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 10, 23, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 10, 23, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 10, 23, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 11, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 5, 11, 128)        1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 5, 11, 128)        512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 5, 11, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 5, 11, 256)        32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 5, 11, 256)        1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 5, 11, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 5, 11, 256)        2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 5, 11, 256)        1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 5, 11, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 5, 11, 256)        65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 5, 11, 256)        1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 5, 11, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 6, 12, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 2, 5, 256)         2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 2, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 2, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 2, 5, 512)         131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 2, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 2, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 2, 5, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 2, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 2, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 2, 5, 512)         262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 2, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 2, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 2, 5, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 2, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 2, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 2, 5, 512)         262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 2, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 2, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 2, 5, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 2, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 2, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 2, 5, 512)         262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 2, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 2, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 2, 5, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 2, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 2, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 2, 5, 512)         262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 2, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 2, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 2, 5, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 2, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 2, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 2, 5, 512)         262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 2, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 2, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 3, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 1, 2, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 1, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 1, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 1, 2, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 1, 2, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 1, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 1, 2, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 1, 2, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 1, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 1, 2, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 1, 2, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 1, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_9 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_preds (Conv2D)          (None, 1, 1, 2)           2050      \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "act_softmax (Activation)     (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,230,338\n",
      "Trainable params: 3,208,450\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import MobileNet\n",
    "\n",
    "model = MobileNet(weights=None, input_shape=(num_rows, num_columns, num_channels), classes=2, include_top=True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11515 samples, validate on 2879 samples\n",
      "Epoch 1/72\n",
      "11515/11515 [==============================] - 17s 1ms/step - loss: 0.5097 - acc: 0.7548 - val_loss: 0.9591 - val_acc: 0.7544\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.95909, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 2/72\n",
      "11515/11515 [==============================] - 7s 623us/step - loss: 0.2819 - acc: 0.8763 - val_loss: 0.3678 - val_acc: 0.8628\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.95909 to 0.36783, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 3/72\n",
      "11515/11515 [==============================] - 7s 613us/step - loss: 0.1614 - acc: 0.9375 - val_loss: 0.2197 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36783 to 0.21974, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 4/72\n",
      "11515/11515 [==============================] - 7s 622us/step - loss: 0.0953 - acc: 0.9653 - val_loss: 0.2192 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.21974 to 0.21922, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 5/72\n",
      "11515/11515 [==============================] - 7s 624us/step - loss: 0.0621 - acc: 0.9766 - val_loss: 0.1166 - val_acc: 0.9611\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.21922 to 0.11659, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 6/72\n",
      "11515/11515 [==============================] - 7s 625us/step - loss: 0.0413 - acc: 0.9865 - val_loss: 0.1303 - val_acc: 0.9611\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.11659\n",
      "Epoch 7/72\n",
      "11515/11515 [==============================] - 7s 626us/step - loss: 0.0406 - acc: 0.9863 - val_loss: 0.1191 - val_acc: 0.9701\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.11659\n",
      "Epoch 8/72\n",
      "11515/11515 [==============================] - 7s 621us/step - loss: 0.0309 - acc: 0.9887 - val_loss: 0.1087 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.11659 to 0.10866, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 9/72\n",
      "11515/11515 [==============================] - 7s 603us/step - loss: 0.0355 - acc: 0.9881 - val_loss: 0.1171 - val_acc: 0.9680\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.10866\n",
      "Epoch 10/72\n",
      "11515/11515 [==============================] - 7s 625us/step - loss: 0.0258 - acc: 0.9920 - val_loss: 0.0836 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.10866 to 0.08365, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 11/72\n",
      "11515/11515 [==============================] - 7s 621us/step - loss: 0.0182 - acc: 0.9937 - val_loss: 0.1602 - val_acc: 0.9698\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.08365\n",
      "Epoch 12/72\n",
      "11515/11515 [==============================] - 7s 620us/step - loss: 0.0240 - acc: 0.9931 - val_loss: 0.1131 - val_acc: 0.9781\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.08365\n",
      "Epoch 13/72\n",
      "11515/11515 [==============================] - 7s 627us/step - loss: 0.0173 - acc: 0.9944 - val_loss: 0.0966 - val_acc: 0.9771\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.08365\n",
      "Epoch 14/72\n",
      "11515/11515 [==============================] - 7s 626us/step - loss: 0.0183 - acc: 0.9942 - val_loss: 0.1420 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.08365\n",
      "Epoch 15/72\n",
      "11515/11515 [==============================] - 7s 625us/step - loss: 0.0190 - acc: 0.9932 - val_loss: 0.0928 - val_acc: 0.9802\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.08365\n",
      "Epoch 16/72\n",
      "11515/11515 [==============================] - 7s 610us/step - loss: 0.0381 - acc: 0.9885 - val_loss: 0.0791 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.08365 to 0.07912, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 17/72\n",
      "11515/11515 [==============================] - 7s 625us/step - loss: 0.0151 - acc: 0.9951 - val_loss: 0.0510 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.07912 to 0.05102, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 18/72\n",
      "11515/11515 [==============================] - 7s 618us/step - loss: 0.0113 - acc: 0.9964 - val_loss: 0.1402 - val_acc: 0.9628\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.05102\n",
      "Epoch 19/72\n",
      "11515/11515 [==============================] - 7s 620us/step - loss: 0.0180 - acc: 0.9939 - val_loss: 0.0397 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.05102 to 0.03970, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 20/72\n",
      "11515/11515 [==============================] - 7s 623us/step - loss: 0.0122 - acc: 0.9962 - val_loss: 0.0733 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.03970\n",
      "Epoch 21/72\n",
      "11515/11515 [==============================] - 7s 621us/step - loss: 0.0208 - acc: 0.9924 - val_loss: 0.0403 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.03970\n",
      "Epoch 22/72\n",
      "11515/11515 [==============================] - 7s 603us/step - loss: 0.0095 - acc: 0.9974 - val_loss: 0.0415 - val_acc: 0.9899\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.03970\n",
      "Epoch 23/72\n",
      "11515/11515 [==============================] - 7s 626us/step - loss: 0.0169 - acc: 0.9953 - val_loss: 0.0598 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.03970\n",
      "Epoch 24/72\n",
      "11515/11515 [==============================] - 7s 619us/step - loss: 0.0180 - acc: 0.9941 - val_loss: 0.0420 - val_acc: 0.9878\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.03970\n",
      "Epoch 25/72\n",
      "11515/11515 [==============================] - 7s 624us/step - loss: 0.0166 - acc: 0.9949 - val_loss: 0.0294 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.03970 to 0.02936, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 26/72\n",
      "11515/11515 [==============================] - 7s 625us/step - loss: 0.0044 - acc: 0.9982 - val_loss: 0.0635 - val_acc: 0.9868\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02936\n",
      "Epoch 27/72\n",
      "11515/11515 [==============================] - 7s 624us/step - loss: 0.0127 - acc: 0.9962 - val_loss: 0.1535 - val_acc: 0.9614\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.02936\n",
      "Epoch 28/72\n",
      "11515/11515 [==============================] - 7s 621us/step - loss: 0.0119 - acc: 0.9977 - val_loss: 0.0473 - val_acc: 0.9871\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02936\n",
      "Epoch 29/72\n",
      "11515/11515 [==============================] - 7s 602us/step - loss: 0.0169 - acc: 0.9948 - val_loss: 0.0861 - val_acc: 0.9736\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02936\n",
      "Epoch 30/72\n",
      "11515/11515 [==============================] - 7s 625us/step - loss: 0.0108 - acc: 0.9967 - val_loss: 0.0568 - val_acc: 0.9809\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02936\n",
      "Epoch 31/72\n",
      "11515/11515 [==============================] - 7s 629us/step - loss: 0.0101 - acc: 0.9966 - val_loss: 0.0340 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02936\n",
      "Epoch 32/72\n",
      "11515/11515 [==============================] - 7s 620us/step - loss: 0.0130 - acc: 0.9951 - val_loss: 0.1159 - val_acc: 0.9698\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02936\n",
      "Epoch 33/72\n",
      "11515/11515 [==============================] - 7s 624us/step - loss: 0.0063 - acc: 0.9983 - val_loss: 0.0481 - val_acc: 0.9889\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02936\n",
      "Epoch 34/72\n",
      "11515/11515 [==============================] - 7s 622us/step - loss: 0.0129 - acc: 0.9968 - val_loss: 0.0513 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02936\n",
      "Epoch 35/72\n",
      "11515/11515 [==============================] - 7s 623us/step - loss: 0.0063 - acc: 0.9977 - val_loss: 0.0633 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02936\n",
      "Epoch 36/72\n",
      "11515/11515 [==============================] - 7s 605us/step - loss: 0.0101 - acc: 0.9977 - val_loss: 0.0391 - val_acc: 0.9875\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.02936\n",
      "Epoch 37/72\n",
      "11515/11515 [==============================] - 7s 625us/step - loss: 0.0064 - acc: 0.9980 - val_loss: 0.0808 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.02936\n",
      "Epoch 38/72\n",
      "11515/11515 [==============================] - 7s 625us/step - loss: 0.0220 - acc: 0.9929 - val_loss: 0.0486 - val_acc: 0.9906\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.02936\n",
      "Epoch 39/72\n",
      "11515/11515 [==============================] - 7s 626us/step - loss: 0.0120 - acc: 0.9965 - val_loss: 0.0261 - val_acc: 0.9934\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.02936 to 0.02614, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 40/72\n",
      "11515/11515 [==============================] - 7s 623us/step - loss: 0.0075 - acc: 0.9981 - val_loss: 0.0768 - val_acc: 0.9833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_loss did not improve from 0.02614\n",
      "Epoch 41/72\n",
      "11515/11515 [==============================] - 7s 622us/step - loss: 0.0263 - acc: 0.9924 - val_loss: 0.0444 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.02614\n",
      "Epoch 42/72\n",
      "11515/11515 [==============================] - 7s 608us/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0512 - val_acc: 0.9899\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.02614\n",
      "Epoch 43/72\n",
      "11515/11515 [==============================] - 7s 617us/step - loss: 0.0099 - acc: 0.9964 - val_loss: 0.0562 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.02614\n",
      "Epoch 44/72\n",
      "11515/11515 [==============================] - 7s 620us/step - loss: 0.0085 - acc: 0.9979 - val_loss: 0.0457 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.02614\n",
      "Epoch 45/72\n",
      "11515/11515 [==============================] - 7s 622us/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.0243 - val_acc: 0.9934\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.02614 to 0.02426, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 46/72\n",
      "11515/11515 [==============================] - 7s 625us/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.0216 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.02426 to 0.02157, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 47/72\n",
      "11515/11515 [==============================] - 7s 624us/step - loss: 0.0071 - acc: 0.9980 - val_loss: 0.2732 - val_acc: 0.9451\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.02157\n",
      "Epoch 48/72\n",
      "11515/11515 [==============================] - 7s 622us/step - loss: 0.0077 - acc: 0.9978 - val_loss: 0.0331 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.02157\n",
      "Epoch 49/72\n",
      "11515/11515 [==============================] - 7s 606us/step - loss: 0.0042 - acc: 0.9991 - val_loss: 0.0267 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.02157\n",
      "Epoch 50/72\n",
      "11515/11515 [==============================] - 7s 628us/step - loss: 0.0118 - acc: 0.9967 - val_loss: 0.1639 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.02157\n",
      "Epoch 51/72\n",
      "11515/11515 [==============================] - 7s 626us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0404 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.02157\n",
      "Epoch 52/72\n",
      "11515/11515 [==============================] - 7s 625us/step - loss: 0.0038 - acc: 0.9984 - val_loss: 0.0173 - val_acc: 0.9934\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.02157 to 0.01727, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 53/72\n",
      "11515/11515 [==============================] - 7s 625us/step - loss: 0.0086 - acc: 0.9970 - val_loss: 0.1958 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01727\n",
      "Epoch 54/72\n",
      "11515/11515 [==============================] - 7s 620us/step - loss: 0.0078 - acc: 0.9978 - val_loss: 0.0275 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.01727\n",
      "Epoch 55/72\n",
      "11515/11515 [==============================] - 7s 628us/step - loss: 0.0114 - acc: 0.9964 - val_loss: 0.0166 - val_acc: 0.9965\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.01727 to 0.01661, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 56/72\n",
      "11515/11515 [==============================] - 7s 612us/step - loss: 0.0066 - acc: 0.9978 - val_loss: 0.0161 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.01661 to 0.01605, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 57/72\n",
      "11515/11515 [==============================] - 7s 632us/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0345 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.01605\n",
      "Epoch 58/72\n",
      "11515/11515 [==============================] - 7s 622us/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0393 - val_acc: 0.9924\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.01605\n",
      "Epoch 59/72\n",
      "11515/11515 [==============================] - 7s 625us/step - loss: 6.4170e-04 - acc: 0.9997 - val_loss: 0.0097 - val_acc: 0.9969\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.01605 to 0.00974, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 60/72\n",
      "11515/11515 [==============================] - 7s 627us/step - loss: 0.0069 - acc: 0.9978 - val_loss: 0.2249 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00974\n",
      "Epoch 61/72\n",
      "11515/11515 [==============================] - 7s 621us/step - loss: 0.0122 - acc: 0.9970 - val_loss: 0.0242 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00974\n",
      "Epoch 62/72\n",
      "11515/11515 [==============================] - 7s 608us/step - loss: 0.0068 - acc: 0.9979 - val_loss: 0.0522 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00974\n",
      "Epoch 63/72\n",
      "11515/11515 [==============================] - 7s 625us/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0339 - val_acc: 0.9899\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00974\n",
      "Epoch 64/72\n",
      "11515/11515 [==============================] - 7s 619us/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.0333 - val_acc: 0.9931\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00974\n",
      "Epoch 65/72\n",
      "11515/11515 [==============================] - 7s 623us/step - loss: 0.0036 - acc: 0.9990 - val_loss: 0.0098 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00974\n",
      "Epoch 66/72\n",
      "11515/11515 [==============================] - 7s 621us/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0467 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00974\n",
      "Epoch 67/72\n",
      "11515/11515 [==============================] - 7s 624us/step - loss: 0.0093 - acc: 0.9967 - val_loss: 0.0193 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00974\n",
      "Epoch 68/72\n",
      "11515/11515 [==============================] - 7s 617us/step - loss: 0.0069 - acc: 0.9979 - val_loss: 0.0394 - val_acc: 0.9951\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00974\n",
      "Epoch 69/72\n",
      "11515/11515 [==============================] - 7s 605us/step - loss: 0.0025 - acc: 0.9990 - val_loss: 0.0194 - val_acc: 0.9969\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00974\n",
      "Epoch 70/72\n",
      "11515/11515 [==============================] - 7s 625us/step - loss: 0.0080 - acc: 0.9980 - val_loss: 0.0323 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00974\n",
      "Epoch 71/72\n",
      "11515/11515 [==============================] - 7s 626us/step - loss: 0.0083 - acc: 0.9970 - val_loss: 0.0124 - val_acc: 0.9951\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00974\n",
      "Epoch 72/72\n",
      "11515/11515 [==============================] - 7s 629us/step - loss: 0.0037 - acc: 0.9986 - val_loss: 0.0202 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00974\n",
      "Training completed in time:  0:09:21.302819\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "num_epochs = 72\n",
    "num_batch_size = 64\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)\n",
    "!telegram-send \"Server: Habe alles gelernt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9994789405123752\n",
      "Testing Accuracy:  0.9961792288989232\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"m8_mobilenet\"\n",
    "os.mkdir(os.path.join(os.getcwd(), \"Models\", folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Models/'+folder+'/weights.model')\n",
    "model.save_weights(\"Models/\"+folder+\"/model.h5\")\n",
    "model_json = model.to_json()\n",
    "with open(\"Models/\"+folder+\"/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 40, 94, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_pad (ZeroPadding2D)       (None, 41, 95, 1)    0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 20, 47, 32)   288         Conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 20, 47, 32)   128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, 20, 47, 32)   0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 20, 47, 32)   288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 20, 47, 32)   128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, 20, 47, 32)   0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 20, 47, 16)   512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 20, 47, 16)   64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 20, 47, 96)   1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 20, 47, 96)   384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, 20, 47, 96)   0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_pad (ZeroPadding2D)     (None, 21, 49, 96)   0           block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 10, 24, 96)   864         block_1_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 10, 24, 96)   384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, 10, 24, 96)   0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 10, 24, 24)   2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 10, 24, 24)   96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 10, 24, 144)  3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 10, 24, 144)  576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, 10, 24, 144)  0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 10, 24, 144)  1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 10, 24, 144)  576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, 10, 24, 144)  0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 10, 24, 24)   3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 10, 24, 24)   96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 10, 24, 24)   0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 10, 24, 144)  3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 10, 24, 144)  576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, 10, 24, 144)  0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_pad (ZeroPadding2D)     (None, 11, 25, 144)  0           block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 5, 12, 144)   1296        block_3_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 5, 12, 144)   576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, 5, 12, 144)   0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 5, 12, 32)    4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 5, 12, 32)    128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 5, 12, 192)   6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 5, 12, 192)   768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, 5, 12, 192)   0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 5, 12, 192)   1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 5, 12, 192)   768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, 5, 12, 192)   0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 5, 12, 32)    6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 5, 12, 32)    128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 5, 12, 32)    0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 5, 12, 192)   6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 5, 12, 192)   768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, 5, 12, 192)   0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 5, 12, 192)   1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 5, 12, 192)   768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, 5, 12, 192)   0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 5, 12, 32)    6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 5, 12, 32)    128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 5, 12, 32)    0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, 5, 12, 192)   6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, 5, 12, 192)   768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, 5, 12, 192)   0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_pad (ZeroPadding2D)     (None, 7, 13, 192)   0           block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, 3, 6, 192)    1728        block_6_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, 3, 6, 192)    768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, 3, 6, 192)    0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, 3, 6, 64)     12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, 3, 6, 64)     256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, 3, 6, 384)    24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, 3, 6, 384)    1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, 3, 6, 384)    0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, 3, 6, 384)    3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, 3, 6, 384)    1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, 3, 6, 384)    0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, 3, 6, 64)     24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, 3, 6, 64)     256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, 3, 6, 64)     0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, 3, 6, 384)    24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, 3, 6, 384)    1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, 3, 6, 384)    0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, 3, 6, 384)    3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, 3, 6, 384)    1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, 3, 6, 384)    0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, 3, 6, 64)     24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, 3, 6, 64)     256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, 3, 6, 64)     0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, 3, 6, 384)    24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, 3, 6, 384)    1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, 3, 6, 384)    0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, 3, 6, 384)    3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, 3, 6, 384)    1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, 3, 6, 384)    0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, 3, 6, 64)     24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, 3, 6, 64)     256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, 3, 6, 64)     0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, 3, 6, 384)    24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, 3, 6, 384)    1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, 3, 6, 384)    0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, 3, 6, 384)    3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, 3, 6, 384)    1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, 3, 6, 384)    0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, 3, 6, 96)     36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, 3, 6, 96)     384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, 3, 6, 576)    55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, 3, 6, 576)    2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, 3, 6, 576)    0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, 3, 6, 576)    5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, 3, 6, 576)    2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, 3, 6, 576)    0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, 3, 6, 96)     55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, 3, 6, 96)     384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, 3, 6, 96)     0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, 3, 6, 576)    55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, 3, 6, 576)    2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, 3, 6, 576)    0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, 3, 6, 576)    5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, 3, 6, 576)    2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, 3, 6, 576)    0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, 3, 6, 96)     55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, 3, 6, 96)     384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, 3, 6, 96)     0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, 3, 6, 576)    55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, 3, 6, 576)    2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, 3, 6, 576)    0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_pad (ZeroPadding2D)    (None, 5, 7, 576)    0           block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, 2, 3, 576)    5184        block_13_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, 2, 3, 576)    2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, 2, 3, 576)    0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, 2, 3, 160)    92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, 2, 3, 160)    640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, 2, 3, 960)    153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, 2, 3, 960)    3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, 2, 3, 960)    0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, 2, 3, 960)    8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, 2, 3, 960)    3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, 2, 3, 960)    0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, 2, 3, 160)    153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, 2, 3, 160)    640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, 2, 3, 160)    0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, 2, 3, 960)    153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, 2, 3, 960)    3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, 2, 3, 960)    0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, 2, 3, 960)    8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, 2, 3, 960)    3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, 2, 3, 960)    0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, 2, 3, 160)    153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, 2, 3, 160)    640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, 2, 3, 160)    0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, 2, 3, 960)    153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, 2, 3, 960)    3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, 2, 3, 960)    0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, 2, 3, 960)    8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, 2, 3, 960)    3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, 2, 3, 960)    0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, 2, 3, 320)    307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, 2, 3, 320)    1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 2, 3, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 2, 3, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 2, 3, 1280)   0           Conv_1_bn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_11 (Gl (None, 1280)         0           out_relu[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Logits (Dense)                  (None, 2)            2562        global_average_pooling2d_11[0][0]\n",
      "==================================================================================================\n",
      "Total params: 2,259,970\n",
      "Trainable params: 2,225,858\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "model = MobileNetV2(weights=None, input_shape=(num_rows, num_columns, num_channels), classes=2, include_top=True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11515 samples, validate on 2879 samples\n",
      "Epoch 1/72\n",
      "11515/11515 [==============================] - 20s 2ms/step - loss: 0.4223 - acc: 0.7937 - val_loss: 2.1439 - val_acc: 0.7457\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.14393, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 2/72\n",
      "11515/11515 [==============================] - 10s 853us/step - loss: 0.1467 - acc: 0.9450 - val_loss: 6.3776 - val_acc: 0.5346\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.14393\n",
      "Epoch 3/72\n",
      "11515/11515 [==============================] - 10s 860us/step - loss: 0.0761 - acc: 0.9739 - val_loss: 0.2978 - val_acc: 0.9535\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.14393 to 0.29780, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 4/72\n",
      "11515/11515 [==============================] - 10s 846us/step - loss: 0.0413 - acc: 0.9866 - val_loss: 4.4458 - val_acc: 0.6419\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.29780\n",
      "Epoch 5/72\n",
      "11515/11515 [==============================] - 10s 869us/step - loss: 0.0330 - acc: 0.9889 - val_loss: 0.9699 - val_acc: 0.9017\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.29780\n",
      "Epoch 6/72\n",
      "11515/11515 [==============================] - 10s 863us/step - loss: 0.0318 - acc: 0.9891 - val_loss: 0.2612 - val_acc: 0.9726\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.29780 to 0.26118, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 7/72\n",
      "11515/11515 [==============================] - 10s 868us/step - loss: 0.0241 - acc: 0.9912 - val_loss: 0.2731 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.26118\n",
      "Epoch 8/72\n",
      "11515/11515 [==============================] - 10s 864us/step - loss: 0.0196 - acc: 0.9937 - val_loss: 0.7440 - val_acc: 0.9281\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.26118\n",
      "Epoch 9/72\n",
      "11515/11515 [==============================] - 10s 854us/step - loss: 0.0275 - acc: 0.9907 - val_loss: 4.3690 - val_acc: 0.6811\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.26118\n",
      "Epoch 10/72\n",
      "11515/11515 [==============================] - 10s 862us/step - loss: 0.0227 - acc: 0.9924 - val_loss: 0.8844 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.26118\n",
      "Epoch 11/72\n",
      "11515/11515 [==============================] - 10s 862us/step - loss: 0.0138 - acc: 0.9948 - val_loss: 0.9272 - val_acc: 0.9212\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.26118\n",
      "Epoch 12/72\n",
      "11515/11515 [==============================] - 10s 865us/step - loss: 0.0143 - acc: 0.9955 - val_loss: 1.2084 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.26118\n",
      "Epoch 13/72\n",
      "11515/11515 [==============================] - 10s 855us/step - loss: 0.0161 - acc: 0.9953 - val_loss: 0.2027 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.26118 to 0.20273, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 14/72\n",
      "11515/11515 [==============================] - 10s 868us/step - loss: 0.0141 - acc: 0.9959 - val_loss: 1.5937 - val_acc: 0.8722\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.20273\n",
      "Epoch 15/72\n",
      "11515/11515 [==============================] - 10s 860us/step - loss: 0.0181 - acc: 0.9945 - val_loss: 0.3854 - val_acc: 0.9680\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.20273\n",
      "Epoch 16/72\n",
      "11515/11515 [==============================] - 10s 860us/step - loss: 0.0082 - acc: 0.9971 - val_loss: 2.1763 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.20273\n",
      "Epoch 17/72\n",
      "11515/11515 [==============================] - 10s 852us/step - loss: 0.0155 - acc: 0.9959 - val_loss: 8.1058 - val_acc: 0.4925\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.20273\n",
      "Epoch 18/72\n",
      "11515/11515 [==============================] - 10s 869us/step - loss: 0.0171 - acc: 0.9941 - val_loss: 3.3349 - val_acc: 0.7513\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.20273\n",
      "Epoch 19/72\n",
      "11515/11515 [==============================] - 10s 869us/step - loss: 0.0197 - acc: 0.9924 - val_loss: 0.5529 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.20273\n",
      "Epoch 20/72\n",
      "11515/11515 [==============================] - 10s 857us/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.1421 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.20273 to 0.14215, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 21/72\n",
      "11515/11515 [==============================] - 10s 870us/step - loss: 0.0083 - acc: 0.9977 - val_loss: 0.1225 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.14215 to 0.12245, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 22/72\n",
      "11515/11515 [==============================] - 10s 855us/step - loss: 0.0138 - acc: 0.9960 - val_loss: 0.6953 - val_acc: 0.9344\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.12245\n",
      "Epoch 23/72\n",
      "11515/11515 [==============================] - 10s 848us/step - loss: 0.0117 - acc: 0.9961 - val_loss: 0.2294 - val_acc: 0.9781\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.12245\n",
      "Epoch 24/72\n",
      "11515/11515 [==============================] - 10s 865us/step - loss: 0.0100 - acc: 0.9969 - val_loss: 0.1866 - val_acc: 0.9823\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.12245\n",
      "Epoch 25/72\n",
      "11515/11515 [==============================] - 10s 872us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.1763 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.12245\n",
      "Epoch 26/72\n",
      "11515/11515 [==============================] - 10s 857us/step - loss: 0.0091 - acc: 0.9972 - val_loss: 0.3211 - val_acc: 0.9691\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.12245\n",
      "Epoch 27/72\n",
      "11515/11515 [==============================] - 10s 859us/step - loss: 0.0065 - acc: 0.9982 - val_loss: 0.6159 - val_acc: 0.9489\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.12245\n",
      "Epoch 28/72\n",
      "11515/11515 [==============================] - 10s 863us/step - loss: 0.0149 - acc: 0.9953 - val_loss: 0.2934 - val_acc: 0.9680\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.12245\n",
      "Epoch 29/72\n",
      "11515/11515 [==============================] - 10s 880us/step - loss: 0.0151 - acc: 0.9953 - val_loss: 2.2975 - val_acc: 0.8336\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.12245\n",
      "Epoch 30/72\n",
      "11515/11515 [==============================] - 10s 864us/step - loss: 0.0088 - acc: 0.9978 - val_loss: 2.0064 - val_acc: 0.8586\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.12245\n",
      "Epoch 31/72\n",
      "11515/11515 [==============================] - 10s 856us/step - loss: 0.0125 - acc: 0.9963 - val_loss: 0.1718 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.12245\n",
      "Epoch 32/72\n",
      "11515/11515 [==============================] - 10s 868us/step - loss: 0.0076 - acc: 0.9977 - val_loss: 0.3319 - val_acc: 0.9729\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.12245\n",
      "Epoch 33/72\n",
      "11515/11515 [==============================] - 10s 873us/step - loss: 0.0139 - acc: 0.9954 - val_loss: 0.7985 - val_acc: 0.9413\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.12245\n",
      "Epoch 34/72\n",
      "11515/11515 [==============================] - 10s 865us/step - loss: 0.0146 - acc: 0.9954 - val_loss: 0.3294 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.12245\n",
      "Epoch 35/72\n",
      "11515/11515 [==============================] - 10s 852us/step - loss: 0.0078 - acc: 0.9975 - val_loss: 0.2115 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.12245\n",
      "Epoch 36/72\n",
      "11515/11515 [==============================] - 10s 866us/step - loss: 0.0044 - acc: 0.9990 - val_loss: 0.1865 - val_acc: 0.9854\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.12245\n",
      "Epoch 37/72\n",
      "11515/11515 [==============================] - 10s 856us/step - loss: 0.0014 - acc: 0.9995 - val_loss: 0.2655 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.12245\n",
      "Epoch 38/72\n",
      "11515/11515 [==============================] - 10s 873us/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.3977 - val_acc: 0.9646\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.12245\n",
      "Epoch 39/72\n",
      "11515/11515 [==============================] - 10s 867us/step - loss: 0.0090 - acc: 0.9969 - val_loss: 0.4237 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.12245\n",
      "Epoch 40/72\n",
      "11515/11515 [==============================] - 10s 872us/step - loss: 0.0118 - acc: 0.9958 - val_loss: 0.1681 - val_acc: 0.9868\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.12245\n",
      "Epoch 41/72\n",
      "11515/11515 [==============================] - 10s 866us/step - loss: 0.0090 - acc: 0.9977 - val_loss: 0.5496 - val_acc: 0.9576\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.12245\n",
      "Epoch 42/72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11515/11515 [==============================] - 10s 864us/step - loss: 0.0061 - acc: 0.9977 - val_loss: 0.7551 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.12245\n",
      "Epoch 43/72\n",
      "11515/11515 [==============================] - 10s 872us/step - loss: 0.0084 - acc: 0.9974 - val_loss: 0.7450 - val_acc: 0.9403\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.12245\n",
      "Epoch 44/72\n",
      "11515/11515 [==============================] - 10s 858us/step - loss: 0.0068 - acc: 0.9979 - val_loss: 0.6886 - val_acc: 0.9458\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.12245\n",
      "Epoch 45/72\n",
      "11515/11515 [==============================] - 10s 859us/step - loss: 0.0084 - acc: 0.9974 - val_loss: 0.4433 - val_acc: 0.9587\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.12245\n",
      "Epoch 46/72\n",
      "11515/11515 [==============================] - 10s 871us/step - loss: 0.0066 - acc: 0.9983 - val_loss: 0.1272 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.12245\n",
      "Epoch 47/72\n",
      "11515/11515 [==============================] - 10s 882us/step - loss: 0.0056 - acc: 0.9980 - val_loss: 0.1101 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.12245 to 0.11010, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 48/72\n",
      "11515/11515 [==============================] - 10s 864us/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0725 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.11010 to 0.07247, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 49/72\n",
      "11515/11515 [==============================] - 10s 869us/step - loss: 0.0037 - acc: 0.9986 - val_loss: 0.2739 - val_acc: 0.9705\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.07247\n",
      "Epoch 50/72\n",
      "11515/11515 [==============================] - 10s 860us/step - loss: 0.0087 - acc: 0.9972 - val_loss: 0.1876 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.07247\n",
      "Epoch 51/72\n",
      "11515/11515 [==============================] - 10s 866us/step - loss: 0.0073 - acc: 0.9977 - val_loss: 0.2121 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.07247\n",
      "Epoch 52/72\n",
      "11515/11515 [==============================] - 10s 870us/step - loss: 0.0031 - acc: 0.9991 - val_loss: 0.1141 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.07247\n",
      "Epoch 53/72\n",
      "11515/11515 [==============================] - 10s 868us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.1454 - val_acc: 0.9868\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.07247\n",
      "Epoch 54/72\n",
      "11515/11515 [==============================] - 10s 869us/step - loss: 0.0132 - acc: 0.9958 - val_loss: 1.6838 - val_acc: 0.8823\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.07247\n",
      "Epoch 55/72\n",
      "11515/11515 [==============================] - 10s 881us/step - loss: 0.0136 - acc: 0.9956 - val_loss: 0.2057 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.07247\n",
      "Epoch 56/72\n",
      "11515/11515 [==============================] - 10s 870us/step - loss: 0.0042 - acc: 0.9990 - val_loss: 0.0843 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.07247\n",
      "Epoch 57/72\n",
      "11515/11515 [==============================] - 10s 864us/step - loss: 1.5044e-04 - acc: 1.0000 - val_loss: 0.0647 - val_acc: 0.9951\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.07247 to 0.06466, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 58/72\n",
      "11515/11515 [==============================] - 10s 878us/step - loss: 2.8776e-05 - acc: 1.0000 - val_loss: 0.0436 - val_acc: 0.9951\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.06466 to 0.04364, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 59/72\n",
      "11515/11515 [==============================] - 10s 882us/step - loss: 7.7165e-05 - acc: 1.0000 - val_loss: 0.0623 - val_acc: 0.9931\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.04364\n",
      "Epoch 60/72\n",
      "11515/11515 [==============================] - 10s 866us/step - loss: 0.0148 - acc: 0.9952 - val_loss: 2.6205 - val_acc: 0.6408\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.04364\n",
      "Epoch 61/72\n",
      "11515/11515 [==============================] - 10s 863us/step - loss: 0.0058 - acc: 0.9980 - val_loss: 0.6594 - val_acc: 0.9389\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.04364\n",
      "Epoch 62/72\n",
      "11515/11515 [==============================] - 10s 856us/step - loss: 0.0111 - acc: 0.9969 - val_loss: 0.2246 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.04364\n",
      "Epoch 63/72\n",
      "11515/11515 [==============================] - 10s 868us/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.2024 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.04364\n",
      "Epoch 64/72\n",
      "11515/11515 [==============================] - 10s 863us/step - loss: 4.8722e-04 - acc: 1.0000 - val_loss: 0.1036 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.04364\n",
      "Epoch 65/72\n",
      "11515/11515 [==============================] - 10s 863us/step - loss: 3.2393e-05 - acc: 1.0000 - val_loss: 0.0797 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.04364\n",
      "Epoch 66/72\n",
      "11515/11515 [==============================] - 10s 866us/step - loss: 1.1001e-05 - acc: 1.0000 - val_loss: 0.0508 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.04364\n",
      "Epoch 67/72\n",
      "11515/11515 [==============================] - 10s 863us/step - loss: 3.0253e-05 - acc: 1.0000 - val_loss: 0.0216 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.04364 to 0.02157, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 68/72\n",
      "11515/11515 [==============================] - 10s 862us/step - loss: 0.0158 - acc: 0.9944 - val_loss: 0.2634 - val_acc: 0.9764\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.02157\n",
      "Epoch 69/72\n",
      "11515/11515 [==============================] - 10s 853us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 0.0940 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.02157\n",
      "Epoch 70/72\n",
      "11515/11515 [==============================] - 10s 865us/step - loss: 0.0046 - acc: 0.9990 - val_loss: 0.1357 - val_acc: 0.9889\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.02157\n",
      "Epoch 71/72\n",
      "11515/11515 [==============================] - 10s 863us/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.1656 - val_acc: 0.9823\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.02157\n",
      "Epoch 72/72\n",
      "11515/11515 [==============================] - 10s 866us/step - loss: 0.0121 - acc: 0.9965 - val_loss: 0.2220 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.02157\n",
      "Training completed in time:  0:13:00.206528\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "num_epochs = 72\n",
    "num_batch_size = 64\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)\n",
    "!telegram-send \"Server: Habe alles gelernt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9906209292227529\n",
      "Testing Accuracy:  0.9826328583209206\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"m9_mobilenet2\"\n",
    "os.mkdir(os.path.join(os.getcwd(), \"Models\", folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Models/'+folder+'/weights.model')\n",
    "model.save_weights(\"Models/\"+folder+\"/model.h5\")\n",
    "model_json = model.to_json()\n",
    "with open(\"Models/\"+folder+\"/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Densenet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 40, 94, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 46, 100, 1)   0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)             (None, 20, 47, 64)   3136        zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1/bn (BatchNormalization)   (None, 20, 47, 64)   256         conv1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1/relu (Activation)         (None, 20, 47, 64)   0           conv1/bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 22, 49, 64)   0           conv1/relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 10, 24, 64)   0           zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 10, 24, 64)   256         pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation (None, 10, 24, 64)   0           conv2_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 10, 24, 128)  8192        conv2_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 10, 24, 128)  512         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 10, 24, 128)  0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 10, 24, 32)   36864       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenat (None, 10, 24, 96)   0           pool1[0][0]                      \n",
      "                                                                 conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormali (None, 10, 24, 96)   384         conv2_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation (None, 10, 24, 96)   0           conv2_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 10, 24, 128)  12288       conv2_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 10, 24, 128)  512         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 10, 24, 128)  0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 10, 24, 32)   36864       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenat (None, 10, 24, 128)  0           conv2_block1_concat[0][0]        \n",
      "                                                                 conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormali (None, 10, 24, 128)  512         conv2_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation (None, 10, 24, 128)  0           conv2_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 10, 24, 128)  16384       conv2_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 10, 24, 128)  512         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 10, 24, 128)  0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 10, 24, 32)   36864       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenat (None, 10, 24, 160)  0           conv2_block2_concat[0][0]        \n",
      "                                                                 conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormali (None, 10, 24, 160)  640         conv2_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation (None, 10, 24, 160)  0           conv2_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)    (None, 10, 24, 128)  20480       conv2_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormali (None, 10, 24, 128)  512         conv2_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation (None, 10, 24, 128)  0           conv2_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)    (None, 10, 24, 32)   36864       conv2_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenat (None, 10, 24, 192)  0           conv2_block3_concat[0][0]        \n",
      "                                                                 conv2_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormali (None, 10, 24, 192)  768         conv2_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation (None, 10, 24, 192)  0           conv2_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)    (None, 10, 24, 128)  24576       conv2_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormali (None, 10, 24, 128)  512         conv2_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation (None, 10, 24, 128)  0           conv2_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)    (None, 10, 24, 32)   36864       conv2_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenat (None, 10, 24, 224)  0           conv2_block4_concat[0][0]        \n",
      "                                                                 conv2_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormali (None, 10, 24, 224)  896         conv2_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation (None, 10, 24, 224)  0           conv2_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)    (None, 10, 24, 128)  28672       conv2_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormali (None, 10, 24, 128)  512         conv2_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation (None, 10, 24, 128)  0           conv2_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)    (None, 10, 24, 32)   36864       conv2_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenat (None, 10, 24, 256)  0           conv2_block5_concat[0][0]        \n",
      "                                                                 conv2_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)   (None, 10, 24, 256)  1024        conv2_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_relu (Activation)         (None, 10, 24, 256)  0           pool2_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)             (None, 10, 24, 128)  32768       pool2_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)   (None, 5, 12, 128)   0           pool2_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 5, 12, 128)   512         pool2_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation (None, 5, 12, 128)   0           conv3_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 5, 12, 128)   16384       conv3_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 5, 12, 128)   512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 5, 12, 128)   0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 5, 12, 32)    36864       conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenat (None, 5, 12, 160)   0           pool2_pool[0][0]                 \n",
      "                                                                 conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormali (None, 5, 12, 160)   640         conv3_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation (None, 5, 12, 160)   0           conv3_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 5, 12, 128)   20480       conv3_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 5, 12, 128)   512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 5, 12, 128)   0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 5, 12, 32)    36864       conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenat (None, 5, 12, 192)   0           conv3_block1_concat[0][0]        \n",
      "                                                                 conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormali (None, 5, 12, 192)   768         conv3_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation (None, 5, 12, 192)   0           conv3_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 5, 12, 128)   24576       conv3_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 5, 12, 128)   512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 5, 12, 128)   0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 5, 12, 32)    36864       conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenat (None, 5, 12, 224)   0           conv3_block2_concat[0][0]        \n",
      "                                                                 conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormali (None, 5, 12, 224)   896         conv3_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation (None, 5, 12, 224)   0           conv3_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 5, 12, 128)   28672       conv3_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 5, 12, 128)   512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 5, 12, 128)   0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 5, 12, 32)    36864       conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenat (None, 5, 12, 256)   0           conv3_block3_concat[0][0]        \n",
      "                                                                 conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormali (None, 5, 12, 256)   1024        conv3_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation (None, 5, 12, 256)   0           conv3_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, 5, 12, 128)   32768       conv3_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 5, 12, 128)   512         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 5, 12, 128)   0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, 5, 12, 32)    36864       conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenat (None, 5, 12, 288)   0           conv3_block4_concat[0][0]        \n",
      "                                                                 conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormali (None, 5, 12, 288)   1152        conv3_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation (None, 5, 12, 288)   0           conv3_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, 5, 12, 128)   36864       conv3_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 5, 12, 128)   512         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 5, 12, 128)   0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, 5, 12, 32)    36864       conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenat (None, 5, 12, 320)   0           conv3_block5_concat[0][0]        \n",
      "                                                                 conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormali (None, 5, 12, 320)   1280        conv3_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation (None, 5, 12, 320)   0           conv3_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, 5, 12, 128)   40960       conv3_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 5, 12, 128)   512         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 5, 12, 128)   0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, 5, 12, 32)    36864       conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenat (None, 5, 12, 352)   0           conv3_block6_concat[0][0]        \n",
      "                                                                 conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormali (None, 5, 12, 352)   1408        conv3_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation (None, 5, 12, 352)   0           conv3_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, 5, 12, 128)   45056       conv3_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 5, 12, 128)   512         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 5, 12, 128)   0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, 5, 12, 32)    36864       conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenat (None, 5, 12, 384)   0           conv3_block7_concat[0][0]        \n",
      "                                                                 conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormali (None, 5, 12, 384)   1536        conv3_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation (None, 5, 12, 384)   0           conv3_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)    (None, 5, 12, 128)   49152       conv3_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormali (None, 5, 12, 128)   512         conv3_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation (None, 5, 12, 128)   0           conv3_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)    (None, 5, 12, 32)    36864       conv3_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenat (None, 5, 12, 416)   0           conv3_block8_concat[0][0]        \n",
      "                                                                 conv3_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormal (None, 5, 12, 416)   1664        conv3_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activatio (None, 5, 12, 416)   0           conv3_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)   (None, 5, 12, 128)   53248       conv3_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormal (None, 5, 12, 128)   512         conv3_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activatio (None, 5, 12, 128)   0           conv3_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)   (None, 5, 12, 32)    36864       conv3_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatena (None, 5, 12, 448)   0           conv3_block9_concat[0][0]        \n",
      "                                                                 conv3_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormal (None, 5, 12, 448)   1792        conv3_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activatio (None, 5, 12, 448)   0           conv3_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)   (None, 5, 12, 128)   57344       conv3_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormal (None, 5, 12, 128)   512         conv3_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activatio (None, 5, 12, 128)   0           conv3_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)   (None, 5, 12, 32)    36864       conv3_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatena (None, 5, 12, 480)   0           conv3_block10_concat[0][0]       \n",
      "                                                                 conv3_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormal (None, 5, 12, 480)   1920        conv3_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activatio (None, 5, 12, 480)   0           conv3_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)   (None, 5, 12, 128)   61440       conv3_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormal (None, 5, 12, 128)   512         conv3_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activatio (None, 5, 12, 128)   0           conv3_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)   (None, 5, 12, 32)    36864       conv3_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatena (None, 5, 12, 512)   0           conv3_block11_concat[0][0]       \n",
      "                                                                 conv3_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)   (None, 5, 12, 512)   2048        conv3_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_relu (Activation)         (None, 5, 12, 512)   0           pool3_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)             (None, 5, 12, 256)   131072      pool3_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)   (None, 2, 6, 256)    0           pool3_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 2, 6, 256)    1024        pool3_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation (None, 2, 6, 256)    0           conv4_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 2, 6, 128)    32768       conv4_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 2, 6, 128)    512         conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 2, 6, 128)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 2, 6, 32)     36864       conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenat (None, 2, 6, 288)    0           pool3_pool[0][0]                 \n",
      "                                                                 conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormali (None, 2, 6, 288)    1152        conv4_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation (None, 2, 6, 288)    0           conv4_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 2, 6, 128)    36864       conv4_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 2, 6, 128)    512         conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 2, 6, 128)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 2, 6, 32)     36864       conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenat (None, 2, 6, 320)    0           conv4_block1_concat[0][0]        \n",
      "                                                                 conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormali (None, 2, 6, 320)    1280        conv4_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation (None, 2, 6, 320)    0           conv4_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 2, 6, 128)    40960       conv4_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 2, 6, 128)    512         conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 2, 6, 128)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 2, 6, 32)     36864       conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenat (None, 2, 6, 352)    0           conv4_block2_concat[0][0]        \n",
      "                                                                 conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormali (None, 2, 6, 352)    1408        conv4_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation (None, 2, 6, 352)    0           conv4_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 2, 6, 128)    45056       conv4_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 2, 6, 128)    512         conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 2, 6, 128)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 2, 6, 32)     36864       conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenat (None, 2, 6, 384)    0           conv4_block3_concat[0][0]        \n",
      "                                                                 conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormali (None, 2, 6, 384)    1536        conv4_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation (None, 2, 6, 384)    0           conv4_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 2, 6, 128)    49152       conv4_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 2, 6, 128)    512         conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 2, 6, 128)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 2, 6, 32)     36864       conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenat (None, 2, 6, 416)    0           conv4_block4_concat[0][0]        \n",
      "                                                                 conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormali (None, 2, 6, 416)    1664        conv4_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation (None, 2, 6, 416)    0           conv4_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 2, 6, 128)    53248       conv4_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 2, 6, 128)    512         conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 2, 6, 128)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 2, 6, 32)     36864       conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenat (None, 2, 6, 448)    0           conv4_block5_concat[0][0]        \n",
      "                                                                 conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormali (None, 2, 6, 448)    1792        conv4_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation (None, 2, 6, 448)    0           conv4_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 2, 6, 128)    57344       conv4_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 2, 6, 128)    512         conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 2, 6, 128)    0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 2, 6, 32)     36864       conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenat (None, 2, 6, 480)    0           conv4_block6_concat[0][0]        \n",
      "                                                                 conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormali (None, 2, 6, 480)    1920        conv4_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation (None, 2, 6, 480)    0           conv4_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 2, 6, 128)    61440       conv4_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 2, 6, 128)    512         conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 2, 6, 128)    0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 2, 6, 32)     36864       conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenat (None, 2, 6, 512)    0           conv4_block7_concat[0][0]        \n",
      "                                                                 conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormali (None, 2, 6, 512)    2048        conv4_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation (None, 2, 6, 512)    0           conv4_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 2, 6, 128)    65536       conv4_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 2, 6, 128)    512         conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 2, 6, 128)    0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 2, 6, 32)     36864       conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenat (None, 2, 6, 544)    0           conv4_block8_concat[0][0]        \n",
      "                                                                 conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormal (None, 2, 6, 544)    2176        conv4_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activatio (None, 2, 6, 544)    0           conv4_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 2, 6, 128)    69632       conv4_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 2, 6, 128)    512         conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 2, 6, 128)    0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 2, 6, 32)     36864       conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatena (None, 2, 6, 576)    0           conv4_block9_concat[0][0]        \n",
      "                                                                 conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormal (None, 2, 6, 576)    2304        conv4_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activatio (None, 2, 6, 576)    0           conv4_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 2, 6, 128)    73728       conv4_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 2, 6, 128)    512         conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 2, 6, 128)    0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 2, 6, 32)     36864       conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatena (None, 2, 6, 608)    0           conv4_block10_concat[0][0]       \n",
      "                                                                 conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormal (None, 2, 6, 608)    2432        conv4_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activatio (None, 2, 6, 608)    0           conv4_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 2, 6, 128)    77824       conv4_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 2, 6, 128)    512         conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 2, 6, 128)    0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 2, 6, 32)     36864       conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatena (None, 2, 6, 640)    0           conv4_block11_concat[0][0]       \n",
      "                                                                 conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormal (None, 2, 6, 640)    2560        conv4_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activatio (None, 2, 6, 640)    0           conv4_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 2, 6, 128)    81920       conv4_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 2, 6, 128)    512         conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 2, 6, 128)    0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 2, 6, 32)     36864       conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatena (None, 2, 6, 672)    0           conv4_block12_concat[0][0]       \n",
      "                                                                 conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormal (None, 2, 6, 672)    2688        conv4_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activatio (None, 2, 6, 672)    0           conv4_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 2, 6, 128)    86016       conv4_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 2, 6, 128)    512         conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 2, 6, 128)    0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 2, 6, 32)     36864       conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatena (None, 2, 6, 704)    0           conv4_block13_concat[0][0]       \n",
      "                                                                 conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormal (None, 2, 6, 704)    2816        conv4_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activatio (None, 2, 6, 704)    0           conv4_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 2, 6, 128)    90112       conv4_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 2, 6, 128)    512         conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 2, 6, 128)    0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 2, 6, 32)     36864       conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatena (None, 2, 6, 736)    0           conv4_block14_concat[0][0]       \n",
      "                                                                 conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormal (None, 2, 6, 736)    2944        conv4_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activatio (None, 2, 6, 736)    0           conv4_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 2, 6, 128)    94208       conv4_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 2, 6, 128)    512         conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 2, 6, 128)    0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 2, 6, 32)     36864       conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatena (None, 2, 6, 768)    0           conv4_block15_concat[0][0]       \n",
      "                                                                 conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormal (None, 2, 6, 768)    3072        conv4_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activatio (None, 2, 6, 768)    0           conv4_block17_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 2, 6, 128)    98304       conv4_block17_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 2, 6, 128)    512         conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 2, 6, 128)    0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 2, 6, 32)     36864       conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatena (None, 2, 6, 800)    0           conv4_block16_concat[0][0]       \n",
      "                                                                 conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormal (None, 2, 6, 800)    3200        conv4_block17_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activatio (None, 2, 6, 800)    0           conv4_block18_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 2, 6, 128)    102400      conv4_block18_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 2, 6, 128)    512         conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 2, 6, 128)    0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 2, 6, 32)     36864       conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatena (None, 2, 6, 832)    0           conv4_block17_concat[0][0]       \n",
      "                                                                 conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormal (None, 2, 6, 832)    3328        conv4_block18_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activatio (None, 2, 6, 832)    0           conv4_block19_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 2, 6, 128)    106496      conv4_block19_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 2, 6, 128)    512         conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 2, 6, 128)    0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 2, 6, 32)     36864       conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatena (None, 2, 6, 864)    0           conv4_block18_concat[0][0]       \n",
      "                                                                 conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormal (None, 2, 6, 864)    3456        conv4_block19_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activatio (None, 2, 6, 864)    0           conv4_block20_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 2, 6, 128)    110592      conv4_block20_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 2, 6, 128)    512         conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 2, 6, 128)    0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 2, 6, 32)     36864       conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatena (None, 2, 6, 896)    0           conv4_block19_concat[0][0]       \n",
      "                                                                 conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormal (None, 2, 6, 896)    3584        conv4_block20_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activatio (None, 2, 6, 896)    0           conv4_block21_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 2, 6, 128)    114688      conv4_block21_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 2, 6, 128)    512         conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 2, 6, 128)    0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 2, 6, 32)     36864       conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatena (None, 2, 6, 928)    0           conv4_block20_concat[0][0]       \n",
      "                                                                 conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormal (None, 2, 6, 928)    3712        conv4_block21_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activatio (None, 2, 6, 928)    0           conv4_block22_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 2, 6, 128)    118784      conv4_block22_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 2, 6, 128)    512         conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 2, 6, 128)    0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 2, 6, 32)     36864       conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatena (None, 2, 6, 960)    0           conv4_block21_concat[0][0]       \n",
      "                                                                 conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormal (None, 2, 6, 960)    3840        conv4_block22_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activatio (None, 2, 6, 960)    0           conv4_block23_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 2, 6, 128)    122880      conv4_block23_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 2, 6, 128)    512         conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 2, 6, 128)    0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 2, 6, 32)     36864       conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatena (None, 2, 6, 992)    0           conv4_block22_concat[0][0]       \n",
      "                                                                 conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormal (None, 2, 6, 992)    3968        conv4_block23_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activatio (None, 2, 6, 992)    0           conv4_block24_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)   (None, 2, 6, 128)    126976      conv4_block24_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, 2, 6, 128)    512         conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activatio (None, 2, 6, 128)    0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)   (None, 2, 6, 32)     36864       conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatena (None, 2, 6, 1024)   0           conv4_block23_concat[0][0]       \n",
      "                                                                 conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)   (None, 2, 6, 1024)   4096        conv4_block24_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_relu (Activation)         (None, 2, 6, 1024)   0           pool4_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)             (None, 2, 6, 512)    524288      pool4_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)   (None, 1, 3, 512)    0           pool4_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 1, 3, 512)    2048        pool4_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation (None, 1, 3, 512)    0           conv5_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 1, 3, 128)    65536       conv5_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 1, 3, 128)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenat (None, 1, 3, 544)    0           pool4_pool[0][0]                 \n",
      "                                                                 conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormali (None, 1, 3, 544)    2176        conv5_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation (None, 1, 3, 544)    0           conv5_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 1, 3, 128)    69632       conv5_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 1, 3, 128)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenat (None, 1, 3, 576)    0           conv5_block1_concat[0][0]        \n",
      "                                                                 conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormali (None, 1, 3, 576)    2304        conv5_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation (None, 1, 3, 576)    0           conv5_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 1, 3, 128)    73728       conv5_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 1, 3, 128)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenat (None, 1, 3, 608)    0           conv5_block2_concat[0][0]        \n",
      "                                                                 conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormali (None, 1, 3, 608)    2432        conv5_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation (None, 1, 3, 608)    0           conv5_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv2D)    (None, 1, 3, 128)    77824       conv5_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation (None, 1, 3, 128)    0           conv5_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenat (None, 1, 3, 640)    0           conv5_block3_concat[0][0]        \n",
      "                                                                 conv5_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormali (None, 1, 3, 640)    2560        conv5_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation (None, 1, 3, 640)    0           conv5_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv2D)    (None, 1, 3, 128)    81920       conv5_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation (None, 1, 3, 128)    0           conv5_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenat (None, 1, 3, 672)    0           conv5_block4_concat[0][0]        \n",
      "                                                                 conv5_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormali (None, 1, 3, 672)    2688        conv5_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation (None, 1, 3, 672)    0           conv5_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv2D)    (None, 1, 3, 128)    86016       conv5_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation (None, 1, 3, 128)    0           conv5_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenat (None, 1, 3, 704)    0           conv5_block5_concat[0][0]        \n",
      "                                                                 conv5_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormali (None, 1, 3, 704)    2816        conv5_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation (None, 1, 3, 704)    0           conv5_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv2D)    (None, 1, 3, 128)    90112       conv5_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation (None, 1, 3, 128)    0           conv5_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenat (None, 1, 3, 736)    0           conv5_block6_concat[0][0]        \n",
      "                                                                 conv5_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormali (None, 1, 3, 736)    2944        conv5_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation (None, 1, 3, 736)    0           conv5_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv2D)    (None, 1, 3, 128)    94208       conv5_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation (None, 1, 3, 128)    0           conv5_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenat (None, 1, 3, 768)    0           conv5_block7_concat[0][0]        \n",
      "                                                                 conv5_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormali (None, 1, 3, 768)    3072        conv5_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation (None, 1, 3, 768)    0           conv5_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv2D)    (None, 1, 3, 128)    98304       conv5_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation (None, 1, 3, 128)    0           conv5_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenat (None, 1, 3, 800)    0           conv5_block8_concat[0][0]        \n",
      "                                                                 conv5_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormal (None, 1, 3, 800)    3200        conv5_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activatio (None, 1, 3, 800)    0           conv5_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv2D)   (None, 1, 3, 128)    102400      conv5_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormal (None, 1, 3, 128)    512         conv5_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activatio (None, 1, 3, 128)    0           conv5_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv2D)   (None, 1, 3, 32)     36864       conv5_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatena (None, 1, 3, 832)    0           conv5_block9_concat[0][0]        \n",
      "                                                                 conv5_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormal (None, 1, 3, 832)    3328        conv5_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activatio (None, 1, 3, 832)    0           conv5_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv2D)   (None, 1, 3, 128)    106496      conv5_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormal (None, 1, 3, 128)    512         conv5_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activatio (None, 1, 3, 128)    0           conv5_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv2D)   (None, 1, 3, 32)     36864       conv5_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatena (None, 1, 3, 864)    0           conv5_block10_concat[0][0]       \n",
      "                                                                 conv5_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormal (None, 1, 3, 864)    3456        conv5_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activatio (None, 1, 3, 864)    0           conv5_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv2D)   (None, 1, 3, 128)    110592      conv5_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormal (None, 1, 3, 128)    512         conv5_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activatio (None, 1, 3, 128)    0           conv5_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv2D)   (None, 1, 3, 32)     36864       conv5_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatena (None, 1, 3, 896)    0           conv5_block11_concat[0][0]       \n",
      "                                                                 conv5_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormal (None, 1, 3, 896)    3584        conv5_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activatio (None, 1, 3, 896)    0           conv5_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_conv (Conv2D)   (None, 1, 3, 128)    114688      conv5_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormal (None, 1, 3, 128)    512         conv5_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activatio (None, 1, 3, 128)    0           conv5_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv2D)   (None, 1, 3, 32)     36864       conv5_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatena (None, 1, 3, 928)    0           conv5_block12_concat[0][0]       \n",
      "                                                                 conv5_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormal (None, 1, 3, 928)    3712        conv5_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activatio (None, 1, 3, 928)    0           conv5_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv2D)   (None, 1, 3, 128)    118784      conv5_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormal (None, 1, 3, 128)    512         conv5_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activatio (None, 1, 3, 128)    0           conv5_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv2D)   (None, 1, 3, 32)     36864       conv5_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatena (None, 1, 3, 960)    0           conv5_block13_concat[0][0]       \n",
      "                                                                 conv5_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormal (None, 1, 3, 960)    3840        conv5_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activatio (None, 1, 3, 960)    0           conv5_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv2D)   (None, 1, 3, 128)    122880      conv5_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormal (None, 1, 3, 128)    512         conv5_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activatio (None, 1, 3, 128)    0           conv5_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv2D)   (None, 1, 3, 32)     36864       conv5_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatena (None, 1, 3, 992)    0           conv5_block14_concat[0][0]       \n",
      "                                                                 conv5_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormal (None, 1, 3, 992)    3968        conv5_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activatio (None, 1, 3, 992)    0           conv5_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv2D)   (None, 1, 3, 128)    126976      conv5_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormal (None, 1, 3, 128)    512         conv5_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activatio (None, 1, 3, 128)    0           conv5_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv2D)   (None, 1, 3, 32)     36864       conv5_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatena (None, 1, 3, 1024)   0           conv5_block15_concat[0][0]       \n",
      "                                                                 conv5_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 1, 3, 1024)   4096        conv5_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "relu (Activation)               (None, 1, 3, 1024)   0           bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 1024)         0           relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc1000 (Dense)                  (None, 2)            2050        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,033,282\n",
      "Trainable params: 6,949,634\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.densenet import DenseNet121\n",
    "num_rows = 40 #40\n",
    "num_columns = 94 #1\n",
    "num_channels = 1\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "model = 0\n",
    "model = DenseNet121(weights=None, input_shape=(num_rows, num_columns, num_channels), classes=2, include_top=True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11515 samples, validate on 2879 samples\n",
      "Epoch 1/72\n",
      "11515/11515 [==============================] - 39s 3ms/step - loss: 0.1655 - acc: 0.9357 - val_loss: 0.8182 - val_acc: 0.7506\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.81820, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 2/72\n",
      "11515/11515 [==============================] - 20s 2ms/step - loss: 0.0725 - acc: 0.9734 - val_loss: 0.1083 - val_acc: 0.9653\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.81820 to 0.10830, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 3/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0415 - acc: 0.9865 - val_loss: 0.2893 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.10830\n",
      "Epoch 4/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0380 - acc: 0.9871 - val_loss: 0.2779 - val_acc: 0.9291\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.10830\n",
      "Epoch 5/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0252 - acc: 0.9921 - val_loss: 0.0127 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.10830 to 0.01268, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 6/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0177 - acc: 0.9943 - val_loss: 0.0937 - val_acc: 0.9667\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01268\n",
      "Epoch 7/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0224 - acc: 0.9918 - val_loss: 0.0347 - val_acc: 0.9878\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01268\n",
      "Epoch 8/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0115 - acc: 0.9964 - val_loss: 0.0461 - val_acc: 0.9809\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01268\n",
      "Epoch 9/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0148 - acc: 0.9941 - val_loss: 0.0179 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01268\n",
      "Epoch 10/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0168 - acc: 0.9949 - val_loss: 0.0920 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.01268\n",
      "Epoch 11/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0192 - acc: 0.9936 - val_loss: 0.0296 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.01268\n",
      "Epoch 12/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0083 - acc: 0.9970 - val_loss: 0.0161 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01268\n",
      "Epoch 13/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0053 - acc: 0.9980 - val_loss: 0.0099 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01268 to 0.00993, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 14/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0093 - acc: 0.9964 - val_loss: 0.0374 - val_acc: 0.9854\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00993\n",
      "Epoch 15/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0091 - acc: 0.9970 - val_loss: 0.0334 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00993\n",
      "Epoch 16/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0085 - acc: 0.9971 - val_loss: 0.0020 - val_acc: 0.9997\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00993 to 0.00200, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 17/72\n",
      "11515/11515 [==============================] - 20s 2ms/step - loss: 0.0062 - acc: 0.9979 - val_loss: 0.0203 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00200\n",
      "Epoch 18/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0156 - acc: 0.9948 - val_loss: 0.0193 - val_acc: 0.9934\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00200\n",
      "Epoch 19/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0123 - acc: 0.9963 - val_loss: 0.0100 - val_acc: 0.9965\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00200\n",
      "Epoch 20/72\n",
      "11515/11515 [==============================] - 20s 2ms/step - loss: 0.0085 - acc: 0.9970 - val_loss: 0.0088 - val_acc: 0.9976\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00200\n",
      "Epoch 21/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.0022 - val_acc: 0.9993\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00200\n",
      "Epoch 22/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 1.3434e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 0.9993\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00200 to 0.00110, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 23/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 1.8972e-04 - acc: 1.0000 - val_loss: 6.3609e-04 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00110 to 0.00064, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 24/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 5.9249e-05 - acc: 1.0000 - val_loss: 6.1526e-04 - val_acc: 0.9997\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00064 to 0.00062, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 25/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 3.6613e-05 - acc: 1.0000 - val_loss: 5.2314e-04 - val_acc: 0.9997\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00062 to 0.00052, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 26/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 1.6188e-05 - acc: 1.0000 - val_loss: 3.9433e-04 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00052 to 0.00039, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 27/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 1.7769e-05 - acc: 1.0000 - val_loss: 3.4848e-04 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00039 to 0.00035, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 28/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0046 - acc: 0.9988 - val_loss: 1.3011 - val_acc: 0.8562\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00035\n",
      "Epoch 29/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0259 - acc: 0.9917 - val_loss: 0.1197 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00035\n",
      "Epoch 30/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0156 - acc: 0.9952 - val_loss: 0.0366 - val_acc: 0.9882\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00035\n",
      "Epoch 31/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0058 - acc: 0.9982 - val_loss: 0.0280 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00035\n",
      "Epoch 32/72\n",
      "11515/11515 [==============================] - 20s 2ms/step - loss: 0.0080 - acc: 0.9975 - val_loss: 0.0419 - val_acc: 0.9875\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00035\n",
      "Epoch 33/72\n",
      "11515/11515 [==============================] - 20s 2ms/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.0056 - val_acc: 0.9983\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00035\n",
      "Epoch 34/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 5.3739e-04 - acc: 0.9999 - val_loss: 0.0035 - val_acc: 0.9986\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00035\n",
      "Epoch 35/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 1.4019e-04 - acc: 1.0000 - val_loss: 9.2589e-04 - val_acc: 0.9997\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00035\n",
      "Epoch 36/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 3.3800e-04 - acc: 0.9998 - val_loss: 0.0030 - val_acc: 0.9990\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00035\n",
      "Epoch 37/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 8.4270e-05 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9997\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00035\n",
      "Epoch 38/72\n",
      "11515/11515 [==============================] - 20s 2ms/step - loss: 4.1466e-05 - acc: 1.0000 - val_loss: 7.7813e-04 - val_acc: 0.9997\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00035\n",
      "Epoch 39/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0257 - acc: 0.9920 - val_loss: 0.0344 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00035\n",
      "Epoch 40/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0111 - acc: 0.9962 - val_loss: 0.0371 - val_acc: 0.9871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00035\n",
      "Epoch 41/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0265 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00035\n",
      "Epoch 42/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0043 - acc: 0.9989 - val_loss: 0.0052 - val_acc: 0.9986\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00035\n",
      "Epoch 43/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 2.1462e-04 - acc: 1.0000 - val_loss: 0.0039 - val_acc: 0.9990\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00035\n",
      "Epoch 44/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 1.1955e-04 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 0.9990\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00035\n",
      "Epoch 45/72\n",
      "11515/11515 [==============================] - 20s 2ms/step - loss: 1.1438e-04 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 0.9993\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00035\n",
      "Epoch 46/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 1.5055e-04 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 0.9993\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00035\n",
      "Epoch 47/72\n",
      "11515/11515 [==============================] - 20s 2ms/step - loss: 0.0037 - acc: 0.9988 - val_loss: 0.2247 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00035\n",
      "Epoch 48/72\n",
      "11515/11515 [==============================] - 20s 2ms/step - loss: 0.0169 - acc: 0.9944 - val_loss: 0.0106 - val_acc: 0.9969\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00035\n",
      "Epoch 49/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0052 - acc: 0.9981 - val_loss: 0.0090 - val_acc: 0.9965\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00035\n",
      "Epoch 50/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 9.5741e-04 - acc: 0.9997 - val_loss: 0.0026 - val_acc: 0.9997\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00035\n",
      "Epoch 51/72\n",
      "11515/11515 [==============================] - 20s 2ms/step - loss: 2.5337e-04 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9990\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00035\n",
      "Epoch 52/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 1.1663e-04 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 0.9993\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00035\n",
      "Epoch 53/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 6.8625e-04 - acc: 0.9997 - val_loss: 0.0419 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00035\n",
      "Epoch 54/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0105 - acc: 0.9964 - val_loss: 0.0280 - val_acc: 0.9934\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00035\n",
      "Epoch 55/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0037 - val_acc: 0.9986\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00035\n",
      "Epoch 56/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 5.2766e-04 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 0.9993\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00035\n",
      "Epoch 57/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0041 - acc: 0.9988 - val_loss: 0.0168 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00035\n",
      "Epoch 58/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0147 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00035\n",
      "Epoch 59/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0043 - acc: 0.9983 - val_loss: 0.0075 - val_acc: 0.9979\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00035\n",
      "Epoch 60/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0050 - acc: 0.9979 - val_loss: 0.0066 - val_acc: 0.9983\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00035\n",
      "Epoch 61/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 4.4155e-04 - acc: 0.9999 - val_loss: 0.0045 - val_acc: 0.9993\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00035\n",
      "Epoch 62/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 2.5704e-04 - acc: 0.9998 - val_loss: 0.0082 - val_acc: 0.9986\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00035\n",
      "Epoch 63/72\n",
      "11515/11515 [==============================] - 20s 2ms/step - loss: 0.0050 - acc: 0.9982 - val_loss: 0.0145 - val_acc: 0.9965\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00035\n",
      "Epoch 64/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0038 - acc: 0.9986 - val_loss: 0.0729 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00035\n",
      "Epoch 65/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0017 - acc: 0.9993 - val_loss: 0.0054 - val_acc: 0.9990\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00035\n",
      "Epoch 66/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 0.0011 - acc: 0.9995 - val_loss: 0.0073 - val_acc: 0.9986\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00035\n",
      "Epoch 67/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 6.0051e-04 - acc: 0.9998 - val_loss: 0.0084 - val_acc: 0.9986\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00035\n",
      "Epoch 68/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 8.2359e-05 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 0.9990\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00035\n",
      "Epoch 69/72\n",
      "11515/11515 [==============================] - 20s 2ms/step - loss: 5.8890e-05 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 0.9993\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00035\n",
      "Epoch 70/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 1.7769e-05 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 0.9993\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00035\n",
      "Epoch 71/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 1.5580e-05 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 0.9993\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00035\n",
      "Epoch 72/72\n",
      "11515/11515 [==============================] - 19s 2ms/step - loss: 3.2031e-05 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 0.9993\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00035\n",
      "Training completed in time:  0:25:26.920612\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "num_epochs = 72\n",
    "num_batch_size = 64\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)\n",
    "!telegram-send \"Server: Habe alles gelernt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  1.0\n",
      "Testing Accuracy:  0.9993053143452588\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"m10_densenet121\"\n",
    "os.mkdir(os.path.join(os.getcwd(), \"Models\", folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Models/'+folder+'/weights.model')\n",
    "model.save_weights(\"Models/\"+folder+\"/model.h5\")\n",
    "model_json = model.to_json()\n",
    "with open(\"Models/\"+folder+\"/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASNetMobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 40, 94, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "stem_conv1 (Conv2D)             (None, 19, 46, 32)   288         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "stem_bn1 (BatchNormalization)   (None, 19, 46, 32)   128         stem_conv1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_377 (Activation)     (None, 19, 46, 32)   0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_conv_1_stem_1 (Conv2D (None, 19, 46, 11)   352         activation_377[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reduction_bn_1_stem_1 (BatchNor (None, 19, 46, 11)   44          reduction_conv_1_stem_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_378 (Activation)     (None, 19, 46, 11)   0           reduction_bn_1_stem_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_380 (Activation)     (None, 19, 46, 32)   0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 23, 49, 11)   0           activation_378[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 25, 51, 32)   0           activation_380[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 10, 23, 11)   396         separable_conv_1_pad_reduction_le\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 10, 23, 11)   1920        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 10, 23, 11)   44          separable_conv_1_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 10, 23, 11)   44          separable_conv_1_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_379 (Activation)     (None, 10, 23, 11)   0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "activation_381 (Activation)     (None, 10, 23, 11)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 10, 23, 11)   396         activation_379[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 10, 23, 11)   660         activation_381[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_382 (Activation)     (None, 19, 46, 32)   0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 10, 23, 11)   44          separable_conv_2_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 10, 23, 11)   44          separable_conv_2_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 25, 51, 32)   0           activation_382[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_384 (Activation)     (None, 19, 46, 32)   0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_1_stem_1 (Add)    (None, 10, 23, 11)   0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 10, 23, 11)   1920        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 23, 49, 32)   0           activation_384[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_386 (Activation)     (None, 10, 23, 11)   0           reduction_add_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 10, 23, 11)   44          separable_conv_1_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 10, 23, 11)   1152        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 10, 23, 11)   220         activation_386[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_383 (Activation)     (None, 10, 23, 11)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 10, 23, 11)   44          separable_conv_1_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 10, 23, 11)   44          separable_conv_1_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_pad_1_stem_1 (ZeroPad (None, 21, 47, 11)   0           reduction_bn_1_stem_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 10, 23, 11)   660         activation_383[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_385 (Activation)     (None, 10, 23, 11)   0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "activation_387 (Activation)     (None, 10, 23, 11)   0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left2_stem_1 (MaxPool (None, 10, 23, 11)   0           reduction_pad_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 10, 23, 11)   44          separable_conv_2_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 10, 23, 11)   396         activation_385[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 10, 23, 11)   220         activation_387[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_relu_1_stem_2 (Activatio (None, 19, 46, 32)   0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_2_stem_1 (Add)    (None, 10, 23, 11)   0           reduction_left2_stem_1[0][0]     \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left3_stem_1 (Average (None, 10, 23, 11)   0           reduction_pad_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 10, 23, 11)   44          separable_conv_2_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left4_stem_1 (Average (None, 10, 23, 11)   0           reduction_add_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 10, 23, 11)   44          separable_conv_2_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_right5_stem_1 (MaxPoo (None, 10, 23, 11)   0           reduction_pad_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPadding2 (None, 20, 47, 32)   0           adjust_relu_1_stem_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add3_stem_1 (Add)     (None, 10, 23, 11)   0           reduction_left3_stem_1[0][0]     \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 10, 23, 11)   0           reduction_add_2_stem_1[0][0]     \n",
      "                                                                 reduction_left4_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add4_stem_1 (Add)     (None, 10, 23, 11)   0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 reduction_right5_stem_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_9 (Cropping2D)       (None, 19, 46, 32)   0           zero_padding2d_11[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reduction_concat_stem_1 (Concat (None, 10, 23, 44)   0           reduction_add_2_stem_1[0][0]     \n",
      "                                                                 reduction_add3_stem_1[0][0]      \n",
      "                                                                 add_9[0][0]                      \n",
      "                                                                 reduction_add4_stem_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_1_stem_2 (Avera (None, 10, 23, 32)   0           adjust_relu_1_stem_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_2_stem_2 (Avera (None, 10, 23, 32)   0           cropping2d_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_388 (Activation)     (None, 10, 23, 44)   0           reduction_concat_stem_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_1_stem_2 (Conv2D)   (None, 10, 23, 11)   352         adjust_avg_pool_1_stem_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_2_stem_2 (Conv2D)   (None, 10, 23, 11)   352         adjust_avg_pool_2_stem_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_conv_1_stem_2 (Conv2D (None, 10, 23, 22)   968         activation_388[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 10, 23, 22)   0           adjust_conv_1_stem_2[0][0]       \n",
      "                                                                 adjust_conv_2_stem_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "reduction_bn_1_stem_2 (BatchNor (None, 10, 23, 22)   88          reduction_conv_1_stem_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_stem_2 (BatchNormaliz (None, 10, 23, 22)   88          concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_389 (Activation)     (None, 10, 23, 22)   0           reduction_bn_1_stem_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_391 (Activation)     (None, 10, 23, 22)   0           adjust_bn_stem_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 13, 27, 22)   0           activation_389[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 15, 29, 22)   0           activation_391[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 5, 12, 22)    1034        separable_conv_1_pad_reduction_le\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 5, 12, 22)    1562        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 5, 12, 22)    88          separable_conv_1_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 5, 12, 22)    88          separable_conv_1_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_390 (Activation)     (None, 5, 12, 22)    0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "activation_392 (Activation)     (None, 5, 12, 22)    0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 5, 12, 22)    1034        activation_390[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 5, 12, 22)    1562        activation_392[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_393 (Activation)     (None, 10, 23, 22)   0           adjust_bn_stem_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 5, 12, 22)    88          separable_conv_2_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 5, 12, 22)    88          separable_conv_2_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 15, 29, 22)   0           activation_393[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_395 (Activation)     (None, 10, 23, 22)   0           adjust_bn_stem_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_1_stem_2 (Add)    (None, 5, 12, 22)    0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 5, 12, 22)    1562        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 13, 27, 22)   0           activation_395[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_397 (Activation)     (None, 5, 12, 22)    0           reduction_add_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 5, 12, 22)    88          separable_conv_1_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 5, 12, 22)    1034        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 5, 12, 22)    682         activation_397[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_394 (Activation)     (None, 5, 12, 22)    0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 5, 12, 22)    88          separable_conv_1_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 5, 12, 22)    88          separable_conv_1_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_pad_1_stem_2 (ZeroPad (None, 11, 25, 22)   0           reduction_bn_1_stem_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 5, 12, 22)    1562        activation_394[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_396 (Activation)     (None, 5, 12, 22)    0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "activation_398 (Activation)     (None, 5, 12, 22)    0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left2_stem_2 (MaxPool (None, 5, 12, 22)    0           reduction_pad_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 5, 12, 22)    88          separable_conv_2_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 5, 12, 22)    1034        activation_396[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 5, 12, 22)    682         activation_398[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_relu_1_0 (Activation)    (None, 10, 23, 44)   0           reduction_concat_stem_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_2_stem_2 (Add)    (None, 5, 12, 22)    0           reduction_left2_stem_2[0][0]     \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left3_stem_2 (Average (None, 5, 12, 22)    0           reduction_pad_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 5, 12, 22)    88          separable_conv_2_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left4_stem_2 (Average (None, 5, 12, 22)    0           reduction_add_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 5, 12, 22)    88          separable_conv_2_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_right5_stem_2 (MaxPoo (None, 5, 12, 22)    0           reduction_pad_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPadding2 (None, 11, 24, 44)   0           adjust_relu_1_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add3_stem_2 (Add)     (None, 5, 12, 22)    0           reduction_left3_stem_2[0][0]     \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 5, 12, 22)    0           reduction_add_2_stem_2[0][0]     \n",
      "                                                                 reduction_left4_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add4_stem_2 (Add)     (None, 5, 12, 22)    0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 reduction_right5_stem_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_10 (Cropping2D)      (None, 10, 23, 44)   0           zero_padding2d_12[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reduction_concat_stem_2 (Concat (None, 5, 12, 88)    0           reduction_add_2_stem_2[0][0]     \n",
      "                                                                 reduction_add3_stem_2[0][0]      \n",
      "                                                                 add_10[0][0]                     \n",
      "                                                                 reduction_add4_stem_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_1_0 (AveragePoo (None, 5, 12, 44)    0           adjust_relu_1_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_2_0 (AveragePoo (None, 5, 12, 44)    0           cropping2d_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_1_0 (Conv2D)        (None, 5, 12, 22)    968         adjust_avg_pool_1_0[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_2_0 (Conv2D)        (None, 5, 12, 22)    968         adjust_avg_pool_2_0[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_399 (Activation)     (None, 5, 12, 88)    0           reduction_concat_stem_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 5, 12, 44)    0           adjust_conv_1_0[0][0]            \n",
      "                                                                 adjust_conv_2_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_0 (Conv2D)        (None, 5, 12, 44)    3872        activation_399[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_0 (BatchNormalization (None, 5, 12, 44)    176         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_0 (BatchNormalizati (None, 5, 12, 44)    176         normal_conv_1_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_400 (Activation)     (None, 5, 12, 44)    0           normal_bn_1_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_402 (Activation)     (None, 5, 12, 44)    0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_404 (Activation)     (None, 5, 12, 44)    0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_406 (Activation)     (None, 5, 12, 44)    0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_408 (Activation)     (None, 5, 12, 44)    0           normal_bn_1_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_0 (None, 5, 12, 44)    3036        activation_400[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 5, 12, 44)    2332        activation_402[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_0 (None, 5, 12, 44)    3036        activation_404[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 5, 12, 44)    2332        activation_406[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_0 (None, 5, 12, 44)    2332        activation_408[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 5, 12, 44)    176         separable_conv_1_normal_left1_0[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 5, 12, 44)    176         separable_conv_1_normal_right1_0[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 5, 12, 44)    176         separable_conv_1_normal_left2_0[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 5, 12, 44)    176         separable_conv_1_normal_right2_0[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 5, 12, 44)    176         separable_conv_1_normal_left5_0[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_401 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_403 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_405 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_407 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_409 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_0 (None, 5, 12, 44)    3036        activation_401[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 5, 12, 44)    2332        activation_403[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_0 (None, 5, 12, 44)    3036        activation_405[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 5, 12, 44)    2332        activation_407[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_0 (None, 5, 12, 44)    2332        activation_409[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 5, 12, 44)    176         separable_conv_2_normal_left1_0[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 5, 12, 44)    176         separable_conv_2_normal_right1_0[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 5, 12, 44)    176         separable_conv_2_normal_left2_0[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 5, 12, 44)    176         separable_conv_2_normal_right2_0[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_0 (AveragePooling2 (None, 5, 12, 44)    0           normal_bn_1_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_0 (AveragePooling2 (None, 5, 12, 44)    0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_0 (AveragePooling (None, 5, 12, 44)    0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 5, 12, 44)    176         separable_conv_2_normal_left5_0[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_0 (Add)            (None, 5, 12, 44)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_0 (Add)            (None, 5, 12, 44)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_0 (Add)            (None, 5, 12, 44)    0           normal_left3_0[0][0]             \n",
      "                                                                 adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_0 (Add)            (None, 5, 12, 44)    0           normal_left4_0[0][0]             \n",
      "                                                                 normal_right4_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_0 (Add)            (None, 5, 12, 44)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_0 (Concatenate)   (None, 5, 12, 264)   0           adjust_bn_0[0][0]                \n",
      "                                                                 normal_add_1_0[0][0]             \n",
      "                                                                 normal_add_2_0[0][0]             \n",
      "                                                                 normal_add_3_0[0][0]             \n",
      "                                                                 normal_add_4_0[0][0]             \n",
      "                                                                 normal_add_5_0[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_410 (Activation)     (None, 5, 12, 88)    0           reduction_concat_stem_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_411 (Activation)     (None, 5, 12, 264)   0           normal_concat_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_1 (Conv2 (None, 5, 12, 44)    3872        activation_410[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_1 (Conv2D)        (None, 5, 12, 44)    11616       activation_411[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_1 (BatchNormalization (None, 5, 12, 44)    176         adjust_conv_projection_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_1 (BatchNormalizati (None, 5, 12, 44)    176         normal_conv_1_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_412 (Activation)     (None, 5, 12, 44)    0           normal_bn_1_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_414 (Activation)     (None, 5, 12, 44)    0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_416 (Activation)     (None, 5, 12, 44)    0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_418 (Activation)     (None, 5, 12, 44)    0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_420 (Activation)     (None, 5, 12, 44)    0           normal_bn_1_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 5, 12, 44)    3036        activation_412[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 5, 12, 44)    2332        activation_414[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 5, 12, 44)    3036        activation_416[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 5, 12, 44)    2332        activation_418[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 5, 12, 44)    2332        activation_420[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 5, 12, 44)    176         separable_conv_1_normal_left1_1[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 5, 12, 44)    176         separable_conv_1_normal_right1_1[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 5, 12, 44)    176         separable_conv_1_normal_left2_1[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 5, 12, 44)    176         separable_conv_1_normal_right2_1[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 5, 12, 44)    176         separable_conv_1_normal_left5_1[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_413 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_415 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_417 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_419 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_421 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 5, 12, 44)    3036        activation_413[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 5, 12, 44)    2332        activation_415[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 5, 12, 44)    3036        activation_417[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 5, 12, 44)    2332        activation_419[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 5, 12, 44)    2332        activation_421[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 5, 12, 44)    176         separable_conv_2_normal_left1_1[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 5, 12, 44)    176         separable_conv_2_normal_right1_1[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 5, 12, 44)    176         separable_conv_2_normal_left2_1[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 5, 12, 44)    176         separable_conv_2_normal_right2_1[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_1 (AveragePooling2 (None, 5, 12, 44)    0           normal_bn_1_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_1 (AveragePooling2 (None, 5, 12, 44)    0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_1 (AveragePooling (None, 5, 12, 44)    0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 5, 12, 44)    176         separable_conv_2_normal_left5_1[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_1 (Add)            (None, 5, 12, 44)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_1 (Add)            (None, 5, 12, 44)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_1 (Add)            (None, 5, 12, 44)    0           normal_left3_1[0][0]             \n",
      "                                                                 adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_1 (Add)            (None, 5, 12, 44)    0           normal_left4_1[0][0]             \n",
      "                                                                 normal_right4_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_1 (Add)            (None, 5, 12, 44)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_1 (Concatenate)   (None, 5, 12, 264)   0           adjust_bn_1[0][0]                \n",
      "                                                                 normal_add_1_1[0][0]             \n",
      "                                                                 normal_add_2_1[0][0]             \n",
      "                                                                 normal_add_3_1[0][0]             \n",
      "                                                                 normal_add_4_1[0][0]             \n",
      "                                                                 normal_add_5_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_422 (Activation)     (None, 5, 12, 264)   0           normal_concat_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_423 (Activation)     (None, 5, 12, 264)   0           normal_concat_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_2 (Conv2 (None, 5, 12, 44)    11616       activation_422[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_2 (Conv2D)        (None, 5, 12, 44)    11616       activation_423[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_2 (BatchNormalization (None, 5, 12, 44)    176         adjust_conv_projection_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_2 (BatchNormalizati (None, 5, 12, 44)    176         normal_conv_1_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_424 (Activation)     (None, 5, 12, 44)    0           normal_bn_1_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_426 (Activation)     (None, 5, 12, 44)    0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_428 (Activation)     (None, 5, 12, 44)    0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_430 (Activation)     (None, 5, 12, 44)    0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_432 (Activation)     (None, 5, 12, 44)    0           normal_bn_1_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_2 (None, 5, 12, 44)    3036        activation_424[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 5, 12, 44)    2332        activation_426[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_2 (None, 5, 12, 44)    3036        activation_428[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 5, 12, 44)    2332        activation_430[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_2 (None, 5, 12, 44)    2332        activation_432[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 5, 12, 44)    176         separable_conv_1_normal_left1_2[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 5, 12, 44)    176         separable_conv_1_normal_right1_2[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 5, 12, 44)    176         separable_conv_1_normal_left2_2[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 5, 12, 44)    176         separable_conv_1_normal_right2_2[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 5, 12, 44)    176         separable_conv_1_normal_left5_2[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_425 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_427 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_429 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_431 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_433 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_2 (None, 5, 12, 44)    3036        activation_425[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 5, 12, 44)    2332        activation_427[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_2 (None, 5, 12, 44)    3036        activation_429[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 5, 12, 44)    2332        activation_431[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_2 (None, 5, 12, 44)    2332        activation_433[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 5, 12, 44)    176         separable_conv_2_normal_left1_2[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 5, 12, 44)    176         separable_conv_2_normal_right1_2[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 5, 12, 44)    176         separable_conv_2_normal_left2_2[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 5, 12, 44)    176         separable_conv_2_normal_right2_2[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_2 (AveragePooling2 (None, 5, 12, 44)    0           normal_bn_1_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_2 (AveragePooling2 (None, 5, 12, 44)    0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_2 (AveragePooling (None, 5, 12, 44)    0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 5, 12, 44)    176         separable_conv_2_normal_left5_2[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_2 (Add)            (None, 5, 12, 44)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_2 (Add)            (None, 5, 12, 44)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_2 (Add)            (None, 5, 12, 44)    0           normal_left3_2[0][0]             \n",
      "                                                                 adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_2 (Add)            (None, 5, 12, 44)    0           normal_left4_2[0][0]             \n",
      "                                                                 normal_right4_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_2 (Add)            (None, 5, 12, 44)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_2 (Concatenate)   (None, 5, 12, 264)   0           adjust_bn_2[0][0]                \n",
      "                                                                 normal_add_1_2[0][0]             \n",
      "                                                                 normal_add_2_2[0][0]             \n",
      "                                                                 normal_add_3_2[0][0]             \n",
      "                                                                 normal_add_4_2[0][0]             \n",
      "                                                                 normal_add_5_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_434 (Activation)     (None, 5, 12, 264)   0           normal_concat_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_435 (Activation)     (None, 5, 12, 264)   0           normal_concat_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_3 (Conv2 (None, 5, 12, 44)    11616       activation_434[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_3 (Conv2D)        (None, 5, 12, 44)    11616       activation_435[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_3 (BatchNormalization (None, 5, 12, 44)    176         adjust_conv_projection_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_3 (BatchNormalizati (None, 5, 12, 44)    176         normal_conv_1_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_436 (Activation)     (None, 5, 12, 44)    0           normal_bn_1_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_438 (Activation)     (None, 5, 12, 44)    0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_440 (Activation)     (None, 5, 12, 44)    0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_442 (Activation)     (None, 5, 12, 44)    0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_444 (Activation)     (None, 5, 12, 44)    0           normal_bn_1_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_3 (None, 5, 12, 44)    3036        activation_436[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 5, 12, 44)    2332        activation_438[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_3 (None, 5, 12, 44)    3036        activation_440[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 5, 12, 44)    2332        activation_442[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_3 (None, 5, 12, 44)    2332        activation_444[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 5, 12, 44)    176         separable_conv_1_normal_left1_3[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 5, 12, 44)    176         separable_conv_1_normal_right1_3[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 5, 12, 44)    176         separable_conv_1_normal_left2_3[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 5, 12, 44)    176         separable_conv_1_normal_right2_3[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 5, 12, 44)    176         separable_conv_1_normal_left5_3[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_437 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_439 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_441 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_443 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_445 (Activation)     (None, 5, 12, 44)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_3 (None, 5, 12, 44)    3036        activation_437[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 5, 12, 44)    2332        activation_439[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_3 (None, 5, 12, 44)    3036        activation_441[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 5, 12, 44)    2332        activation_443[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_3 (None, 5, 12, 44)    2332        activation_445[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 5, 12, 44)    176         separable_conv_2_normal_left1_3[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 5, 12, 44)    176         separable_conv_2_normal_right1_3[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 5, 12, 44)    176         separable_conv_2_normal_left2_3[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 5, 12, 44)    176         separable_conv_2_normal_right2_3[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_3 (AveragePooling2 (None, 5, 12, 44)    0           normal_bn_1_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_3 (AveragePooling2 (None, 5, 12, 44)    0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_3 (AveragePooling (None, 5, 12, 44)    0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 5, 12, 44)    176         separable_conv_2_normal_left5_3[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_3 (Add)            (None, 5, 12, 44)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_3 (Add)            (None, 5, 12, 44)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_3 (Add)            (None, 5, 12, 44)    0           normal_left3_3[0][0]             \n",
      "                                                                 adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_3 (Add)            (None, 5, 12, 44)    0           normal_left4_3[0][0]             \n",
      "                                                                 normal_right4_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_3 (Add)            (None, 5, 12, 44)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_3 (Concatenate)   (None, 5, 12, 264)   0           adjust_bn_3[0][0]                \n",
      "                                                                 normal_add_1_3[0][0]             \n",
      "                                                                 normal_add_2_3[0][0]             \n",
      "                                                                 normal_add_3_3[0][0]             \n",
      "                                                                 normal_add_4_3[0][0]             \n",
      "                                                                 normal_add_5_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_447 (Activation)     (None, 5, 12, 264)   0           normal_concat_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_446 (Activation)     (None, 5, 12, 264)   0           normal_concat_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_conv_1_reduce_4 (Conv (None, 5, 12, 88)    23232       activation_447[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_reduce_4 (None, 5, 12, 88)    23232       activation_446[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reduction_bn_1_reduce_4 (BatchN (None, 5, 12, 88)    352         reduction_conv_1_reduce_4[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_reduce_4 (BatchNormal (None, 5, 12, 88)    352         adjust_conv_projection_reduce_4[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_448 (Activation)     (None, 5, 12, 88)    0           reduction_bn_1_reduce_4[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_450 (Activation)     (None, 5, 12, 88)    0           adjust_bn_reduce_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 9, 15, 88)    0           activation_448[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 11, 17, 88)   0           activation_450[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 3, 6, 88)     9944        separable_conv_1_pad_reduction_le\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 3, 6, 88)     12056       separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 3, 6, 88)     352         separable_conv_1_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 3, 6, 88)     352         separable_conv_1_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_449 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "activation_451 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 3, 6, 88)     9944        activation_449[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 3, 6, 88)     12056       activation_451[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_452 (Activation)     (None, 5, 12, 88)    0           adjust_bn_reduce_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 3, 6, 88)     352         separable_conv_2_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 3, 6, 88)     352         separable_conv_2_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 11, 17, 88)   0           activation_452[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_454 (Activation)     (None, 5, 12, 88)    0           adjust_bn_reduce_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_1_reduce_4 (Add)  (None, 3, 6, 88)     0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 3, 6, 88)     12056       separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 9, 15, 88)    0           activation_454[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_456 (Activation)     (None, 3, 6, 88)     0           reduction_add_1_reduce_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 3, 6, 88)     352         separable_conv_1_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 3, 6, 88)     9944        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 3, 6, 88)     8536        activation_456[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_453 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 3, 6, 88)     352         separable_conv_1_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 3, 6, 88)     352         separable_conv_1_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_pad_1_reduce_4 (ZeroP (None, 7, 13, 88)    0           reduction_bn_1_reduce_4[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 3, 6, 88)     12056       activation_453[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_455 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "activation_457 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left2_reduce_4 (MaxPo (None, 3, 6, 88)     0           reduction_pad_1_reduce_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 3, 6, 88)     352         separable_conv_2_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 3, 6, 88)     9944        activation_455[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 3, 6, 88)     8536        activation_457[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_relu_1_5 (Activation)    (None, 5, 12, 264)   0           normal_concat_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_2_reduce_4 (Add)  (None, 3, 6, 88)     0           reduction_left2_reduce_4[0][0]   \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left3_reduce_4 (Avera (None, 3, 6, 88)     0           reduction_pad_1_reduce_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 3, 6, 88)     352         separable_conv_2_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left4_reduce_4 (Avera (None, 3, 6, 88)     0           reduction_add_1_reduce_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 3, 6, 88)     352         separable_conv_2_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_right5_reduce_4 (MaxP (None, 3, 6, 88)     0           reduction_pad_1_reduce_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPadding2 (None, 6, 13, 264)   0           adjust_relu_1_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add3_reduce_4 (Add)   (None, 3, 6, 88)     0           reduction_left3_reduce_4[0][0]   \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 3, 6, 88)     0           reduction_add_2_reduce_4[0][0]   \n",
      "                                                                 reduction_left4_reduce_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add4_reduce_4 (Add)   (None, 3, 6, 88)     0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 reduction_right5_reduce_4[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_11 (Cropping2D)      (None, 5, 12, 264)   0           zero_padding2d_13[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reduction_concat_reduce_4 (Conc (None, 3, 6, 352)    0           reduction_add_2_reduce_4[0][0]   \n",
      "                                                                 reduction_add3_reduce_4[0][0]    \n",
      "                                                                 add_11[0][0]                     \n",
      "                                                                 reduction_add4_reduce_4[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_1_5 (AveragePoo (None, 3, 6, 264)    0           adjust_relu_1_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_2_5 (AveragePoo (None, 3, 6, 264)    0           cropping2d_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_1_5 (Conv2D)        (None, 3, 6, 44)     11616       adjust_avg_pool_1_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_2_5 (Conv2D)        (None, 3, 6, 44)     11616       adjust_avg_pool_2_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_458 (Activation)     (None, 3, 6, 352)    0           reduction_concat_reduce_4[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 3, 6, 88)     0           adjust_conv_1_5[0][0]            \n",
      "                                                                 adjust_conv_2_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_5 (Conv2D)        (None, 3, 6, 88)     30976       activation_458[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_5 (BatchNormalization (None, 3, 6, 88)     352         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_5 (BatchNormalizati (None, 3, 6, 88)     352         normal_conv_1_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_459 (Activation)     (None, 3, 6, 88)     0           normal_bn_1_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_461 (Activation)     (None, 3, 6, 88)     0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_463 (Activation)     (None, 3, 6, 88)     0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_465 (Activation)     (None, 3, 6, 88)     0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_467 (Activation)     (None, 3, 6, 88)     0           normal_bn_1_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_5 (None, 3, 6, 88)     9944        activation_459[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 3, 6, 88)     8536        activation_461[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_5 (None, 3, 6, 88)     9944        activation_463[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 3, 6, 88)     8536        activation_465[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_5 (None, 3, 6, 88)     8536        activation_467[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 3, 6, 88)     352         separable_conv_1_normal_left1_5[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 3, 6, 88)     352         separable_conv_1_normal_right1_5[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 3, 6, 88)     352         separable_conv_1_normal_left2_5[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 3, 6, 88)     352         separable_conv_1_normal_right2_5[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 3, 6, 88)     352         separable_conv_1_normal_left5_5[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_460 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_462 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_464 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_466 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_468 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_5 (None, 3, 6, 88)     9944        activation_460[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 3, 6, 88)     8536        activation_462[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_5 (None, 3, 6, 88)     9944        activation_464[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 3, 6, 88)     8536        activation_466[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_5 (None, 3, 6, 88)     8536        activation_468[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 3, 6, 88)     352         separable_conv_2_normal_left1_5[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 3, 6, 88)     352         separable_conv_2_normal_right1_5[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 3, 6, 88)     352         separable_conv_2_normal_left2_5[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 3, 6, 88)     352         separable_conv_2_normal_right2_5[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_5 (AveragePooling2 (None, 3, 6, 88)     0           normal_bn_1_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_5 (AveragePooling2 (None, 3, 6, 88)     0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_5 (AveragePooling (None, 3, 6, 88)     0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 3, 6, 88)     352         separable_conv_2_normal_left5_5[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_5 (Add)            (None, 3, 6, 88)     0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_5 (Add)            (None, 3, 6, 88)     0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_5 (Add)            (None, 3, 6, 88)     0           normal_left3_5[0][0]             \n",
      "                                                                 adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_5 (Add)            (None, 3, 6, 88)     0           normal_left4_5[0][0]             \n",
      "                                                                 normal_right4_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_5 (Add)            (None, 3, 6, 88)     0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_5 (Concatenate)   (None, 3, 6, 528)    0           adjust_bn_5[0][0]                \n",
      "                                                                 normal_add_1_5[0][0]             \n",
      "                                                                 normal_add_2_5[0][0]             \n",
      "                                                                 normal_add_3_5[0][0]             \n",
      "                                                                 normal_add_4_5[0][0]             \n",
      "                                                                 normal_add_5_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_469 (Activation)     (None, 3, 6, 352)    0           reduction_concat_reduce_4[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_470 (Activation)     (None, 3, 6, 528)    0           normal_concat_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_6 (Conv2 (None, 3, 6, 88)     30976       activation_469[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_6 (Conv2D)        (None, 3, 6, 88)     46464       activation_470[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_6 (BatchNormalization (None, 3, 6, 88)     352         adjust_conv_projection_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_6 (BatchNormalizati (None, 3, 6, 88)     352         normal_conv_1_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_471 (Activation)     (None, 3, 6, 88)     0           normal_bn_1_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_473 (Activation)     (None, 3, 6, 88)     0           adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_475 (Activation)     (None, 3, 6, 88)     0           adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_477 (Activation)     (None, 3, 6, 88)     0           adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_479 (Activation)     (None, 3, 6, 88)     0           normal_bn_1_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_6 (None, 3, 6, 88)     9944        activation_471[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 3, 6, 88)     8536        activation_473[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_6 (None, 3, 6, 88)     9944        activation_475[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 3, 6, 88)     8536        activation_477[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_6 (None, 3, 6, 88)     8536        activation_479[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 3, 6, 88)     352         separable_conv_1_normal_left1_6[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 3, 6, 88)     352         separable_conv_1_normal_right1_6[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 3, 6, 88)     352         separable_conv_1_normal_left2_6[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 3, 6, 88)     352         separable_conv_1_normal_right2_6[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 3, 6, 88)     352         separable_conv_1_normal_left5_6[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_472 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_474 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_476 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_478 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_480 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_6 (None, 3, 6, 88)     9944        activation_472[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 3, 6, 88)     8536        activation_474[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_6 (None, 3, 6, 88)     9944        activation_476[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 3, 6, 88)     8536        activation_478[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_6 (None, 3, 6, 88)     8536        activation_480[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 3, 6, 88)     352         separable_conv_2_normal_left1_6[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 3, 6, 88)     352         separable_conv_2_normal_right1_6[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 3, 6, 88)     352         separable_conv_2_normal_left2_6[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 3, 6, 88)     352         separable_conv_2_normal_right2_6[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_6 (AveragePooling2 (None, 3, 6, 88)     0           normal_bn_1_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_6 (AveragePooling2 (None, 3, 6, 88)     0           adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_6 (AveragePooling (None, 3, 6, 88)     0           adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 3, 6, 88)     352         separable_conv_2_normal_left5_6[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_6 (Add)            (None, 3, 6, 88)     0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_6 (Add)            (None, 3, 6, 88)     0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_6 (Add)            (None, 3, 6, 88)     0           normal_left3_6[0][0]             \n",
      "                                                                 adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_6 (Add)            (None, 3, 6, 88)     0           normal_left4_6[0][0]             \n",
      "                                                                 normal_right4_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_6 (Add)            (None, 3, 6, 88)     0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_6 (Concatenate)   (None, 3, 6, 528)    0           adjust_bn_6[0][0]                \n",
      "                                                                 normal_add_1_6[0][0]             \n",
      "                                                                 normal_add_2_6[0][0]             \n",
      "                                                                 normal_add_3_6[0][0]             \n",
      "                                                                 normal_add_4_6[0][0]             \n",
      "                                                                 normal_add_5_6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_481 (Activation)     (None, 3, 6, 528)    0           normal_concat_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_482 (Activation)     (None, 3, 6, 528)    0           normal_concat_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_7 (Conv2 (None, 3, 6, 88)     46464       activation_481[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_7 (Conv2D)        (None, 3, 6, 88)     46464       activation_482[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_7 (BatchNormalization (None, 3, 6, 88)     352         adjust_conv_projection_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_7 (BatchNormalizati (None, 3, 6, 88)     352         normal_conv_1_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_483 (Activation)     (None, 3, 6, 88)     0           normal_bn_1_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_485 (Activation)     (None, 3, 6, 88)     0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_487 (Activation)     (None, 3, 6, 88)     0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_489 (Activation)     (None, 3, 6, 88)     0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_491 (Activation)     (None, 3, 6, 88)     0           normal_bn_1_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_7 (None, 3, 6, 88)     9944        activation_483[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 3, 6, 88)     8536        activation_485[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_7 (None, 3, 6, 88)     9944        activation_487[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 3, 6, 88)     8536        activation_489[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_7 (None, 3, 6, 88)     8536        activation_491[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 3, 6, 88)     352         separable_conv_1_normal_left1_7[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 3, 6, 88)     352         separable_conv_1_normal_right1_7[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 3, 6, 88)     352         separable_conv_1_normal_left2_7[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 3, 6, 88)     352         separable_conv_1_normal_right2_7[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 3, 6, 88)     352         separable_conv_1_normal_left5_7[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_484 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_486 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_488 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_490 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_492 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_7 (None, 3, 6, 88)     9944        activation_484[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 3, 6, 88)     8536        activation_486[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_7 (None, 3, 6, 88)     9944        activation_488[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 3, 6, 88)     8536        activation_490[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_7 (None, 3, 6, 88)     8536        activation_492[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 3, 6, 88)     352         separable_conv_2_normal_left1_7[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 3, 6, 88)     352         separable_conv_2_normal_right1_7[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 3, 6, 88)     352         separable_conv_2_normal_left2_7[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 3, 6, 88)     352         separable_conv_2_normal_right2_7[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_7 (AveragePooling2 (None, 3, 6, 88)     0           normal_bn_1_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_7 (AveragePooling2 (None, 3, 6, 88)     0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_7 (AveragePooling (None, 3, 6, 88)     0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 3, 6, 88)     352         separable_conv_2_normal_left5_7[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_7 (Add)            (None, 3, 6, 88)     0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_7 (Add)            (None, 3, 6, 88)     0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_7 (Add)            (None, 3, 6, 88)     0           normal_left3_7[0][0]             \n",
      "                                                                 adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_7 (Add)            (None, 3, 6, 88)     0           normal_left4_7[0][0]             \n",
      "                                                                 normal_right4_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_7 (Add)            (None, 3, 6, 88)     0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_7 (Concatenate)   (None, 3, 6, 528)    0           adjust_bn_7[0][0]                \n",
      "                                                                 normal_add_1_7[0][0]             \n",
      "                                                                 normal_add_2_7[0][0]             \n",
      "                                                                 normal_add_3_7[0][0]             \n",
      "                                                                 normal_add_4_7[0][0]             \n",
      "                                                                 normal_add_5_7[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_493 (Activation)     (None, 3, 6, 528)    0           normal_concat_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_494 (Activation)     (None, 3, 6, 528)    0           normal_concat_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_8 (Conv2 (None, 3, 6, 88)     46464       activation_493[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_8 (Conv2D)        (None, 3, 6, 88)     46464       activation_494[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_8 (BatchNormalization (None, 3, 6, 88)     352         adjust_conv_projection_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_8 (BatchNormalizati (None, 3, 6, 88)     352         normal_conv_1_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_495 (Activation)     (None, 3, 6, 88)     0           normal_bn_1_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_497 (Activation)     (None, 3, 6, 88)     0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_499 (Activation)     (None, 3, 6, 88)     0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_501 (Activation)     (None, 3, 6, 88)     0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_503 (Activation)     (None, 3, 6, 88)     0           normal_bn_1_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_8 (None, 3, 6, 88)     9944        activation_495[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 3, 6, 88)     8536        activation_497[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_8 (None, 3, 6, 88)     9944        activation_499[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 3, 6, 88)     8536        activation_501[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_8 (None, 3, 6, 88)     8536        activation_503[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 3, 6, 88)     352         separable_conv_1_normal_left1_8[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 3, 6, 88)     352         separable_conv_1_normal_right1_8[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 3, 6, 88)     352         separable_conv_1_normal_left2_8[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 3, 6, 88)     352         separable_conv_1_normal_right2_8[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 3, 6, 88)     352         separable_conv_1_normal_left5_8[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_496 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_498 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_500 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_502 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_504 (Activation)     (None, 3, 6, 88)     0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_8 (None, 3, 6, 88)     9944        activation_496[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 3, 6, 88)     8536        activation_498[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_8 (None, 3, 6, 88)     9944        activation_500[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 3, 6, 88)     8536        activation_502[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_8 (None, 3, 6, 88)     8536        activation_504[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 3, 6, 88)     352         separable_conv_2_normal_left1_8[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 3, 6, 88)     352         separable_conv_2_normal_right1_8[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 3, 6, 88)     352         separable_conv_2_normal_left2_8[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 3, 6, 88)     352         separable_conv_2_normal_right2_8[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_8 (AveragePooling2 (None, 3, 6, 88)     0           normal_bn_1_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_8 (AveragePooling2 (None, 3, 6, 88)     0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_8 (AveragePooling (None, 3, 6, 88)     0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 3, 6, 88)     352         separable_conv_2_normal_left5_8[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_8 (Add)            (None, 3, 6, 88)     0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_8 (Add)            (None, 3, 6, 88)     0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_8 (Add)            (None, 3, 6, 88)     0           normal_left3_8[0][0]             \n",
      "                                                                 adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_8 (Add)            (None, 3, 6, 88)     0           normal_left4_8[0][0]             \n",
      "                                                                 normal_right4_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_8 (Add)            (None, 3, 6, 88)     0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_8 (Concatenate)   (None, 3, 6, 528)    0           adjust_bn_8[0][0]                \n",
      "                                                                 normal_add_1_8[0][0]             \n",
      "                                                                 normal_add_2_8[0][0]             \n",
      "                                                                 normal_add_3_8[0][0]             \n",
      "                                                                 normal_add_4_8[0][0]             \n",
      "                                                                 normal_add_5_8[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_506 (Activation)     (None, 3, 6, 528)    0           normal_concat_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_505 (Activation)     (None, 3, 6, 528)    0           normal_concat_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_conv_1_reduce_8 (Conv (None, 3, 6, 176)    92928       activation_506[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_reduce_8 (None, 3, 6, 176)    92928       activation_505[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reduction_bn_1_reduce_8 (BatchN (None, 3, 6, 176)    704         reduction_conv_1_reduce_8[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_reduce_8 (BatchNormal (None, 3, 6, 176)    704         adjust_conv_projection_reduce_8[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_507 (Activation)     (None, 3, 6, 176)    0           reduction_bn_1_reduce_8[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_509 (Activation)     (None, 3, 6, 176)    0           adjust_bn_reduce_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 7, 9, 176)    0           activation_507[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 9, 11, 176)   0           activation_509[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 2, 3, 176)    35376       separable_conv_1_pad_reduction_le\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 2, 3, 176)    39600       separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 2, 3, 176)    704         separable_conv_1_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 2, 3, 176)    704         separable_conv_1_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_508 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "activation_510 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 2, 3, 176)    35376       activation_508[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 2, 3, 176)    39600       activation_510[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_511 (Activation)     (None, 3, 6, 176)    0           adjust_bn_reduce_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 2, 3, 176)    704         separable_conv_2_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 2, 3, 176)    704         separable_conv_2_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 9, 11, 176)   0           activation_511[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_513 (Activation)     (None, 3, 6, 176)    0           adjust_bn_reduce_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_1_reduce_8 (Add)  (None, 2, 3, 176)    0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 2, 3, 176)    39600       separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 7, 9, 176)    0           activation_513[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_515 (Activation)     (None, 2, 3, 176)    0           reduction_add_1_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 2, 3, 176)    704         separable_conv_1_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 2, 3, 176)    35376       separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 2, 3, 176)    32560       activation_515[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_512 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 2, 3, 176)    704         separable_conv_1_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 2, 3, 176)    704         separable_conv_1_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_pad_1_reduce_8 (ZeroP (None, 5, 7, 176)    0           reduction_bn_1_reduce_8[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 2, 3, 176)    39600       activation_512[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_514 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "activation_516 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left2_reduce_8 (MaxPo (None, 2, 3, 176)    0           reduction_pad_1_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 2, 3, 176)    704         separable_conv_2_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 2, 3, 176)    35376       activation_514[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 2, 3, 176)    32560       activation_516[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_relu_1_9 (Activation)    (None, 3, 6, 528)    0           normal_concat_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_2_reduce_8 (Add)  (None, 2, 3, 176)    0           reduction_left2_reduce_8[0][0]   \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left3_reduce_8 (Avera (None, 2, 3, 176)    0           reduction_pad_1_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 2, 3, 176)    704         separable_conv_2_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left4_reduce_8 (Avera (None, 2, 3, 176)    0           reduction_add_1_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 2, 3, 176)    704         separable_conv_2_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_right5_reduce_8 (MaxP (None, 2, 3, 176)    0           reduction_pad_1_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_14 (ZeroPadding2 (None, 4, 7, 528)    0           adjust_relu_1_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add3_reduce_8 (Add)   (None, 2, 3, 176)    0           reduction_left3_reduce_8[0][0]   \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 2, 3, 176)    0           reduction_add_2_reduce_8[0][0]   \n",
      "                                                                 reduction_left4_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add4_reduce_8 (Add)   (None, 2, 3, 176)    0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 reduction_right5_reduce_8[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_12 (Cropping2D)      (None, 3, 6, 528)    0           zero_padding2d_14[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reduction_concat_reduce_8 (Conc (None, 2, 3, 704)    0           reduction_add_2_reduce_8[0][0]   \n",
      "                                                                 reduction_add3_reduce_8[0][0]    \n",
      "                                                                 add_12[0][0]                     \n",
      "                                                                 reduction_add4_reduce_8[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_1_9 (AveragePoo (None, 2, 3, 528)    0           adjust_relu_1_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_2_9 (AveragePoo (None, 2, 3, 528)    0           cropping2d_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_1_9 (Conv2D)        (None, 2, 3, 88)     46464       adjust_avg_pool_1_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_2_9 (Conv2D)        (None, 2, 3, 88)     46464       adjust_avg_pool_2_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_517 (Activation)     (None, 2, 3, 704)    0           reduction_concat_reduce_8[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 2, 3, 176)    0           adjust_conv_1_9[0][0]            \n",
      "                                                                 adjust_conv_2_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_9 (Conv2D)        (None, 2, 3, 176)    123904      activation_517[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_9 (BatchNormalization (None, 2, 3, 176)    704         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_9 (BatchNormalizati (None, 2, 3, 176)    704         normal_conv_1_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_518 (Activation)     (None, 2, 3, 176)    0           normal_bn_1_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_520 (Activation)     (None, 2, 3, 176)    0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_522 (Activation)     (None, 2, 3, 176)    0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_524 (Activation)     (None, 2, 3, 176)    0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_526 (Activation)     (None, 2, 3, 176)    0           normal_bn_1_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_9 (None, 2, 3, 176)    35376       activation_518[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 2, 3, 176)    32560       activation_520[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_9 (None, 2, 3, 176)    35376       activation_522[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 2, 3, 176)    32560       activation_524[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_9 (None, 2, 3, 176)    32560       activation_526[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 3, 176)    704         separable_conv_1_normal_left1_9[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 2, 3, 176)    704         separable_conv_1_normal_right1_9[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 3, 176)    704         separable_conv_1_normal_left2_9[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 2, 3, 176)    704         separable_conv_1_normal_right2_9[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 3, 176)    704         separable_conv_1_normal_left5_9[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_519 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_521 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_523 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_525 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_527 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_9 (None, 2, 3, 176)    35376       activation_519[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 2, 3, 176)    32560       activation_521[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_9 (None, 2, 3, 176)    35376       activation_523[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 2, 3, 176)    32560       activation_525[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_9 (None, 2, 3, 176)    32560       activation_527[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 3, 176)    704         separable_conv_2_normal_left1_9[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 2, 3, 176)    704         separable_conv_2_normal_right1_9[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 3, 176)    704         separable_conv_2_normal_left2_9[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 2, 3, 176)    704         separable_conv_2_normal_right2_9[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_9 (AveragePooling2 (None, 2, 3, 176)    0           normal_bn_1_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_9 (AveragePooling2 (None, 2, 3, 176)    0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_9 (AveragePooling (None, 2, 3, 176)    0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 3, 176)    704         separable_conv_2_normal_left5_9[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_9 (Add)            (None, 2, 3, 176)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_9 (Add)            (None, 2, 3, 176)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_9 (Add)            (None, 2, 3, 176)    0           normal_left3_9[0][0]             \n",
      "                                                                 adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_9 (Add)            (None, 2, 3, 176)    0           normal_left4_9[0][0]             \n",
      "                                                                 normal_right4_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_9 (Add)            (None, 2, 3, 176)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_9 (Concatenate)   (None, 2, 3, 1056)   0           adjust_bn_9[0][0]                \n",
      "                                                                 normal_add_1_9[0][0]             \n",
      "                                                                 normal_add_2_9[0][0]             \n",
      "                                                                 normal_add_3_9[0][0]             \n",
      "                                                                 normal_add_4_9[0][0]             \n",
      "                                                                 normal_add_5_9[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_528 (Activation)     (None, 2, 3, 704)    0           reduction_concat_reduce_8[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_529 (Activation)     (None, 2, 3, 1056)   0           normal_concat_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_10 (Conv (None, 2, 3, 176)    123904      activation_528[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_10 (Conv2D)       (None, 2, 3, 176)    185856      activation_529[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_10 (BatchNormalizatio (None, 2, 3, 176)    704         adjust_conv_projection_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_10 (BatchNormalizat (None, 2, 3, 176)    704         normal_conv_1_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_530 (Activation)     (None, 2, 3, 176)    0           normal_bn_1_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_532 (Activation)     (None, 2, 3, 176)    0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_534 (Activation)     (None, 2, 3, 176)    0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_536 (Activation)     (None, 2, 3, 176)    0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_538 (Activation)     (None, 2, 3, 176)    0           normal_bn_1_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 2, 3, 176)    35376       activation_530[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 2, 3, 176)    32560       activation_532[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 2, 3, 176)    35376       activation_534[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 2, 3, 176)    32560       activation_536[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 2, 3, 176)    32560       activation_538[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 3, 176)    704         separable_conv_1_normal_left1_10[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 2, 3, 176)    704         separable_conv_1_normal_right1_10\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 3, 176)    704         separable_conv_1_normal_left2_10[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 2, 3, 176)    704         separable_conv_1_normal_right2_10\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 3, 176)    704         separable_conv_1_normal_left5_10[\n",
      "__________________________________________________________________________________________________\n",
      "activation_531 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_533 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_535 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_537 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_539 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 2, 3, 176)    35376       activation_531[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 2, 3, 176)    32560       activation_533[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 2, 3, 176)    35376       activation_535[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 2, 3, 176)    32560       activation_537[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 2, 3, 176)    32560       activation_539[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 3, 176)    704         separable_conv_2_normal_left1_10[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 2, 3, 176)    704         separable_conv_2_normal_right1_10\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 3, 176)    704         separable_conv_2_normal_left2_10[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 2, 3, 176)    704         separable_conv_2_normal_right2_10\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_10 (AveragePooling (None, 2, 3, 176)    0           normal_bn_1_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_10 (AveragePooling (None, 2, 3, 176)    0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_10 (AveragePoolin (None, 2, 3, 176)    0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 3, 176)    704         separable_conv_2_normal_left5_10[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_10 (Add)           (None, 2, 3, 176)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_10 (Add)           (None, 2, 3, 176)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_10 (Add)           (None, 2, 3, 176)    0           normal_left3_10[0][0]            \n",
      "                                                                 adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_10 (Add)           (None, 2, 3, 176)    0           normal_left4_10[0][0]            \n",
      "                                                                 normal_right4_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_10 (Add)           (None, 2, 3, 176)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_10 (Concatenate)  (None, 2, 3, 1056)   0           adjust_bn_10[0][0]               \n",
      "                                                                 normal_add_1_10[0][0]            \n",
      "                                                                 normal_add_2_10[0][0]            \n",
      "                                                                 normal_add_3_10[0][0]            \n",
      "                                                                 normal_add_4_10[0][0]            \n",
      "                                                                 normal_add_5_10[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_540 (Activation)     (None, 2, 3, 1056)   0           normal_concat_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_541 (Activation)     (None, 2, 3, 1056)   0           normal_concat_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_11 (Conv (None, 2, 3, 176)    185856      activation_540[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_11 (Conv2D)       (None, 2, 3, 176)    185856      activation_541[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_11 (BatchNormalizatio (None, 2, 3, 176)    704         adjust_conv_projection_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_11 (BatchNormalizat (None, 2, 3, 176)    704         normal_conv_1_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_542 (Activation)     (None, 2, 3, 176)    0           normal_bn_1_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_544 (Activation)     (None, 2, 3, 176)    0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_546 (Activation)     (None, 2, 3, 176)    0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_548 (Activation)     (None, 2, 3, 176)    0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_550 (Activation)     (None, 2, 3, 176)    0           normal_bn_1_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 2, 3, 176)    35376       activation_542[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 2, 3, 176)    32560       activation_544[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 2, 3, 176)    35376       activation_546[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 2, 3, 176)    32560       activation_548[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 2, 3, 176)    32560       activation_550[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 3, 176)    704         separable_conv_1_normal_left1_11[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 2, 3, 176)    704         separable_conv_1_normal_right1_11\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 3, 176)    704         separable_conv_1_normal_left2_11[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 2, 3, 176)    704         separable_conv_1_normal_right2_11\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 3, 176)    704         separable_conv_1_normal_left5_11[\n",
      "__________________________________________________________________________________________________\n",
      "activation_543 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_545 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_547 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_549 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_551 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 2, 3, 176)    35376       activation_543[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 2, 3, 176)    32560       activation_545[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 2, 3, 176)    35376       activation_547[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 2, 3, 176)    32560       activation_549[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 2, 3, 176)    32560       activation_551[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 3, 176)    704         separable_conv_2_normal_left1_11[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 2, 3, 176)    704         separable_conv_2_normal_right1_11\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 3, 176)    704         separable_conv_2_normal_left2_11[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 2, 3, 176)    704         separable_conv_2_normal_right2_11\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_11 (AveragePooling (None, 2, 3, 176)    0           normal_bn_1_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_11 (AveragePooling (None, 2, 3, 176)    0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_11 (AveragePoolin (None, 2, 3, 176)    0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 3, 176)    704         separable_conv_2_normal_left5_11[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_11 (Add)           (None, 2, 3, 176)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_11 (Add)           (None, 2, 3, 176)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_11 (Add)           (None, 2, 3, 176)    0           normal_left3_11[0][0]            \n",
      "                                                                 adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_11 (Add)           (None, 2, 3, 176)    0           normal_left4_11[0][0]            \n",
      "                                                                 normal_right4_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_11 (Add)           (None, 2, 3, 176)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_11 (Concatenate)  (None, 2, 3, 1056)   0           adjust_bn_11[0][0]               \n",
      "                                                                 normal_add_1_11[0][0]            \n",
      "                                                                 normal_add_2_11[0][0]            \n",
      "                                                                 normal_add_3_11[0][0]            \n",
      "                                                                 normal_add_4_11[0][0]            \n",
      "                                                                 normal_add_5_11[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_552 (Activation)     (None, 2, 3, 1056)   0           normal_concat_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_553 (Activation)     (None, 2, 3, 1056)   0           normal_concat_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_12 (Conv (None, 2, 3, 176)    185856      activation_552[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_12 (Conv2D)       (None, 2, 3, 176)    185856      activation_553[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_12 (BatchNormalizatio (None, 2, 3, 176)    704         adjust_conv_projection_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_12 (BatchNormalizat (None, 2, 3, 176)    704         normal_conv_1_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_554 (Activation)     (None, 2, 3, 176)    0           normal_bn_1_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_556 (Activation)     (None, 2, 3, 176)    0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_558 (Activation)     (None, 2, 3, 176)    0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_560 (Activation)     (None, 2, 3, 176)    0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_562 (Activation)     (None, 2, 3, 176)    0           normal_bn_1_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 2, 3, 176)    35376       activation_554[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 2, 3, 176)    32560       activation_556[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 2, 3, 176)    35376       activation_558[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 2, 3, 176)    32560       activation_560[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 2, 3, 176)    32560       activation_562[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 3, 176)    704         separable_conv_1_normal_left1_12[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 2, 3, 176)    704         separable_conv_1_normal_right1_12\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 3, 176)    704         separable_conv_1_normal_left2_12[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 2, 3, 176)    704         separable_conv_1_normal_right2_12\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 3, 176)    704         separable_conv_1_normal_left5_12[\n",
      "__________________________________________________________________________________________________\n",
      "activation_555 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_557 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_559 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_561 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_563 (Activation)     (None, 2, 3, 176)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 2, 3, 176)    35376       activation_555[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 2, 3, 176)    32560       activation_557[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 2, 3, 176)    35376       activation_559[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 2, 3, 176)    32560       activation_561[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 2, 3, 176)    32560       activation_563[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 3, 176)    704         separable_conv_2_normal_left1_12[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 2, 3, 176)    704         separable_conv_2_normal_right1_12\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 3, 176)    704         separable_conv_2_normal_left2_12[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 2, 3, 176)    704         separable_conv_2_normal_right2_12\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_12 (AveragePooling (None, 2, 3, 176)    0           normal_bn_1_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_12 (AveragePooling (None, 2, 3, 176)    0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_12 (AveragePoolin (None, 2, 3, 176)    0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 3, 176)    704         separable_conv_2_normal_left5_12[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_12 (Add)           (None, 2, 3, 176)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_12 (Add)           (None, 2, 3, 176)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_12 (Add)           (None, 2, 3, 176)    0           normal_left3_12[0][0]            \n",
      "                                                                 adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_12 (Add)           (None, 2, 3, 176)    0           normal_left4_12[0][0]            \n",
      "                                                                 normal_right4_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_12 (Add)           (None, 2, 3, 176)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_12 (Concatenate)  (None, 2, 3, 1056)   0           adjust_bn_12[0][0]               \n",
      "                                                                 normal_add_1_12[0][0]            \n",
      "                                                                 normal_add_2_12[0][0]            \n",
      "                                                                 normal_add_3_12[0][0]            \n",
      "                                                                 normal_add_4_12[0][0]            \n",
      "                                                                 normal_add_5_12[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_564 (Activation)     (None, 2, 3, 1056)   0           normal_concat_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 1056)         0           activation_564[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 2)            2114        global_average_pooling2d_3[0][0] \n",
      "==================================================================================================\n",
      "Total params: 4,271,254\n",
      "Trainable params: 4,234,516\n",
      "Non-trainable params: 36,738\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.nasnet import NASNetMobile\n",
    "num_rows = 40 #40\n",
    "num_columns = 94 #1\n",
    "num_channels = 1\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "model = 0\n",
    "model = NASNetMobile(weights=None, input_shape=(num_rows, num_columns, num_channels), classes=2, include_top=True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11515 samples, validate on 2879 samples\n",
      "Epoch 1/3\n",
      "11515/11515 [==============================] - 108s 9ms/step - loss: 0.3197 - acc: 0.8562 - val_loss: 0.6688 - val_acc: 0.7840\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66885, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 2/3\n",
      "11515/11515 [==============================] - 37s 3ms/step - loss: 0.1153 - acc: 0.9568 - val_loss: 2.3795 - val_acc: 0.6367\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.66885\n",
      "Epoch 3/3\n",
      "11515/11515 [==============================] - 37s 3ms/step - loss: 0.0565 - acc: 0.9797 - val_loss: 0.1268 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.66885 to 0.12682, saving model to weights.best.basic_cnn.hdf5\n",
      "Training completed in time:  0:17:32.408061\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "num_batch_size = 64\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)\n",
    "!telegram-send \"Server: Habe alles gelernt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9754233608336952\n",
      "Testing Accuracy:  0.9676971170545329\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"m11_nasnetmobile\"\n",
    "os.mkdir(os.path.join(os.getcwd(), \"Models\", folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Models/'+folder+'/weights.model')\n",
    "model.save_weights(\"Models/\"+folder+\"/model.h5\")\n",
    "model_json = model.to_json()\n",
    "with open(\"Models/\"+folder+\"/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Data with MNv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(features) =  44107\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-489.44638, -487.61813, -491.89145, -487.434...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>[[-583.7778, -572.5055, -571.0877, -572.32904,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>[[-648.4932, -645.98413, -643.8437, -645.07465...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>[[-706.79474, -712.2376, -725.35156, -719.0223...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>[[-704.52527, -697.11365, -694.676, -696.69824...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>[[-733.2682, -723.3961, -725.324, -724.75464, ...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>[[-400.89813, -398.52722, -406.01337, -407.850...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>[[-602.1073, -598.3384, -596.61847, -598.0705,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>[[-294.3977, -293.51813, -298.5968, -314.58716...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>[[-333.1444, -337.65695, -349.35815, -349.3763...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>[[-605.18396, -592.51605, -576.4091, -570.9218...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11000</th>\n",
       "      <td>[[-460.46603, -451.17792, -433.82147, -421.351...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12000</th>\n",
       "      <td>[[-437.95483, -424.24734, -430.593, -439.40277...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13000</th>\n",
       "      <td>[[-555.34235, -550.3275, -552.94556, -549.0249...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000</th>\n",
       "      <td>[[-685.04645, -684.145, -695.4721, -698.83545,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>[[-615.2604, -614.54443, -617.6584, -614.22003...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16000</th>\n",
       "      <td>[[-774.86127, -760.97906, -753.68207, -736.665...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17000</th>\n",
       "      <td>[[-484.82086, -488.29892, -489.48734, -489.737...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18000</th>\n",
       "      <td>[[-641.45795, -641.05176, -635.0478, -628.8725...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19000</th>\n",
       "      <td>[[-323.67203, -327.3548, -336.21918, -339.1480...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>[[-410.54178, -404.85245, -402.072, -398.14847...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21000</th>\n",
       "      <td>[[-509.09116, -499.13287, -494.0155, -490.7643...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22000</th>\n",
       "      <td>[[-1131.3708, -1131.3708, -1131.3708, -1131.37...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23000</th>\n",
       "      <td>[[-424.20013, -416.59317, -419.1245, -425.6668...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24000</th>\n",
       "      <td>[[-393.55722, -392.72363, -399.57117, -398.990...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>[[-661.2192, -630.6355, -611.2145, -634.87244,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26000</th>\n",
       "      <td>[[-611.68195, -595.6832, -572.8366, -573.6482,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27000</th>\n",
       "      <td>[[-661.14764, -659.0432, -672.0589, -681.3338,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28000</th>\n",
       "      <td>[[-540.7501, -537.7313, -541.9344, -543.2146, ...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29000</th>\n",
       "      <td>[[-461.3705, -460.93246, -464.36716, -468.7779...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>[[-255.49411, -251.67213, -263.7165, -279.6376...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31000</th>\n",
       "      <td>[[-645.9902, -640.4962, -636.7748, -630.03784,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32000</th>\n",
       "      <td>[[-622.60895, -627.5016, -634.39716, -635.4017...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33000</th>\n",
       "      <td>[[-426.15225, -407.54187, -394.83066, -392.424...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34000</th>\n",
       "      <td>[[-339.14264, -338.27438, -348.3389, -355.0115...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>[[-479.11697, -478.94763, -479.41803, -481.813...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36000</th>\n",
       "      <td>[[-408.0443, -389.32028, -355.2022, -321.0328,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37000</th>\n",
       "      <td>[[-469.35855, -468.85226, -474.5085, -481.2562...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38000</th>\n",
       "      <td>[[-632.64044, -635.9438, -648.1173, -644.9244,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39000</th>\n",
       "      <td>[[-358.27032, -337.87335, -307.69174, -306.213...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40000</th>\n",
       "      <td>[[-379.0592, -381.64258, -391.8768, -404.22556...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41000</th>\n",
       "      <td>[[-348.59708, -334.61734, -311.72153, -273.722...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42000</th>\n",
       "      <td>[[-479.39932, -472.6906, -479.94785, -485.3870...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43000</th>\n",
       "      <td>[[-713.95184, -697.3888, -703.25543, -707.2634...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44000</th>\n",
       "      <td>[[-323.2366, -301.37042, -286.38428, -287.867,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 feature class_label\n",
       "0      [[-489.44638, -487.61813, -491.89145, -487.434...       klapp\n",
       "1000   [[-583.7778, -572.5055, -571.0877, -572.32904,...       klapp\n",
       "2000   [[-648.4932, -645.98413, -643.8437, -645.07465...       klapp\n",
       "3000   [[-706.79474, -712.2376, -725.35156, -719.0223...       klapp\n",
       "4000   [[-704.52527, -697.11365, -694.676, -696.69824...       klapp\n",
       "5000   [[-733.2682, -723.3961, -725.324, -724.75464, ...       klapp\n",
       "6000   [[-400.89813, -398.52722, -406.01337, -407.850...       klapp\n",
       "7000   [[-602.1073, -598.3384, -596.61847, -598.0705,...       klapp\n",
       "8000   [[-294.3977, -293.51813, -298.5968, -314.58716...       klapp\n",
       "9000   [[-333.1444, -337.65695, -349.35815, -349.3763...       klapp\n",
       "10000  [[-605.18396, -592.51605, -576.4091, -570.9218...       klapp\n",
       "11000  [[-460.46603, -451.17792, -433.82147, -421.351...       klapp\n",
       "12000  [[-437.95483, -424.24734, -430.593, -439.40277...       klapp\n",
       "13000  [[-555.34235, -550.3275, -552.94556, -549.0249...       klapp\n",
       "14000  [[-685.04645, -684.145, -695.4721, -698.83545,...       klapp\n",
       "15000  [[-615.2604, -614.54443, -617.6584, -614.22003...       klapp\n",
       "16000  [[-774.86127, -760.97906, -753.68207, -736.665...       klapp\n",
       "17000  [[-484.82086, -488.29892, -489.48734, -489.737...       klapp\n",
       "18000  [[-641.45795, -641.05176, -635.0478, -628.8725...       klapp\n",
       "19000  [[-323.67203, -327.3548, -336.21918, -339.1480...       klapp\n",
       "20000  [[-410.54178, -404.85245, -402.072, -398.14847...       klapp\n",
       "21000  [[-509.09116, -499.13287, -494.0155, -490.7643...       klapp\n",
       "22000  [[-1131.3708, -1131.3708, -1131.3708, -1131.37...       klapp\n",
       "23000  [[-424.20013, -416.59317, -419.1245, -425.6668...     noklapp\n",
       "24000  [[-393.55722, -392.72363, -399.57117, -398.990...     noklapp\n",
       "25000  [[-661.2192, -630.6355, -611.2145, -634.87244,...       klapp\n",
       "26000  [[-611.68195, -595.6832, -572.8366, -573.6482,...     noklapp\n",
       "27000  [[-661.14764, -659.0432, -672.0589, -681.3338,...     noklapp\n",
       "28000  [[-540.7501, -537.7313, -541.9344, -543.2146, ...     noklapp\n",
       "29000  [[-461.3705, -460.93246, -464.36716, -468.7779...     noklapp\n",
       "30000  [[-255.49411, -251.67213, -263.7165, -279.6376...     noklapp\n",
       "31000  [[-645.9902, -640.4962, -636.7748, -630.03784,...     noklapp\n",
       "32000  [[-622.60895, -627.5016, -634.39716, -635.4017...     noklapp\n",
       "33000  [[-426.15225, -407.54187, -394.83066, -392.424...     noklapp\n",
       "34000  [[-339.14264, -338.27438, -348.3389, -355.0115...     noklapp\n",
       "35000  [[-479.11697, -478.94763, -479.41803, -481.813...     noklapp\n",
       "36000  [[-408.0443, -389.32028, -355.2022, -321.0328,...     noklapp\n",
       "37000  [[-469.35855, -468.85226, -474.5085, -481.2562...     noklapp\n",
       "38000  [[-632.64044, -635.9438, -648.1173, -644.9244,...     noklapp\n",
       "39000  [[-358.27032, -337.87335, -307.69174, -306.213...     noklapp\n",
       "40000  [[-379.0592, -381.64258, -391.8768, -404.22556...     noklapp\n",
       "41000  [[-348.59708, -334.61734, -311.72153, -273.722...     noklapp\n",
       "42000  [[-479.39932, -472.6906, -479.94785, -485.3870...     noklapp\n",
       "43000  [[-713.95184, -697.3888, -703.25543, -707.2634...     noklapp\n",
       "44000  [[-323.2366, -301.37042, -286.38428, -287.867,...     noklapp"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pickle.load(open(\"Pickles/pickle_new.p\", \"rb\"))\n",
    "print(\"len(features) = \", len(features))\n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "featuresdf[::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = [np.expand_dims(features[0][0], axis=2)] #[] #np.array(featuresdf.feature.tolist())\n",
    "for f in features[1::]:\n",
    "    xi = np.expand_dims(f[0], axis=2)\n",
    "    X = np.append(X, [xi], axis=0)\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\n",
    "!telegram-send \"Server: Features konvertiert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 40 #40\n",
    "num_columns = 94 #1\n",
    "num_channels = 1\n",
    "\n",
    "#x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "#x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mf151/.direnv/python-3.7.3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40, 94, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_pad (ZeroPadding2D)       (None, 41, 95, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 20, 47, 32)   288         Conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 20, 47, 32)   128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, 20, 47, 32)   0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 20, 47, 32)   288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 20, 47, 32)   128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, 20, 47, 32)   0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 20, 47, 16)   512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 20, 47, 16)   64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 20, 47, 96)   1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 20, 47, 96)   384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, 20, 47, 96)   0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_pad (ZeroPadding2D)     (None, 21, 49, 96)   0           block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 10, 24, 96)   864         block_1_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 10, 24, 96)   384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, 10, 24, 96)   0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 10, 24, 24)   2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 10, 24, 24)   96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 10, 24, 144)  3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 10, 24, 144)  576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, 10, 24, 144)  0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 10, 24, 144)  1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 10, 24, 144)  576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, 10, 24, 144)  0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 10, 24, 24)   3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 10, 24, 24)   96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 10, 24, 24)   0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 10, 24, 144)  3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 10, 24, 144)  576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, 10, 24, 144)  0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_pad (ZeroPadding2D)     (None, 11, 25, 144)  0           block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 5, 12, 144)   1296        block_3_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 5, 12, 144)   576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, 5, 12, 144)   0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 5, 12, 32)    4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 5, 12, 32)    128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 5, 12, 192)   6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 5, 12, 192)   768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, 5, 12, 192)   0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 5, 12, 192)   1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 5, 12, 192)   768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, 5, 12, 192)   0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 5, 12, 32)    6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 5, 12, 32)    128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 5, 12, 32)    0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 5, 12, 192)   6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 5, 12, 192)   768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, 5, 12, 192)   0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 5, 12, 192)   1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 5, 12, 192)   768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, 5, 12, 192)   0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 5, 12, 32)    6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 5, 12, 32)    128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 5, 12, 32)    0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, 5, 12, 192)   6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, 5, 12, 192)   768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, 5, 12, 192)   0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_pad (ZeroPadding2D)     (None, 7, 13, 192)   0           block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, 3, 6, 192)    1728        block_6_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, 3, 6, 192)    768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, 3, 6, 192)    0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, 3, 6, 64)     12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, 3, 6, 64)     256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, 3, 6, 384)    24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, 3, 6, 384)    1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, 3, 6, 384)    0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, 3, 6, 384)    3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, 3, 6, 384)    1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, 3, 6, 384)    0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, 3, 6, 64)     24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, 3, 6, 64)     256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, 3, 6, 64)     0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, 3, 6, 384)    24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, 3, 6, 384)    1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, 3, 6, 384)    0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, 3, 6, 384)    3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, 3, 6, 384)    1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, 3, 6, 384)    0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, 3, 6, 64)     24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, 3, 6, 64)     256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, 3, 6, 64)     0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, 3, 6, 384)    24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, 3, 6, 384)    1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, 3, 6, 384)    0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, 3, 6, 384)    3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, 3, 6, 384)    1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, 3, 6, 384)    0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, 3, 6, 64)     24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, 3, 6, 64)     256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, 3, 6, 64)     0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, 3, 6, 384)    24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, 3, 6, 384)    1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, 3, 6, 384)    0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, 3, 6, 384)    3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, 3, 6, 384)    1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, 3, 6, 384)    0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, 3, 6, 96)     36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, 3, 6, 96)     384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, 3, 6, 576)    55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, 3, 6, 576)    2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, 3, 6, 576)    0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, 3, 6, 576)    5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, 3, 6, 576)    2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, 3, 6, 576)    0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, 3, 6, 96)     55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, 3, 6, 96)     384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, 3, 6, 96)     0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, 3, 6, 576)    55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, 3, 6, 576)    2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, 3, 6, 576)    0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, 3, 6, 576)    5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, 3, 6, 576)    2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, 3, 6, 576)    0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, 3, 6, 96)     55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, 3, 6, 96)     384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, 3, 6, 96)     0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, 3, 6, 576)    55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, 3, 6, 576)    2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, 3, 6, 576)    0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_pad (ZeroPadding2D)    (None, 5, 7, 576)    0           block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, 2, 3, 576)    5184        block_13_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, 2, 3, 576)    2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, 2, 3, 576)    0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, 2, 3, 160)    92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, 2, 3, 160)    640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, 2, 3, 960)    153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, 2, 3, 960)    3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, 2, 3, 960)    0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, 2, 3, 960)    8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, 2, 3, 960)    3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, 2, 3, 960)    0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, 2, 3, 160)    153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, 2, 3, 160)    640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, 2, 3, 160)    0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, 2, 3, 960)    153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, 2, 3, 960)    3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, 2, 3, 960)    0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, 2, 3, 960)    8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, 2, 3, 960)    3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, 2, 3, 960)    0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, 2, 3, 160)    153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, 2, 3, 160)    640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, 2, 3, 160)    0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, 2, 3, 960)    153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, 2, 3, 960)    3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, 2, 3, 960)    0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, 2, 3, 960)    8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, 2, 3, 960)    3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, 2, 3, 960)    0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, 2, 3, 320)    307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, 2, 3, 320)    1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 2, 3, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 2, 3, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 2, 3, 1280)   0           Conv_1_bn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 1280)         0           out_relu[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Logits (Dense)                  (None, 2)            2562        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 2,259,970\n",
      "Trainable params: 2,225,858\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "model = MobileNetV2(weights=None, input_shape=(num_rows, num_columns, num_channels), classes=2, include_top=True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mf151/.direnv/python-3.7.3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 35285 samples, validate on 8822 samples\n",
      "Epoch 1/72\n",
      "35285/35285 [==============================] - 37s 1ms/step - loss: 0.2640 - acc: 0.8813 - val_loss: 0.3338 - val_acc: 0.9525\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.33383, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 2/72\n",
      "35285/35285 [==============================] - 30s 855us/step - loss: 0.0513 - acc: 0.9824 - val_loss: 0.5086 - val_acc: 0.9442\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.33383\n",
      "Epoch 3/72\n",
      "35285/35285 [==============================] - 31s 866us/step - loss: 0.0345 - acc: 0.9888 - val_loss: 0.5123 - val_acc: 0.9363\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.33383\n",
      "Epoch 4/72\n",
      "35285/35285 [==============================] - 31s 866us/step - loss: 0.0300 - acc: 0.9901 - val_loss: 0.1302 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.33383 to 0.13022, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 5/72\n",
      "35285/35285 [==============================] - 30s 861us/step - loss: 0.0228 - acc: 0.9923 - val_loss: 0.2821 - val_acc: 0.9746\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.13022\n",
      "Epoch 6/72\n",
      "35285/35285 [==============================] - 30s 856us/step - loss: 0.0219 - acc: 0.9931 - val_loss: 0.2176 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.13022\n",
      "Epoch 7/72\n",
      "35285/35285 [==============================] - 30s 860us/step - loss: 0.0173 - acc: 0.9937 - val_loss: 0.4800 - val_acc: 0.9653 1s - loss: 0.0174 - acc: 0.993 - ETA: 1s - loss: 0.01\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13022\n",
      "Epoch 8/72\n",
      "35285/35285 [==============================] - 30s 855us/step - loss: 0.0192 - acc: 0.9939 - val_loss: 0.2543 - val_acc: 0.9771\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.13022\n",
      "Epoch 9/72\n",
      "35285/35285 [==============================] - 31s 865us/step - loss: 0.0164 - acc: 0.9942 - val_loss: 0.2087 - val_acc: 0.9825\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.13022\n",
      "Epoch 10/72\n",
      "35285/35285 [==============================] - 31s 865us/step - loss: 0.0161 - acc: 0.9950 - val_loss: 0.2553 - val_acc: 0.9771\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.13022\n",
      "Epoch 11/72\n",
      "35285/35285 [==============================] - 30s 864us/step - loss: 0.0130 - acc: 0.9957 - val_loss: 0.1752 - val_acc: 0.9864\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.13022\n",
      "Epoch 12/72\n",
      "35285/35285 [==============================] - 30s 855us/step - loss: 0.0179 - acc: 0.9943 - val_loss: 1.8421 - val_acc: 0.8684\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.13022\n",
      "Epoch 13/72\n",
      "35285/35285 [==============================] - 30s 860us/step - loss: 0.0157 - acc: 0.9948 - val_loss: 0.1168 - val_acc: 0.9914\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.13022 to 0.11684, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 14/72\n",
      "35285/35285 [==============================] - 30s 861us/step - loss: 0.0108 - acc: 0.9967 - val_loss: 0.1731 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.11684\n",
      "Epoch 15/72\n",
      "35285/35285 [==============================] - 30s 856us/step - loss: 0.0145 - acc: 0.9955 - val_loss: 0.8935 - val_acc: 0.9382\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11684\n",
      "Epoch 16/72\n",
      "35285/35285 [==============================] - 30s 856us/step - loss: 0.0134 - acc: 0.9957 - val_loss: 0.4540 - val_acc: 0.9666\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.11684\n",
      "Epoch 17/72\n",
      "35285/35285 [==============================] - 30s 863us/step - loss: 0.0089 - acc: 0.9971 - val_loss: 0.0976 - val_acc: 0.9903\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.11684 to 0.09756, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 18/72\n",
      "35285/35285 [==============================] - 30s 858us/step - loss: 0.0091 - acc: 0.9969 - val_loss: 0.1253 - val_acc: 0.9875\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.09756\n",
      "Epoch 19/72\n",
      "35285/35285 [==============================] - 31s 871us/step - loss: 0.0097 - acc: 0.9967 - val_loss: 0.6331 - val_acc: 0.9509\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.09756\n",
      "Epoch 20/72\n",
      "35285/35285 [==============================] - 30s 857us/step - loss: 0.0061 - acc: 0.9982 - val_loss: 0.1819 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.09756\n",
      "Epoch 21/72\n",
      "35285/35285 [==============================] - 30s 858us/step - loss: 0.0086 - acc: 0.9967 - val_loss: 0.1234 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.09756\n",
      "Epoch 22/72\n",
      "35285/35285 [==============================] - 30s 860us/step - loss: 0.0083 - acc: 0.9974 - val_loss: 0.0476 - val_acc: 0.9957\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.09756 to 0.04762, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 23/72\n",
      "35285/35285 [==============================] - 31s 866us/step - loss: 0.0066 - acc: 0.9981 - val_loss: 0.0422 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.04762 to 0.04221, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 24/72\n",
      "35285/35285 [==============================] - 30s 854us/step - loss: 0.0066 - acc: 0.9980 - val_loss: 0.4465 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.04221\n",
      "Epoch 25/72\n",
      "35285/35285 [==============================] - 30s 860us/step - loss: 0.0086 - acc: 0.9973 - val_loss: 0.1509 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.04221\n",
      "Epoch 26/72\n",
      "35285/35285 [==============================] - 30s 861us/step - loss: 0.0052 - acc: 0.9981 - val_loss: 0.0509 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.04221\n",
      "Epoch 27/72\n",
      "35285/35285 [==============================] - 30s 848us/step - loss: 0.0067 - acc: 0.9976 - val_loss: 0.3221 - val_acc: 0.9654\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.04221\n",
      "Epoch 28/72\n",
      "35285/35285 [==============================] - 30s 853us/step - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0967 - val_acc: 0.9878\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.04221\n",
      "Epoch 29/72\n",
      "35285/35285 [==============================] - 30s 846us/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.0521 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.04221\n",
      "Epoch 30/72\n",
      "35285/35285 [==============================] - 30s 855us/step - loss: 0.0053 - acc: 0.9985 - val_loss: 0.1190 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.04221\n",
      "Epoch 31/72\n",
      "35285/35285 [==============================] - 30s 855us/step - loss: 0.0053 - acc: 0.9984 - val_loss: 0.1590 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.04221\n",
      "Epoch 32/72\n",
      "35285/35285 [==============================] - 30s 857us/step - loss: 0.0045 - acc: 0.9984 - val_loss: 0.0663 - val_acc: 0.9914\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.04221\n",
      "Epoch 33/72\n",
      "35285/35285 [==============================] - 30s 855us/step - loss: 0.0048 - acc: 0.9985 - val_loss: 0.0466 - val_acc: 0.9916\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.04221\n",
      "Epoch 34/72\n",
      "35285/35285 [==============================] - 30s 856us/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0091 - val_acc: 0.9983\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.04221 to 0.00913, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 35/72\n",
      "35285/35285 [==============================] - 30s 852us/step - loss: 0.0050 - acc: 0.9982 - val_loss: 0.0206 - val_acc: 0.9973\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00913\n",
      "Epoch 36/72\n",
      "35285/35285 [==============================] - 30s 856us/step - loss: 7.2046e-04 - acc: 0.9998 - val_loss: 0.0073 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00913 to 0.00735, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 37/72\n",
      "35285/35285 [==============================] - 30s 859us/step - loss: 0.0050 - acc: 0.9984 - val_loss: 0.0534 - val_acc: 0.9932\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00735\n",
      "Epoch 38/72\n",
      "35285/35285 [==============================] - 30s 855us/step - loss: 0.0054 - acc: 0.9982 - val_loss: 0.0336 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00735\n",
      "Epoch 39/72\n",
      "35285/35285 [==============================] - 30s 859us/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.3380 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00735\n",
      "Epoch 40/72\n",
      "35285/35285 [==============================] - 30s 862us/step - loss: 0.0057 - acc: 0.9980 - val_loss: 0.0564 - val_acc: 0.9929\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00735\n",
      "Epoch 41/72\n",
      "35285/35285 [==============================] - 30s 854us/step - loss: 0.0024 - acc: 0.9992 - val_loss: 0.1149 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00735\n",
      "Epoch 42/72\n",
      "35285/35285 [==============================] - 30s 858us/step - loss: 0.0043 - acc: 0.9985 - val_loss: 0.0206 - val_acc: 0.9965\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00735\n",
      "Epoch 43/72\n",
      "35285/35285 [==============================] - 30s 861us/step - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0244 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00735\n",
      "Epoch 44/72\n",
      "35285/35285 [==============================] - 30s 851us/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.0129 - val_acc: 0.9977\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00735\n",
      "Epoch 45/72\n",
      "35285/35285 [==============================] - 30s 852us/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0100 - val_acc: 0.9977\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00735\n",
      "Epoch 46/72\n",
      "35285/35285 [==============================] - 30s 855us/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0144 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00735\n",
      "Epoch 47/72\n",
      "35285/35285 [==============================] - 30s 860us/step - loss: 0.0033 - acc: 0.9991 - val_loss: 0.0299 - val_acc: 0.9946\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00735\n",
      "Epoch 48/72\n",
      "35285/35285 [==============================] - 30s 851us/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0198 - val_acc: 0.9973\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00735\n",
      "Epoch 49/72\n",
      "35285/35285 [==============================] - 30s 860us/step - loss: 0.0025 - acc: 0.9989 - val_loss: 0.1570 - val_acc: 0.9717\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00735\n",
      "Epoch 50/72\n",
      "35285/35285 [==============================] - 30s 856us/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.0164 - val_acc: 0.9976\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00735\n",
      "Epoch 51/72\n",
      "35285/35285 [==============================] - 30s 858us/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0264 - val_acc: 0.9955\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00735\n",
      "Epoch 52/72\n",
      "35285/35285 [==============================] - 30s 861us/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0089 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00735\n",
      "Epoch 53/72\n",
      "35285/35285 [==============================] - 30s 858us/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0674 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00735\n",
      "Epoch 54/72\n",
      "35285/35285 [==============================] - 30s 852us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0108 - val_acc: 0.9975\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00735\n",
      "Epoch 55/72\n",
      "35285/35285 [==============================] - 30s 849us/step - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0240 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00735\n",
      "Epoch 56/72\n",
      "35285/35285 [==============================] - 30s 851us/step - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0936 - val_acc: 0.9883\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00735\n",
      "Epoch 57/72\n",
      "35285/35285 [==============================] - 30s 853us/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0324 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00735\n",
      "Epoch 58/72\n",
      "35285/35285 [==============================] - 30s 855us/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.1021 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00735\n",
      "Epoch 59/72\n",
      "35285/35285 [==============================] - 30s 853us/step - loss: 0.0047 - acc: 0.9986 - val_loss: 0.0270 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00735\n",
      "Epoch 60/72\n",
      "35285/35285 [==============================] - 30s 860us/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0147 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00735\n",
      "Epoch 61/72\n",
      "35285/35285 [==============================] - 30s 858us/step - loss: 4.4293e-04 - acc: 0.9999 - val_loss: 0.0061 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00735 to 0.00611, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 62/72\n",
      "35285/35285 [==============================] - 30s 854us/step - loss: 7.3409e-06 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00611 to 0.00567, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 63/72\n",
      "35285/35285 [==============================] - 30s 859us/step - loss: 4.1375e-06 - acc: 1.0000 - val_loss: 0.0056 - val_acc: 0.9989\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00567 to 0.00558, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 64/72\n",
      "35285/35285 [==============================] - 30s 857us/step - loss: 0.0046 - acc: 0.9985 - val_loss: 0.0211 - val_acc: 0.9959\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00558\n",
      "Epoch 65/72\n",
      "35285/35285 [==============================] - 30s 854us/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0097 - val_acc: 0.9974\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00558\n",
      "Epoch 66/72\n",
      "35285/35285 [==============================] - 30s 856us/step - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0234 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00558\n",
      "Epoch 67/72\n",
      "35285/35285 [==============================] - 30s 854us/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0156 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00558\n",
      "Epoch 68/72\n",
      "35285/35285 [==============================] - 30s 855us/step - loss: 7.8489e-04 - acc: 0.9997 - val_loss: 0.0054 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00558 to 0.00539, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 69/72\n",
      "35285/35285 [==============================] - 30s 851us/step - loss: 0.0027 - acc: 0.9990 - val_loss: 0.1753 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00539\n",
      "Epoch 70/72\n",
      "35285/35285 [==============================] - 30s 852us/step - loss: 0.0013 - acc: 0.9995 - val_loss: 0.0247 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00539\n",
      "Epoch 71/72\n",
      "35285/35285 [==============================] - 30s 850us/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0206 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00539\n",
      "Epoch 72/72\n",
      "35285/35285 [==============================] - 31s 865us/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0126 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00539\n",
      "Training completed in time:  0:36:52.402641\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "num_epochs = 72\n",
    "num_batch_size = 64\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)\n",
    "!telegram-send \"Server: Habe alles gelernt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9996599121439705\n",
      "Testing Accuracy:  0.9980729993198821\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"m12\"\n",
    "os.mkdir(os.path.join(os.getcwd(), \"Models\", folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Models/'+folder+'/weights.model')\n",
    "model.save_weights(\"Models/\"+folder+\"/model.h5\")\n",
    "model_json = model.to_json()\n",
    "with open(\"Models/\"+folder+\"/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Data with MNv2, for CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(features) =  38358\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-489.44638, -487.61813, -491.89145, -487.434...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>[[-559.9284, -548.13257, -542.2534, -549.5109,...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>[[-547.9967, -539.6625, -532.66705, -524.167, ...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>[[-774.3586, -767.3389, -759.1228, -756.4714, ...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>[[-440.1031, -439.75232, -448.6616, -456.94064...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>[[-430.08942, -421.84375, -427.56442, -431.628...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12000</th>\n",
       "      <td>[[-512.4252, -502.7536, -489.03235, -496.12973...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000</th>\n",
       "      <td>[[-542.21643, -553.39233, -573.47577, -581.750...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16000</th>\n",
       "      <td>[[-541.2151, -535.37646, -521.6501, -509.19128...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18000</th>\n",
       "      <td>[[-520.8307, -506.76318, -504.6328, -512.44025...</td>\n",
       "      <td>klapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>[[-515.4299, -533.1644, -577.53436, -592.33276...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22000</th>\n",
       "      <td>[[-517.7281, -501.6516, -496.5102, -515.05554,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24000</th>\n",
       "      <td>[[-472.57288, -467.11646, -451.7746, -435.8296...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26000</th>\n",
       "      <td>[[-459.8, -452.9947, -455.80804, -458.1623, -4...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28000</th>\n",
       "      <td>[[-713.8814, -701.543, -690.76495, -683.69214,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>[[-566.3405, -561.4, -558.4519, -555.6437, -55...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32000</th>\n",
       "      <td>[[-465.20746, -468.62073, -487.09195, -506.499...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34000</th>\n",
       "      <td>[[-388.41394, -377.38126, -371.03766, -369.937...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36000</th>\n",
       "      <td>[[-209.54222, -207.74629, -212.64536, -210.152...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38000</th>\n",
       "      <td>[[-533.6394, -527.0151, -522.06995, -500.6618,...</td>\n",
       "      <td>noklapp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 feature class_label\n",
       "0      [[-489.44638, -487.61813, -491.89145, -487.434...       klapp\n",
       "2000   [[-559.9284, -548.13257, -542.2534, -549.5109,...       klapp\n",
       "4000   [[-547.9967, -539.6625, -532.66705, -524.167, ...       klapp\n",
       "6000   [[-774.3586, -767.3389, -759.1228, -756.4714, ...       klapp\n",
       "8000   [[-440.1031, -439.75232, -448.6616, -456.94064...       klapp\n",
       "10000  [[-430.08942, -421.84375, -427.56442, -431.628...       klapp\n",
       "12000  [[-512.4252, -502.7536, -489.03235, -496.12973...       klapp\n",
       "14000  [[-542.21643, -553.39233, -573.47577, -581.750...       klapp\n",
       "16000  [[-541.2151, -535.37646, -521.6501, -509.19128...       klapp\n",
       "18000  [[-520.8307, -506.76318, -504.6328, -512.44025...       klapp\n",
       "20000  [[-515.4299, -533.1644, -577.53436, -592.33276...     noklapp\n",
       "22000  [[-517.7281, -501.6516, -496.5102, -515.05554,...     noklapp\n",
       "24000  [[-472.57288, -467.11646, -451.7746, -435.8296...     noklapp\n",
       "26000  [[-459.8, -452.9947, -455.80804, -458.1623, -4...     noklapp\n",
       "28000  [[-713.8814, -701.543, -690.76495, -683.69214,...     noklapp\n",
       "30000  [[-566.3405, -561.4, -558.4519, -555.6437, -55...     noklapp\n",
       "32000  [[-465.20746, -468.62073, -487.09195, -506.499...     noklapp\n",
       "34000  [[-388.41394, -377.38126, -371.03766, -369.937...     noklapp\n",
       "36000  [[-209.54222, -207.74629, -212.64536, -210.152...     noklapp\n",
       "38000  [[-533.6394, -527.0151, -522.06995, -500.6618,...     noklapp"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pickle.load(open(\"Pickles/pickle_new2.p\", \"rb\"))\n",
    "print(\"len(features) = \", len(features))\n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "featuresdf[::2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = [np.expand_dims(features[0][0], axis=2)] #[] #np.array(featuresdf.feature.tolist())\n",
    "for f in features[1::]:\n",
    "    xi = np.expand_dims(f[0], axis=2)\n",
    "    X = np.append(X, [xi], axis=0)\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\n",
    "!telegram-send \"Server: Features konvertiert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 40 #40\n",
    "num_columns = 94 #1\n",
    "num_channels = 1\n",
    "\n",
    "#x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "#x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mf151/.direnv/python-3.7.3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40, 94, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_pad (ZeroPadding2D)       (None, 41, 95, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 20, 47, 32)   288         Conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 20, 47, 32)   128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, 20, 47, 32)   0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 20, 47, 32)   288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 20, 47, 32)   128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, 20, 47, 32)   0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 20, 47, 16)   512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 20, 47, 16)   64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 20, 47, 96)   1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 20, 47, 96)   384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, 20, 47, 96)   0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_pad (ZeroPadding2D)     (None, 21, 49, 96)   0           block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 10, 24, 96)   864         block_1_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 10, 24, 96)   384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, 10, 24, 96)   0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 10, 24, 24)   2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 10, 24, 24)   96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 10, 24, 144)  3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 10, 24, 144)  576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, 10, 24, 144)  0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 10, 24, 144)  1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 10, 24, 144)  576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, 10, 24, 144)  0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 10, 24, 24)   3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 10, 24, 24)   96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 10, 24, 24)   0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 10, 24, 144)  3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 10, 24, 144)  576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, 10, 24, 144)  0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_pad (ZeroPadding2D)     (None, 11, 25, 144)  0           block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 5, 12, 144)   1296        block_3_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 5, 12, 144)   576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, 5, 12, 144)   0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 5, 12, 32)    4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 5, 12, 32)    128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 5, 12, 192)   6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 5, 12, 192)   768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, 5, 12, 192)   0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 5, 12, 192)   1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 5, 12, 192)   768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, 5, 12, 192)   0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 5, 12, 32)    6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 5, 12, 32)    128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 5, 12, 32)    0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 5, 12, 192)   6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 5, 12, 192)   768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, 5, 12, 192)   0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 5, 12, 192)   1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 5, 12, 192)   768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, 5, 12, 192)   0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 5, 12, 32)    6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 5, 12, 32)    128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 5, 12, 32)    0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, 5, 12, 192)   6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, 5, 12, 192)   768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, 5, 12, 192)   0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_pad (ZeroPadding2D)     (None, 7, 13, 192)   0           block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, 3, 6, 192)    1728        block_6_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, 3, 6, 192)    768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, 3, 6, 192)    0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, 3, 6, 64)     12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, 3, 6, 64)     256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, 3, 6, 384)    24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, 3, 6, 384)    1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, 3, 6, 384)    0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, 3, 6, 384)    3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, 3, 6, 384)    1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, 3, 6, 384)    0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, 3, 6, 64)     24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, 3, 6, 64)     256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, 3, 6, 64)     0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, 3, 6, 384)    24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, 3, 6, 384)    1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, 3, 6, 384)    0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, 3, 6, 384)    3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, 3, 6, 384)    1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, 3, 6, 384)    0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, 3, 6, 64)     24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, 3, 6, 64)     256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, 3, 6, 64)     0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, 3, 6, 384)    24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, 3, 6, 384)    1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, 3, 6, 384)    0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, 3, 6, 384)    3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, 3, 6, 384)    1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, 3, 6, 384)    0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, 3, 6, 64)     24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, 3, 6, 64)     256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, 3, 6, 64)     0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, 3, 6, 384)    24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, 3, 6, 384)    1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, 3, 6, 384)    0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, 3, 6, 384)    3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, 3, 6, 384)    1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, 3, 6, 384)    0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, 3, 6, 96)     36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, 3, 6, 96)     384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, 3, 6, 576)    55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, 3, 6, 576)    2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, 3, 6, 576)    0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, 3, 6, 576)    5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, 3, 6, 576)    2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, 3, 6, 576)    0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, 3, 6, 96)     55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, 3, 6, 96)     384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, 3, 6, 96)     0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, 3, 6, 576)    55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, 3, 6, 576)    2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, 3, 6, 576)    0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, 3, 6, 576)    5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, 3, 6, 576)    2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, 3, 6, 576)    0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, 3, 6, 96)     55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, 3, 6, 96)     384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, 3, 6, 96)     0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, 3, 6, 576)    55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, 3, 6, 576)    2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, 3, 6, 576)    0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_pad (ZeroPadding2D)    (None, 5, 7, 576)    0           block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, 2, 3, 576)    5184        block_13_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, 2, 3, 576)    2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, 2, 3, 576)    0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, 2, 3, 160)    92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, 2, 3, 160)    640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, 2, 3, 960)    153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, 2, 3, 960)    3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, 2, 3, 960)    0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, 2, 3, 960)    8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, 2, 3, 960)    3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, 2, 3, 960)    0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, 2, 3, 160)    153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, 2, 3, 160)    640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, 2, 3, 160)    0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, 2, 3, 960)    153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, 2, 3, 960)    3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, 2, 3, 960)    0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, 2, 3, 960)    8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, 2, 3, 960)    3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, 2, 3, 960)    0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, 2, 3, 160)    153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, 2, 3, 160)    640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, 2, 3, 160)    0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, 2, 3, 960)    153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, 2, 3, 960)    3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, 2, 3, 960)    0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, 2, 3, 960)    8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, 2, 3, 960)    3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, 2, 3, 960)    0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, 2, 3, 320)    307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, 2, 3, 320)    1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 2, 3, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 2, 3, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 2, 3, 1280)   0           Conv_1_bn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 1280)         0           out_relu[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Logits (Dense)                  (None, 2)            2562        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 2,259,970\n",
      "Trainable params: 2,225,858\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "model = MobileNetV2(weights=None, input_shape=(num_rows, num_columns, num_channels), classes=2, include_top=True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mf151/.direnv/python-3.7.3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 30686 samples, validate on 7672 samples\n",
      "Epoch 1/50\n",
      "30686/30686 [==============================] - 33s 1ms/step - loss: 0.2602 - acc: 0.8820 - val_loss: 0.3159 - val_acc: 0.9562\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.31587, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 2/50\n",
      "30686/30686 [==============================] - 26s 849us/step - loss: 0.0484 - acc: 0.9836 - val_loss: 0.4185 - val_acc: 0.9472\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.31587\n",
      "Epoch 3/50\n",
      "30686/30686 [==============================] - 26s 862us/step - loss: 0.0297 - acc: 0.9896 - val_loss: 0.1205 - val_acc: 0.9859\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.31587 to 0.12052, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 4/50\n",
      "30686/30686 [==============================] - 26s 858us/step - loss: 0.0223 - acc: 0.9921 - val_loss: 0.2944 - val_acc: 0.9691\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.12052\n",
      "Epoch 5/50\n",
      "30686/30686 [==============================] - 26s 853us/step - loss: 0.0225 - acc: 0.9929 - val_loss: 0.5872 - val_acc: 0.9429\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.12052\n",
      "Epoch 6/50\n",
      "30686/30686 [==============================] - 26s 859us/step - loss: 0.0192 - acc: 0.9934 - val_loss: 0.1331 - val_acc: 0.9855\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.12052\n",
      "Epoch 7/50\n",
      "30686/30686 [==============================] - 26s 852us/step - loss: 0.0166 - acc: 0.9946 - val_loss: 0.5245 - val_acc: 0.9565\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.12052\n",
      "Epoch 8/50\n",
      "30686/30686 [==============================] - 26s 857us/step - loss: 0.0149 - acc: 0.9950 - val_loss: 0.3989 - val_acc: 0.9660\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12052\n",
      "Epoch 9/50\n",
      "30686/30686 [==============================] - 26s 857us/step - loss: 0.0174 - acc: 0.9940 - val_loss: 0.6147 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12052\n",
      "Epoch 10/50\n",
      "30686/30686 [==============================] - 26s 857us/step - loss: 0.0140 - acc: 0.9952 - val_loss: 0.4894 - val_acc: 0.9553\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.12052\n",
      "Epoch 11/50\n",
      "30686/30686 [==============================] - 26s 852us/step - loss: 0.0170 - acc: 0.9941 - val_loss: 1.6261 - val_acc: 0.8832\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.12052\n",
      "Epoch 12/50\n",
      "30686/30686 [==============================] - 26s 856us/step - loss: 0.0107 - acc: 0.9965 - val_loss: 0.2271 - val_acc: 0.9803\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.12052\n",
      "Epoch 13/50\n",
      "30686/30686 [==============================] - 26s 860us/step - loss: 0.0117 - acc: 0.9964 - val_loss: 0.2087 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.12052\n",
      "Epoch 14/50\n",
      "30686/30686 [==============================] - 26s 858us/step - loss: 0.0121 - acc: 0.9963 - val_loss: 0.2670 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.12052\n",
      "Epoch 15/50\n",
      "30686/30686 [==============================] - 26s 860us/step - loss: 0.0131 - acc: 0.9963 - val_loss: 0.6563 - val_acc: 0.9475\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.12052\n",
      "Epoch 16/50\n",
      "30686/30686 [==============================] - 26s 850us/step - loss: 0.0084 - acc: 0.9972 - val_loss: 0.1701 - val_acc: 0.9864\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.12052\n",
      "Epoch 17/50\n",
      "30686/30686 [==============================] - 26s 858us/step - loss: 0.0091 - acc: 0.9970 - val_loss: 0.0984 - val_acc: 0.9914\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.12052 to 0.09841, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 18/50\n",
      "30686/30686 [==============================] - 27s 865us/step - loss: 0.0143 - acc: 0.9951 - val_loss: 0.4852 - val_acc: 0.9589\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.09841\n",
      "Epoch 19/50\n",
      "30686/30686 [==============================] - 26s 857us/step - loss: 0.0080 - acc: 0.9968 - val_loss: 0.5585 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.09841\n",
      "Epoch 20/50\n",
      "30686/30686 [==============================] - 26s 856us/step - loss: 0.0076 - acc: 0.9973 - val_loss: 0.2343 - val_acc: 0.9793\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.09841\n",
      "Epoch 21/50\n",
      "30686/30686 [==============================] - 26s 855us/step - loss: 0.0105 - acc: 0.9971 - val_loss: 0.2454 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.09841\n",
      "Epoch 22/50\n",
      "30686/30686 [==============================] - 26s 857us/step - loss: 0.0065 - acc: 0.9980 - val_loss: 0.0955 - val_acc: 0.9909\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.09841 to 0.09545, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 23/50\n",
      "30686/30686 [==============================] - 26s 860us/step - loss: 0.0093 - acc: 0.9972 - val_loss: 0.1446 - val_acc: 0.9871\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.09545\n",
      "Epoch 24/50\n",
      "30686/30686 [==============================] - 26s 855us/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.1154 - val_acc: 0.9891\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.09545\n",
      "Epoch 25/50\n",
      "30686/30686 [==============================] - 26s 857us/step - loss: 0.0087 - acc: 0.9975 - val_loss: 0.0996 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.09545\n",
      "Epoch 26/50\n",
      "30686/30686 [==============================] - 26s 856us/step - loss: 0.0051 - acc: 0.9982 - val_loss: 0.1220 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.09545\n",
      "Epoch 27/50\n",
      "30686/30686 [==============================] - 26s 859us/step - loss: 0.0054 - acc: 0.9984 - val_loss: 0.0371 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.09545 to 0.03705, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 28/50\n",
      "30686/30686 [==============================] - 26s 856us/step - loss: 0.0028 - acc: 0.9990 - val_loss: 0.2841 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.03705\n",
      "Epoch 29/50\n",
      "30686/30686 [==============================] - 26s 863us/step - loss: 0.0066 - acc: 0.9980 - val_loss: 0.0805 - val_acc: 0.9898\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.03705\n",
      "Epoch 30/50\n",
      "30686/30686 [==============================] - 26s 860us/step - loss: 0.0052 - acc: 0.9984 - val_loss: 0.0726 - val_acc: 0.9914\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.03705\n",
      "Epoch 31/50\n",
      "30686/30686 [==============================] - 26s 861us/step - loss: 0.0046 - acc: 0.9985 - val_loss: 0.0645 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.03705\n",
      "Epoch 32/50\n",
      "30686/30686 [==============================] - 26s 860us/step - loss: 0.0040 - acc: 0.9988 - val_loss: 0.0138 - val_acc: 0.9977\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.03705 to 0.01376, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 33/50\n",
      "30686/30686 [==============================] - 26s 855us/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.0393 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01376\n",
      "Epoch 34/50\n",
      "30686/30686 [==============================] - 26s 861us/step - loss: 0.0035 - acc: 0.9987 - val_loss: 0.2556 - val_acc: 0.9549\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.01376\n",
      "Epoch 35/50\n",
      "30686/30686 [==============================] - 26s 853us/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.0841 - val_acc: 0.9905\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01376\n",
      "Epoch 36/50\n",
      "30686/30686 [==============================] - 26s 858us/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.0308 - val_acc: 0.9934\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01376\n",
      "Epoch 37/50\n",
      "30686/30686 [==============================] - 26s 855us/step - loss: 0.0055 - acc: 0.9982 - val_loss: 0.0888 - val_acc: 0.9868\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01376\n",
      "Epoch 38/50\n",
      "30686/30686 [==============================] - 26s 856us/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.0397 - val_acc: 0.9931\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01376\n",
      "Epoch 39/50\n",
      "30686/30686 [==============================] - 26s 863us/step - loss: 0.0044 - acc: 0.9984 - val_loss: 0.1427 - val_acc: 0.9851\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01376\n",
      "Epoch 40/50\n",
      "30686/30686 [==============================] - 27s 868us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0324 - val_acc: 0.9945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01376\n",
      "Epoch 41/50\n",
      "30686/30686 [==============================] - 26s 859us/step - loss: 0.0018 - acc: 0.9993 - val_loss: 0.0129 - val_acc: 0.9974\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.01376 to 0.01293, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 42/50\n",
      "30686/30686 [==============================] - 26s 860us/step - loss: 0.0038 - acc: 0.9989 - val_loss: 0.9851 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.01293\n",
      "Epoch 43/50\n",
      "30686/30686 [==============================] - 27s 865us/step - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0391 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.01293\n",
      "Epoch 44/50\n",
      "30686/30686 [==============================] - 26s 860us/step - loss: 0.0029 - acc: 0.9992 - val_loss: 0.0266 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01293\n",
      "Epoch 45/50\n",
      "30686/30686 [==============================] - 26s 860us/step - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0614 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01293\n",
      "Epoch 46/50\n",
      "30686/30686 [==============================] - 26s 857us/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0114 - val_acc: 0.9977\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.01293 to 0.01143, saving model to weights.best.basic_cnn.hdf5\n",
      "Epoch 47/50\n",
      "30686/30686 [==============================] - 26s 856us/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0205 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01143\n",
      "Epoch 48/50\n",
      "30686/30686 [==============================] - 26s 859us/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.0210 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01143\n",
      "Epoch 49/50\n",
      "30686/30686 [==============================] - 26s 859us/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.1624 - val_acc: 0.9761\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01143\n",
      "Epoch 50/50\n",
      "30686/30686 [==============================] - 27s 865us/step - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0762 - val_acc: 0.9889\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01143\n",
      "Training completed in time:  0:22:26.607516\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "num_batch_size = 64\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)\n",
    "!telegram-send \"Server: Habe alles gelernt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9907123769525364\n",
      "Testing Accuracy:  0.9889207507820647\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"m13_coreml\"\n",
    "os.mkdir(os.path.join(os.getcwd(), \"Models\", folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Models/'+folder+'/weights.model')\n",
    "model.save_weights(\"Models/\"+folder+\"/model.h5\")\n",
    "model_json = model.to_json()\n",
    "with open(\"Models/\"+folder+\"/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für CoreML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Models/'+folder+'/weights_only.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ONNX",
   "language": "python",
   "name": "python-3.7.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
